<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>大数据默认端口号汇总</title>
    <url>/2020/05/31/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3%E5%8F%B7%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>大数据默认端口号汇总</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>默认端口号</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>访问HDFS页面的端口号</td>
<td>50070/9870(hadoop2.X/hadoop3.x)</td>
</tr>
<tr>
<td></td>
<td>历史服务器</td>
<td>19888</td>
</tr>
<tr>
<td></td>
<td>客户端访问集群(内部通信)</td>
<td>9000/8020(hadoop2.x/hadoop3.x)</td>
</tr>
<tr>
<td>Yarn</td>
<td>访问MR执行情况的端口</td>
<td>8088</td>
</tr>
<tr>
<td>zookeeper</td>
<td>Zookeeper对客户端提供服务端口</td>
<td>2181</td>
</tr>
<tr>
<td></td>
<td>zookeeper集群内部通讯端口，leader监听</td>
<td>2888</td>
</tr>
<tr>
<td></td>
<td>zookeeper集群内部通讯端口，leader选举</td>
<td>3888</td>
</tr>
<tr>
<td>HIVE</td>
<td>metastore服务默认监听端口</td>
<td>9083</td>
</tr>
<tr>
<td></td>
<td>hiveserver2默认端口(JDBC)</td>
<td>10000</td>
</tr>
<tr>
<td>KAFKA</td>
<td>kafka节点间通信的RPC端口</td>
<td>9092</td>
</tr>
<tr>
<td>Hbase</td>
<td>Hbase页面访问端口</td>
<td>16010</td>
</tr>
<tr>
<td>pheonix</td>
<td>pheonix客户端端口</td>
<td>2181</td>
</tr>
<tr>
<td>Azkaban</td>
<td>Azkaban页面访问端口</td>
<td>8081</td>
</tr>
<tr>
<td>superset</td>
<td>superset页面访问端口</td>
<td>8787</td>
</tr>
<tr>
<td>kylin</td>
<td>kylin页面访问端口</td>
<td>7070</td>
</tr>
<tr>
<td>presto</td>
<td>presto默认通讯端口</td>
<td>8080</td>
</tr>
<tr>
<td>ElasticSearch</td>
<td>ES默认通讯端口</td>
<td>9200</td>
</tr>
<tr>
<td>Kibana</td>
<td>Kibana页面访问端口</td>
<td>5601</td>
</tr>
<tr>
<td>redis</td>
<td>redis通讯端口</td>
<td>6379</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>大数据</category>
        <category>端口</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>端口</tag>
      </tags>
  </entry>
  <entry>
    <title>Druid连接池配置信息解释</title>
    <url>/2019/06/16/JDBC/Druid%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%E8%A7%A3%E9%87%8A/</url>
    <content><![CDATA[<h1 id="Druid连接池配置信息解释"><a href="#Druid连接池配置信息解释" class="headerlink" title="Druid连接池配置信息解释"></a>Druid连接池配置信息解释</h1><h2 id="连接池创建"><a href="#连接池创建" class="headerlink" title="连接池创建"></a>连接池创建</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DruidConnection</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		<span class="comment">//创配配置文件对象</span></span><br><span class="line">		Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">		<span class="comment">//加载配置文件</span></span><br><span class="line">		InputStream in = DruidConnection.class.getClassLoader().getResourceAsStream(<span class="string">&quot;prop.properties&quot;</span>);</span><br><span class="line">		prop.load(in);</span><br><span class="line"></span><br><span class="line">		<span class="comment">//System.out.println(prop.getProperty(&quot;maxWait&quot;));</span></span><br><span class="line">		<span class="comment">//获取德鲁伊配置对象</span></span><br><span class="line">		DataSource ds = DruidDataSourceFactory.createDataSource(prop);</span><br><span class="line"></span><br><span class="line">		<span class="comment">//得到连接对象</span></span><br><span class="line">		Connection coon = ds.getConnection();</span><br><span class="line">		</span><br><span class="line">		coon.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="配置信息解释"><a href="#配置信息解释" class="headerlink" title="配置信息解释"></a>配置信息解释</h2><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Durid</span></span><br><span class="line"><span class="attr">username</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">password</span>=<span class="string">123456</span></span><br><span class="line"><span class="attr">url</span>=<span class="string">jdbc:mysql://localhost:3306/test</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时</span></span><br><span class="line"><span class="attr">initialSize</span>=<span class="string">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#最大连接池数量</span></span><br><span class="line"><span class="attr">maxActive</span>=<span class="string">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#已经不再使用，配置了也没效果</span></span><br><span class="line"><span class="attr">maxldle</span>=<span class="string">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。</span></span><br><span class="line"><span class="attr">maxWait</span>=<span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用来检测连接是否有效的sql，要求是一个查询语句。如果validationQuery为null，testOnBorrow、testOnReturn、testWhileIdle都不会其作用</span></span><br><span class="line"><span class="attr">validationQuery</span>=<span class="string">SELECT 1 FROM DUAL</span></span><br><span class="line"><span class="comment">#申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。</span></span><br><span class="line"><span class="attr">testWhileIdle</span>=<span class="string">true</span></span><br><span class="line"><span class="comment">#归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能</span></span><br><span class="line"><span class="attr">testOnBorrow</span>=<span class="string">false</span></span><br><span class="line"><span class="comment">#建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。</span></span><br><span class="line"><span class="attr">testOnReturn</span>=<span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有：</span></span><br><span class="line"><span class="comment">#                                       监控统计用的filter:stat</span></span><br><span class="line"><span class="comment">#                                       日志用的filter:log4j</span></span><br><span class="line"><span class="comment">#                                       防御sql注入的filter:wall</span></span><br><span class="line"><span class="attr">filters</span>=<span class="string">wall</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql下建议关闭。</span></span><br><span class="line"><span class="attr">poolPreparedStatements</span>=<span class="string">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题，可以把这个数值配置大一些，比如说100</span></span><br><span class="line"><span class="attr">maxOpenPreparedStatements</span>=<span class="string">-1</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>java</category>
        <category>JDBC</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>那些年走过的azkaban的坑</title>
    <url>/2020/07/06/azkaban/%E9%82%A3%E4%BA%9B%E5%B9%B4%E8%B5%B0%E8%BF%87%E7%9A%84azkaban%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<p>写在前面的话：azkaban这个轻量级的调度工具说实话报错信息真的很让人捉急，基本上提供不了什么有用的信息</p>
<h2 id="1-java-lang-IllegalStateException-Process-has-not-yet-started"><a href="#1-java-lang-IllegalStateException-Process-has-not-yet-started" class="headerlink" title="1. java.lang.IllegalStateException: Process has not yet started"></a>1. java.lang.IllegalStateException: Process has not yet started</h2><p><img src="https://s1.ax1x.com/2020/07/06/UinScn.png" alt="UinScn.png"><br>检查点一：检查flow文件内容是否写正确，格式问题，脚本路径。这一切都很重要！！！<br>检查点二：如果在集群中部署了多Executor模式。那么在这种模式下Azkaban web Server会根据策略，选取其中一个Executor取执行任务。如果给Azkaban调度扽脚本所需要的应用只在某些节点部署了。<br>方案一：指定特定的Executor取执行任务</p>
<ol>
<li>在MySQL中azkaban数据库executors表中，查询应用所在节点上的Executor的id。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> use azkaban;</span><br><span class="line">Reading <span class="keyword">table</span> information <span class="keyword">for</span> completion <span class="keyword">of</span> <span class="keyword">table</span> <span class="keyword">and</span> <span class="keyword">column</span> names</span><br><span class="line">You can turn off this feature <span class="keyword">to</span> <span class="keyword">get</span> a quicker startup <span class="keyword">with</span> <span class="operator">-</span>A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> executors;</span><br><span class="line"><span class="operator">+</span><span class="comment">----+-----------+-------+--------+</span></span><br><span class="line"><span class="operator">|</span> id <span class="operator">|</span> host          <span class="operator">|</span> port  <span class="operator">|</span> active <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----+-----------+-------+--------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">1</span>   <span class="operator">|</span> hadoop103 <span class="operator">|</span> <span class="number">35985</span> <span class="operator">|</span>      <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">2</span>   <span class="operator">|</span> hadoop104 <span class="operator">|</span> <span class="number">36363</span> <span class="operator">|</span>      <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">3</span>   <span class="operator">|</span> hadoop102 <span class="operator">|</span> <span class="number">12321</span> <span class="operator">|</span>      <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----+-----------+-------+--------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li>
<li>在执行工作流程时加入useExecutor属性，如下<br><img src="https://s1.ax1x.com/2020/07/06/UiuVVf.png" alt="UiuVVf.png"><br>方案二：在Executor所在所有节点部署任务所需脚本和应用。</li>
</ol>
<h2 id="2-upload-job时，显示已经上传，但是看不到任务问题"><a href="#2-upload-job时，显示已经上传，但是看不到任务问题" class="headerlink" title="2.upload job时，显示已经上传，但是看不到任务问题"></a>2.upload job时，显示已经上传，但是看不到任务问题</h2><p>检查.flow的文件，文件名的后缀是否正确。Azkaban并不会告诉你文件后缀不正确。。。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>azkaban</category>
      </categories>
      <tags>
        <tag>azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title>数据仓库关于用户拉链表制作</title>
    <url>/2020/01/14/dataMall/%E7%94%A8%E6%88%B7%E6%8B%89%E9%93%BE%E8%A1%A8%E5%88%B6%E4%BD%9C/</url>
    <content><![CDATA[<h1 id="数据仓库关于用户拉链表制作"><a href="#数据仓库关于用户拉链表制作" class="headerlink" title="数据仓库关于用户拉链表制作"></a>数据仓库关于用户拉链表制作</h1><h2 id="为什么需要拉链表"><a href="#为什么需要拉链表" class="headerlink" title="为什么需要拉链表"></a>为什么需要拉链表</h2><p>在数据仓库中，存在有这样一部分数据：数据量大，但是变化幅度很小。可能99%的数据都是一样的。<br>比如：用户信息会发生变化，但是每天变化的比例不高。如果数据量有一定规模，按照每日全量的方式保存效率很低。 比如：1亿用户*365天，每天一份用户信息。(做每日全量效率低)</p>
<h2 id="拉链表制作流程"><a href="#拉链表制作流程" class="headerlink" title="拉链表制作流程"></a>拉链表制作流程</h2><p>用户当日全部数据和MySQL中每天变化的数据拼接在一起，形成一个新的临时拉链表数据。用临时的拉链表覆盖旧的拉链表数据。（这就解决了hive表中数据不能更新的问题）<br><img src="https://i.loli.net/2020/07/01/Vd8ixLfnuqG5waj.png" alt="拉链表.png"></p>
<h2 id="拉链表制作过程"><a href="#拉链表制作过程" class="headerlink" title="拉链表制作过程"></a>拉链表制作过程</h2><p><img src="https://s1.ax1x.com/2020/07/01/N7LNX6.png" alt="N7LNX6.png"></p>
<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>原表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1001	zhangsan	male	2020-01-01	9999-99-99</span><br><span class="line">1002	lisi	female	2020-01-01	9999-99-99</span><br><span class="line">1003	wangwu	male	2020-01-01	9999-99-99</span><br><span class="line">1004	zhaoliu	female	2020-01-01	9999-99-99</span><br></pre></td></tr></table></figure>
<p>变动数据表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1002	lisi	male	2020-01-02	9999-99-99</span><br><span class="line">1004	zhaoliu	female	2020-01-02	9999-99-99</span><br><span class="line">1005	xiaoming	male	2020-01-02	9999-99-99</span><br></pre></td></tr></table></figure>
<p>建表语句<br>原表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> zip(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">sex string,</span><br><span class="line">start_date string,</span><br><span class="line">end_date string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span><span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>要改变的内容</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> ud(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">sex string,</span><br><span class="line">start_date string,</span><br><span class="line">end_date string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span><span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="拉链表制作"><a href="#拉链表制作" class="headerlink" title="拉链表制作"></a>拉链表制作</h3><ol>
<li><p>首先需要使用原表(zip表)去join需要修改的表。找出需要被修改的数据,如果数据被修改了就要将被修改的数据的end_date改写成修改表start_date的前一天。</p>
</li>
<li><p>在join的on条件中，还需要再加入一个条件为原表的end_date的时间一定要是9999-99-99.否则，这个join语句会将原表所有的end_date都进行修改</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       aa.id,</span><br><span class="line">       aa.name,</span><br><span class="line">       aa.sex,</span><br><span class="line">       aa.start_date,</span><br><span class="line">       if(bb.id <span class="keyword">is</span> <span class="keyword">null</span>,aa.end_date,date_sub(bb.start_date,<span class="number">1</span>))</span><br><span class="line">   <span class="keyword">from</span></span><br><span class="line">       zip aa</span><br><span class="line">   <span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">       ud bb</span><br><span class="line">   <span class="keyword">on</span>  aa.id<span class="operator">=</span>bb.id <span class="keyword">and</span> aa.end_date <span class="operator">=</span> <span class="string">&#x27;9999-99-99&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li><p>将查询结果与修改表进行union，得到最后的结果，再将原表进行覆盖操作</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> zip</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        aa.id,</span><br><span class="line">        aa.name,</span><br><span class="line">        aa.sex,</span><br><span class="line">        aa.start_date,</span><br><span class="line">        if(bb.id <span class="keyword">is</span> <span class="keyword">null</span>,aa.end_date,date_sub(bb.start_date,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        zip aa</span><br><span class="line">    <span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">        ud bb</span><br><span class="line">    <span class="keyword">on</span>  aa.id<span class="operator">=</span>bb.id <span class="keyword">and</span> aa.end_date <span class="operator">=</span> <span class="string">&#x27;9999-99-99&#x27;</span></span><br><span class="line">) t1</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> ud</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">id;</span><br></pre></td></tr></table></figure>
<h2 id="拉链表注意事项"><a href="#拉链表注意事项" class="headerlink" title="拉链表注意事项"></a>拉链表注意事项</h2><p>在企业级开发中：<br>由于需要对原表进行覆盖操作，通常这一步骤非常危险！！！<br>万一在覆盖的过程中集群宕机了，很可能用户表中所有的数据都会丢失！！！！<br>所以一定要先建立一个临时表，将数据先存入临时表中，再将临时表的数据覆盖写入用户表。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume问题记录：Flume在修改文件名后会重复读取文件问题</title>
    <url>/2020/05/18/flume/Flume%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%EF%BC%9AFlume%E5%9C%A8%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%90%8D%E5%90%8E%E4%BC%9A%E9%87%8D%E5%A4%8D%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="1-Flume在修改文件名后会重复读取文件问题"><a href="#1-Flume在修改文件名后会重复读取文件问题" class="headerlink" title="1.Flume在修改文件名后会重复读取文件问题"></a>1.Flume在修改文件名后会重复读取文件问题</h2><p>问题描述：<br>使用正则表示监控文件名时，当修改文件名称之后，会重复读取数据。<br>问题场景：<br>在生产环境下，使用log4j打印日志框架时，会变更打印日志名称，造成flume重复读取<br>问题重现：</p>
<ol>
<li><p>配置信息 test.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; TAILDIR</span><br><span class="line">a1.sources.r1.filegroups &#x3D; f1</span><br><span class="line">a1.sources.r1.filegroups.f1 &#x3D; &#x2F;opt&#x2F;module&#x2F;data&#x2F;flume.*</span><br><span class="line">a1.sources.r1.positionFile &#x3D; &#x2F;opt&#x2F;module&#x2F;flume&#x2F;taildir&#x2F;taildir_flume.json</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>启动任务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;flume-ng agent -n a1 -c conf -f conf&#x2F;test.conf Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure></li>
<li><p>测试</p>
</li>
<li><p>1 在/opt/module/data目录下创建flume.开头的文件</p>
</li>
<li><p>2 写入数据</p>
</li>
<li><p>3 修改文件名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch flume.log</span><br><span class="line">echo aaa &gt;&gt; flume.log </span><br><span class="line">echo bbb &gt;&gt; flume.log </span><br><span class="line">mv flume.log flume.log123 </span><br></pre></td></tr></table></figure></li>
<li><p>查看控制台，会发现控制台上当前文件输出了两次。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2020-06-24 15:37:34,682 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 61 61                                        aaa &#125;</span><br><span class="line">2020-06-24 15:38:06,712 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 62 62 62                                        bbb &#125;</span><br><span class="line">2020-06-24 15:38:36,719 (PollableSourceRunner-TaildirSource-r1) [INFO - org.apache.flume.source.taildir.ReliableTaildirEventReader.openFile(ReliableTaildirEventReader.java:290)] Opening file: &#x2F;opt&#x2F;module&#x2F;data&#x2F;flume.log123, inode: 405574, pos: 0</span><br><span class="line">2020-06-24 15:38:36,719 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61 61 61                                        aaa &#125;</span><br><span class="line">2020-06-24 15:38:36,720 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 62 62 62                                        bbb &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>Flume在判断文件是否为新文件的时候会记录两个值：<br>indode：linux文件的唯一id<br>file：文件路径</p>
<h2 id="解决方案一：协商处理"><a href="#解决方案一：协商处理" class="headerlink" title="解决方案一：协商处理"></a>解决方案一：协商处理</h2><p>不变更文件名</p>
</li>
</ol>
<p>跟公司后台人员协商；<br>让他们使用类似logback不更名打印日志框架，不要使用log4j会更名的打印日志框架。</p>
<h2 id="解决方案二：修改源码"><a href="#解决方案二：修改源码" class="headerlink" title="解决方案二：修改源码"></a>解决方案二：修改源码</h2><p>下载flume的源码包，打开<strong>flume-taildir-source</strong>项目文件。<br>修改地点一：<strong>TailFile.java</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">updatePos</span><span class="params">(String path, <span class="keyword">long</span> inode, <span class="keyword">long</span> pos)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">     <span class="comment">//if (this.inode == inode &amp;&amp; this.path.equals(path)) &#123;</span></span><br><span class="line">     <span class="keyword">if</span> (<span class="keyword">this</span>.inode == inode) &#123;</span><br><span class="line">         setPos(pos);</span><br><span class="line">         updateFilePos(pos);</span><br><span class="line">         logger.info(<span class="string">&quot;Updated position, file: &quot;</span> + path + <span class="string">&quot;, inode: &quot;</span> + inode + <span class="string">&quot;, pos: &quot;</span> + pos);</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>将if判断条件的&amp;&amp;逻辑语句后半段删除。<br>修改地点二：<strong>ReliableTaildirEventReader.java</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Long&gt; <span class="title">updateTailFiles</span><span class="params">(<span class="keyword">boolean</span> skipToEnd)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        updateTime = System.currentTimeMillis();</span><br><span class="line">        List&lt;Long&gt; updatedInodes = Lists.newArrayList();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TaildirMatcher taildir : taildirCache) &#123;</span><br><span class="line">            Map&lt;String, String&gt; headers = headerTable.row(taildir.getFileGroup());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (File f : taildir.getMatchingFiles()) &#123;</span><br><span class="line">                <span class="keyword">long</span> inode;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    inode = getInode(f);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (NoSuchFileException e) &#123;</span><br><span class="line">                    logger.info(<span class="string">&quot;File has been deleted in the meantime: &quot;</span> + e.getMessage());</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                TailFile tf = tailFiles.get(inode);</span><br><span class="line">                <span class="comment">//if (tf == null || !tf.getPath().equals(f.getAbsolutePath())) &#123;</span></span><br><span class="line">                <span class="keyword">if</span> (tf == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">long</span> startPos = skipToEnd ? f.length() : <span class="number">0</span>;</span><br><span class="line">                    tf = openFile(f, headers, inode, startPos);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">boolean</span> updated = tf.getLastUpdated() &lt; f.lastModified() || tf.getPos() != f.length();</span><br><span class="line">                    <span class="keyword">if</span> (updated) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (tf.getRaf() == <span class="keyword">null</span>) &#123;</span><br><span class="line">                            tf = openFile(f, headers, inode, tf.getPos());</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">if</span> (f.length() &lt; tf.getPos()) &#123;</span><br><span class="line">                            logger.info(<span class="string">&quot;Pos &quot;</span> + tf.getPos() + <span class="string">&quot; is larger than file size! &quot;</span></span><br><span class="line">                                    + <span class="string">&quot;Restarting from pos 0, file: &quot;</span> + tf.getPath() + <span class="string">&quot;, inode: &quot;</span> + inode);</span><br><span class="line">                            tf.updatePos(tf.getPath(), inode, <span class="number">0</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    tf.setNeedTail(updated);</span><br><span class="line">                &#125;</span><br><span class="line">                tailFiles.put(inode, tf);</span><br><span class="line">                updatedInodes.add(inode);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> updatedInodes;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>对于新生成的文件不再添加路径是否相同条件</p>
<p>编译成jar包，替换掉flume/lib目录下的<strong>flume-taildir-source-1.9.0.jar</strong>版本可能不同</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume问题记录：使用kafuka Source时日志日期不对问题</title>
    <url>/2020/05/18/flume/Flume%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%EF%BC%9A%E4%BD%BF%E7%94%A8kafuka_Source%E6%97%B6%E6%97%A5%E5%BF%97%E6%97%A5%E6%9C%9F%E4%B8%8D%E5%AF%B9%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="1-使用kafuka-Source时日志日期不对问题"><a href="#1-使用kafuka-Source时日志日期不对问题" class="headerlink" title="1.使用kafuka Source时日志日期不对问题"></a>1.使用kafuka Source时日志日期不对问题</h2><p>问题描述：<br>HDFS Sink要想根据时间滚动文件夹，必须在Event头信息中添加”timestamp”的key用于提供给HDFS Sink使用。<br>我们目前使用的是KafkaSource，会根据当前系统时间添加该头信息。<br>问题：我们使用的是按照每天的具体时间来创建新的目录，假如我们Flume任务在夜间11点多挂了，零点以后任务才被重新启动，那么昨天的挂掉之后的数据就会被算作第二天的数据了。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>我们需要根据事件内部时间来控制HDFS目录时间的创建，思路为自定义拦截器来修改KafkaSource自动添加的时间戳。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop Apache 2.7.2 安装教程</title>
    <url>/2020/03/02/hadoop/Hadoop_Apache_2.7.2_%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="Hadoop-Apache-2-7-2-安装教程"><a href="#Hadoop-Apache-2-7-2-安装教程" class="headerlink" title="Hadoop Apache 2.7.2 安装教程"></a>Hadoop Apache 2.7.2 安装教程</h1><h2 id="1-JDK安装"><a href="#1-JDK安装" class="headerlink" title="1.JDK安装"></a>1.JDK安装</h2><p>&emsp;&emsp;hadoop的功能运行需要的JDK版本在1.7以上。所以先要查询JDK的版本是否在1.7以上。<br>1.查询方法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">rpm <span class="operator">-</span>ga <span class="operator">|</span> grep java</span><br></pre></td></tr></table></figure>
<p>2.若不是，卸载，并安装jdk1.7以上版本</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sudu rpm <span class="operator">-</span>e 软件包</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/maobois/article/details/53414723">如何对当前用户获取root权限。</a></p>
<blockquote>
<p><strong>jdk-8u144-linux-x64.tar安装包</strong><br>链接：<a href="https://pan.baidu.com/s/10-Vxjw3PgJrgOcXd4b14oA">https://pan.baidu.com/s/10-Vxjw3PgJrgOcXd4b14oA</a><br>提取码：kqk6</p>
</blockquote>
<p>3.进入到JDK的软件包目录，解压JDK,</p>
<blockquote>
<p>笔者这里在/opt目录下创建了一个module目录，专门放各种软件</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">tar <span class="operator">-</span>zxvf jdk<span class="number">-8</span>u144<span class="operator">-</span>linux<span class="operator">-</span>x64.tar.gz <span class="operator">-</span>C <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span></span><br></pre></td></tr></table></figure>
<p>4.配置JDK的环境变量<br>(1)获取JDK的路径：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">pwd</span><br></pre></td></tr></table></figure>

<blockquote>
<p>/opt/module/jdk1.8.0_144</p>
</blockquote>
<p>(2)打开/etc/profile.d/env.sh.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sudo vim <span class="operator">/</span>etc<span class="operator">/</span>profile.d<span class="operator">/</span>env.sh</span><br></pre></td></tr></table></figure>
<p>在env.sh中添加JDK路径</p>
<blockquote>
<p>#JAVA_HOME<br>export JAVA_HOME=/opt/module/jdk1.8.0_144<br>export PATH=$PATH:$JAVA_HOME/bin</p>
</blockquote>
<p>注：很多人这里配置的环境变量目录是/etc/profile的文件。两者的区别是/etc/profile.d目录下的文件在之后集群ssh到其他节点后会直接加载。避免了之后ssh到其他节点要重新刷新配置文件<br>(3) 刷新配置文件</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">source <span class="operator">/</span>etc<span class="operator">/</span>profile</span><br></pre></td></tr></table></figure>
<p>(4)测试JDK是否安装成功</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">java <span class="operator">-</span>version</span><br></pre></td></tr></table></figure>

<h2 id="2-安装Hadoop"><a href="#2-安装Hadoop" class="headerlink" title="2.安装Hadoop"></a>2.安装Hadoop</h2><p>Hadoop下载地址：<a href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/">https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/</a><br>1.将hadoop-2.7.2.tar.gz导入到linux<br>2.解压hadoop-2.7.2.tar.gz</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">tar <span class="operator">-</span>zxvf hadoop<span class="number">-2.7</span><span class="number">.2</span>.tar.gz <span class="operator">-</span>C <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span></span><br></pre></td></tr></table></figure>
<p>3.配置hadoop的环境变量<br>(1)获取hadoop的安装路径</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">pwd</span><br></pre></td></tr></table></figure>

<blockquote>
<p>/opt/module/hadoop-2.7.2</p>
</blockquote>
<p>(2)打开/etc/profile.d/env.sh文件</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sudo vim <span class="operator">/</span>etc<span class="operator">/</span>profile.d<span class="operator">/</span>env.sh</span><br></pre></td></tr></table></figure>
<p>(3)在文件末尾加上Hadoop路径</p>
<blockquote>
<p>#HADOOP_HOME<br>export HADOOP_HOME=/opt/module/hadoop-2.7.2<br>export PATH=$PATH:$HADOOP_HOME/bin<br>export PATH=$PATH:$HADOOP_HOME/sbin</p>
</blockquote>
<p>(4)刷新配置文件让文件生效</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">source <span class="operator">/</span>etc<span class="operator">/</span>profile</span><br></pre></td></tr></table></figure>
<p>(5)测试Hadoop是否安装成功</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure>
<h2 id="3-完全分布式环境配置"><a href="#3-完全分布式环境配置" class="headerlink" title="3.完全分布式环境配置"></a>3.完全分布式环境配置</h2><p>安装前确认：</p>
<ol>
<li>确定客户机的台数(关闭防火墙，静态ip，主机名)，笔者为了测试简便，使用3台客户机，分别为hadoop111，hadoop112，hadoop113</li>
<li>安装JDK。参照上面</li>
<li>配置环境变量。参照上面</li>
<li>安装Hadoop。参照上面</li>
<li>配置集群</li>
<li>设置单点启动</li>
<li>配置ssh</li>
<li>群起测试集群</li>
</ol>
<p>(1)编写集群分发脚本<br>&emsp;&emsp;①在家目录下创建bin目录，并在bin目录下创建分发脚本xsync</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> ~]<span class="variable">$</span> mkdir bin</span><br><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> ~]<span class="variable">$</span> <span class="built_in">cd</span> bin/</span><br><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> <span class="type">bin</span>]<span class="variable">$</span> vi xsync</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;②在脚本文件中编写分发代码</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment">#1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=<span class="variable">$</span><span class="comment">#</span></span><br><span class="line"><span class="keyword">if</span> ((pcount==<span class="number">0</span>)); then</span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="keyword">exit</span>;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 获取文件名称</span></span><br><span class="line">p1=<span class="variable">$1</span></span><br><span class="line">fname=`basename <span class="variable">$p1</span>`</span><br><span class="line"><span class="built_in">echo</span> fname=<span class="variable">$fname</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd <span class="literal">-P</span> <span class="variable">$</span>(dirname <span class="variable">$p1</span>); <span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">echo</span> pdir=<span class="variable">$pdir</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="comment">#5 循环</span></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> hadoop111 hadoop112 hadoop113</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> ------------------- <span class="variable">$host</span> --------------</span><br><span class="line">    rsync <span class="literal">-av</span> <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$user</span><span class="selector-tag">@</span><span class="variable">$host:</span><span class="variable">$pdir</span></span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;③修改权限</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> <span class="type">bin</span>]<span class="variable">$</span> chmod <span class="number">777</span> xsync</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;④测试脚本</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> <span class="type">bin</span>]<span class="variable">$</span> xsync xsync</span><br></pre></td></tr></table></figure>
<p>（2）<a href="https://www.linuxidc.com/Linux/2017-07/145450.htm">配置SSH无秘钥登录</a><br>简单叙述：进入家目录的.ssh目录<br>①生成公钥和私钥：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> <span class="type">.ssh</span>]<span class="variable">$</span> ssh<span class="literal">-keygen</span> <span class="literal">-t</span> rsa</span><br></pre></td></tr></table></figure>
<blockquote>
<p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p>
</blockquote>
<p>②将公钥拷贝到要免密登录的目标机器上<br>[ziyang@hadoop111 .ssh]$ ssh-copy-id hadoop111<br>[ziyang@hadoop111 .ssh]$ ssh-copy-id hadoop112<br>[ziyang@hadoop111 .ssh]$ ssh-copy-id hadoop113<br><font color=red><br>注意：<br>还需要在hadoop112上采用atguigu账号配置一下无密登录到hadoop111、hadoop112、hadoop113服务器上。<br>hadoop113同理<br></font><br>(3)集群规划<br>①在搭集群前，首先要对集群有一个详细的规划。<br>注意：NameNode和SecondaryNameNode还有ResourceManager不要安装在同一台服务器<br>所以对于3台机器的集群规划<br>|| hadoop111 | hadoop112 | hadoop113 |<br>|–|–|–|–|<br>| HDFS | NameNode、DateNode |DateNode|SecondaryNameNode、DateNode|<br>| YARN | NodeManager |ResourceManager、NodeManager|NodeManager|<br>②核心文件配置<br>核心文件的配置一共有<br>core-site.xml<br>hdfs-site.xml<br>yarn-site.xml<br>mapred-site.xml</p>
<p>hadoop-env.sh<br>yarn-env.sh<br>mapred-env.sh</p>
<p>slaves<br>1)进入hadoop的配置目录</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> ~]<span class="variable">$</span> <span class="built_in">cd</span> /opt/module/hadoop<span class="literal">-2</span>.<span class="number">7.2</span>/etc/hadoop/</span><br></pre></td></tr></table></figure>
<p>2）配置核心配置文件core-site.xml<br>在该文件中编写如下配置</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hdfs://hadoop102:<span class="number">9000</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;/opt/module/hadoop<span class="literal">-2</span>.<span class="number">7.2</span>/<span class="keyword">data</span>/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>3）配置HDFS配置文件hdfs-site.xml与hadoop-env.sh<br>3.1配置hdfs-site.xml<br>在该文件中编写如下配置</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;<span class="number">3</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.namenode.secondary.http<span class="literal">-address</span>&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hadoop113:<span class="number">50090</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>3.2配置hadoop-env.sh</p>
<blockquote>
<p>仔细找，在如下的信息下面<br> <code># The java implementation to use.</code><br>在下一行去掉注释，加上JDK的路径</p>
</blockquote>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.<span class="number">8.0</span>_144</span><br></pre></td></tr></table></figure>

<p>4）配置Yarn配置文件yarn-site.xml与yarn-env.sh<br>4.1配置yarn-site.xml文件<br>在该文件中编写如下配置</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Reducer获取数据的方式 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.aux<span class="literal">-services</span>&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hadoop112&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 日志聚集功能使能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log<span class="literal">-aggregation</span><span class="literal">-enable</span>&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 日志保留时间设置<span class="number">7</span>天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log<span class="literal">-aggregation</span>.retain<span class="literal">-seconds</span>&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="number">604800</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>4.2配置yarn-env.sh文件</p>
<blockquote>
<p>仔细找，在如下信息下面<br> <code># some Java parameters</code><br> 在下一行去掉注释，加上JDK路径</p>
</blockquote>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.<span class="number">8.0</span>_144</span><br></pre></td></tr></table></figure>
<p>5）配置MapReduce配置文件中mapred-site.xml与mapred-env.sh<br>5.1配置mapred-site.xml</p>
<blockquote>
<p>将mapred-site.xml的后缀名去掉<br>mv mapred-site.xml.template mapred-site.xml</p>
</blockquote>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定MR运行在Yarn上 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">		&lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop102:<span class="number">10020</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 历史服务器web端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop102:<span class="number">19888</span>&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>5.2配置mapred-env.sh</p>
<blockquote>
<p>还是一样配置JDK路径<br>export JAVA_HOME=/opt/module/jdk1.8.0_144</p>
</blockquote>
<p>6）配置salaves<br>在文件中加入客户端的名字</p>
<blockquote>
<p>hadoop111<br>hadoop112<br>hadoop113</p>
</blockquote>
<font color=red>
注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。
</font>

<p>(4) 分别在hadoop112、hadoop113的/opt目录下创建module文件夹，并修改所有者和所有者组为你的用户名</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop112</span> <span class="type">opt</span>]<span class="variable">$</span> sudo mkdir module</span><br><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop112</span> <span class="type">opt</span>]<span class="variable">$</span> sudo chown atguigu:atguigu module/</span><br><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop113</span> <span class="type">opt</span>]<span class="variable">$</span> sudo mkdir module</span><br><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop113</span> <span class="type">opt</span>]<span class="variable">$</span> sudo chown atguigu:atguigu module/</span><br></pre></td></tr></table></figure>
<p>分发JDK文件与Hadoop文件</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop<span class="literal">-2</span>.<span class="number">7.2</span>/</span><br><span class="line">xsync jdk1.<span class="number">8.0</span>_144/</span><br></pre></td></tr></table></figure>
<p>在其他的客户端集群配置好配置文件<br>(5)群启集群<br>1）如果集群是第一次启动，需要格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> <span class="type">hadoop</span>-<span class="number">2.7</span><span class="type">.2</span>]<span class="variable">$</span> bin/hdfs namenode <span class="literal">-format</span></span><br></pre></td></tr></table></figure>
<p>2）启动HDFS</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop111</span> <span class="type">hadoop</span>-<span class="number">2.7</span><span class="type">.2</span>]<span class="variable">$</span> sbin/<span class="built_in">start-dsf</span>.sh</span><br></pre></td></tr></table></figure>
<p>3)启动Yarn<br><font color=red><br>因为yarn配置在hadoop112所以要去hadoop112启动<br></font></p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop112</span> <span class="type">hadoop</span>-<span class="number">2.7</span><span class="type">.2</span>]<span class="variable">$</span> sbin/<span class="built_in">start-yarn</span>.sh</span><br></pre></td></tr></table></figure>
<h2 id="4-支持LZO压缩配置"><a href="#4-支持LZO压缩配置" class="headerlink" title="4.支持LZO压缩配置"></a>4.支持LZO压缩配置</h2><p>(1）hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。<br>以下是编译好的Lzo压缩文件</p>
<blockquote>
<p>链接：<a href="https://pan.baidu.com/s/1b46cClcMlruI3FYViM_eVw">https://pan.baidu.com/s/1b46cClcMlruI3FYViM_eVw</a><br>提取码：nkzh</p>
</blockquote>
<p>(2)直接将编译好的jar包放入hadoop-2.7.2/share/hadoop/common/<br>并同步到hadoop112，hadoop113</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop112</span> <span class="type">common</span>]<span class="variable">$</span> xsync hadoop<span class="literal">-lzo</span><span class="literal">-0</span>.<span class="number">4.20</span>.jar</span><br></pre></td></tr></table></figure>
<p>(3)在core-site.xml中增加支持LZO压缩</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">&lt;value&gt;</span><br><span class="line">org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>同步到hadoop112，hadoop113</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop112</span> <span class="type">hadoop</span>]<span class="variable">$</span> xsync core<span class="literal">-site</span>.xml</span><br></pre></td></tr></table></figure>
<p>(4)启动集群<br><font color=red><br>LZO压缩文件可切片性的特性依赖于启索引，所以，在分析LZO文件的时候我们都要手动为其创建索引<br></font></p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">ziyang</span>@<span class="type">hadoop112</span> <span class="type">module</span>]<span class="variable">$</span> hadoop jar /opt/module/hadoop<span class="literal">-2</span>.<span class="number">7.2</span>/share/hadoop/common/hadoop<span class="literal">-lzo</span><span class="literal">-0</span>.<span class="number">4.20</span>.jar  com.hadoop.compression.lzo.DistributedLzoIndexer 文件在HDFS上的路径</span><br></pre></td></tr></table></figure>

<h2 id="5-HDFS扩容配置"><a href="#5-HDFS扩容配置" class="headerlink" title="5.HDFS扩容配置"></a>5.HDFS扩容配置</h2><p>有需要留言更新</p>
<h2 id="6-HDFS基准测试"><a href="#6-HDFS基准测试" class="headerlink" title="6.HDFS基准测试"></a>6.HDFS基准测试</h2><p>此内容用于测试HDFS的读写性能，<strong>PC端请量力而行</strong>可以跳过<br>(1)测试HDFS写的性能<br>&emsp;测试内容：向HDFS集群写100个128M的文件</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop<span class="literal">-2</span>.<span class="number">7.2</span>/share/hadoop/mapreduce/hadoop<span class="literal">-mapreduce</span><span class="literal">-client</span><span class="literal">-jobclient</span><span class="literal">-2</span>.<span class="number">7.2</span><span class="literal">-tests</span>.jar TestDFSIO <span class="literal">-write</span> <span class="literal">-nrFiles</span> <span class="number">100</span> <span class="literal">-fileSize</span> <span class="number">128</span>MB</span><br></pre></td></tr></table></figure>
<p>(2)测试HDFS读的性能<br>emsp;测试内容：向HDFS集群读100个128M的文件</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop<span class="literal">-2</span>.<span class="number">7.2</span>/share/hadoop/mapreduce/hadoop<span class="literal">-mapreduce</span><span class="literal">-client</span><span class="literal">-jobclient</span><span class="literal">-2</span>.<span class="number">7.2</span><span class="literal">-tests</span>.jar TestDFSIO <span class="literal">-read</span> <span class="literal">-nrFiles</span> <span class="number">100</span> <span class="literal">-fileSize</span> <span class="number">128</span>MB</span><br></pre></td></tr></table></figure>
<p>(3)删除测试生成数据</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop<span class="literal">-2</span>.<span class="number">7.2</span>/share/hadoop/mapreduce/hadoop<span class="literal">-mapreduce</span><span class="literal">-client</span><span class="literal">-jobclient</span><span class="literal">-2</span>.<span class="number">7.2</span><span class="literal">-tests</span>.jar TestDFSIO <span class="literal">-clean</span></span><br></pre></td></tr></table></figure>
<h2 id="7-Hadoop参数调优"><a href="#7-Hadoop参数调优" class="headerlink" title="7.Hadoop参数调优"></a>7.Hadoop参数调优</h2><p>有需要留言更新</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase协处理器错误记录</title>
    <url>/2020/06/18/hbase/HBase%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2 id="2-Hbase2-0-协处理器失效问题"><a href="#2-Hbase2-0-协处理器失效问题" class="headerlink" title="2.Hbase2.0+协处理器失效问题"></a>2.Hbase2.0+协处理器失效问题</h2><p>再Hbase2.0+协处理器没用</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>Hbase2.0+移除了继承BaseRegionObserver，改为实现 RegionObserver, RegionCoprocessor<br>并要添加方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> Optional&lt;RegionObserver&gt; <span class="title">getRegionObserver</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> Optional.of(<span class="keyword">this</span>);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>


<h2 id="2-在将HBase添加协处理器时，出现错误"><a href="#2-在将HBase添加协处理器时，出现错误" class="headerlink" title="2.在将HBase添加协处理器时，出现错误"></a>2.在将HBase添加协处理器时，出现错误</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ERROR: org.apache.hadoop.hbase.DoNotRetryIOException: Class cn.bryce.textstuCoprocessor cannot be loaded Set hbase.table.sanity.checks to false at conf or table descriptor if you want to bypass sanity checks</span><br><span class="line">    at org.apache.hadoop.hbase.master.HMaster.warnOrThrowExceptionForFailure(HMaster.java:1707)</span><br><span class="line">    at org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1568)</span><br><span class="line">    at org.apache.hadoop.hbase.master.HMaster.modifyTable(HMaster.java:2055)</span><br><span class="line">    at org.apache.hadoop.hbase.master.MasterRpcServices.modifyTable(MasterRpcServices.java:1176)</span><br><span class="line">    at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:55680)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2170)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)</span><br><span class="line">    at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108)</span><br><span class="line">    at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><strong>重点</strong>1.首先检查协处理器代码是否写正确<br>2.再hdfs-site.xml添加配置信息</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.table.sanity.checks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>注意：若是协处理器代码的问题，将会导致下面的问题。</p>
<h2 id="3-HBase找不到协处理器导致RegionServer全部挂掉"><a href="#3-HBase找不到协处理器导致RegionServer全部挂掉" class="headerlink" title="3.HBase找不到协处理器导致RegionServer全部挂掉"></a>3.HBase找不到协处理器导致RegionServer全部挂掉</h2><h2 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h2><ol>
<li>修改HBase的默认配置<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--该配置会禁用协处理器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.coprocessor.abortonerror<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>找到出现问题的带有协处理器的表，del掉</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive关于开窗的小结</title>
    <url>/2020/01/12/hive/hive%E5%85%B3%E4%BA%8E%E5%BC%80%E7%AA%97%E7%9A%84%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Hive关于开窗的小结"><a href="#Hive关于开窗的小结" class="headerlink" title="Hive关于开窗的小结"></a>Hive关于开窗的小结</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p><strong>数据填写</strong></p>
<p>1001    zhangsan    male<br>1002    lisi    female<br>1003    wangwu    male<br>1004    zhaoliu    female<br>1005    xiaoming    male<br>1006    xiaohong    male</p>
<p>保存成over.txt文件存在~/data目录下</p>
<p><strong>表的建立</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table testOver(</span><br><span class="line">id string,</span><br><span class="line">name string,</span><br><span class="line">sex string)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure>

<p><strong>导入数据</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">load data local inpath &#39;&#x2F;home&#x2F;ziyang&#x2F;data&#x2F;over.txt&#39; into table testOver;</span><br></pre></td></tr></table></figure>

<p><strong>数据查询</strong></p>
<blockquote>
<p>testover.id    testover.name    testover.sex<br>1001    zhangsan    male<br>1002    lisi    female<br>1003    wangwu    male<br>1004    zhaoliu    female<br>1005    xiaoming    male<br>1006    xiaohong    male</p>
</blockquote>
<h2 id="Over函数"><a href="#Over函数" class="headerlink" title="Over函数"></a>Over函数</h2><p>相比于group，窗口函数会给每一条数据都会独立开一个窗口。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select count(*) over() from testOver;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>count_window_0<br>6<br>6<br>6<br>6<br>6<br>6</p>
</blockquote>
<p><strong>partition与order by</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select sex,count(*) over(partition by sex) from testOver;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>sex    count_window_0<br>female    2<br>female    2<br>male    4<br>male    4<br>male    4<br>male    4</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select id,sex,sum(id) over(partition by sex order by id) from testOver;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>id    sex    sum_window_0<br>1002    female    1002.0<br>1004    female    2006.0<br>1001    male    1001.0<br>1003    male    2004.0<br>1005    male    3009.0<br>1006    male    4015.0</p>
</blockquote>
<p>2.窗口函数当中所写的orderby，partition by的意义:</p>
<p>​    over中写的数据在于限定窗口大小<br>​    partition by：   限定当前窗口可活动的最大范围<br>​    order by：   最排序的字段区分活动范围</p>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>在企业中能不用窗口函数就不用窗口函数，窗口函数会给每一条数据单独增加一列。会很严重增加资源的损耗</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive常用的函数汇总</title>
    <url>/2020/09/08/hive/hive%E5%B8%B8%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<h1 id="Hive常用的函数汇总"><a href="#Hive常用的函数汇总" class="headerlink" title="Hive常用的函数汇总"></a>Hive常用的函数汇总</h1><p>有时候忽然发现想一个函数忘记了。所以准备写一篇关于hive的函数总结。也总来平时让自己复习复习吧。</p>
<h2 id="常用的日期函数"><a href="#常用的日期函数" class="headerlink" title="常用的日期函数"></a>常用的日期函数</h2><p><font color="#dd0000">时间默认格式为：YYYY-MM-dd：</font><br /><br>unix_timestamp:返回当前或指定时间的时间戳<br>from_unixtime：将时间戳转为日期格式<br>current_date：当前日期<br>current_timestamp：当前的日期加时间<br>to_date：抽取日期部分<br>year：获取年<br>month：获取月<br>day：获取日<br>hour：获取时<br>minute：获取分<br>second：获取秒<br>weekofyear：当前时间是一年中的第几周<br>dayofmonth：当前时间是一个月中的第几天<br>months_between： 两个日期间的月份<br>add_months：日期加减月<br>datediff：两个日期相差的天数<br>date_add：日期加天数<br>date_sub：日期减天数<br>last_day：日期的当月的最后一天</p>
<h2 id="常用的取整函数"><a href="#常用的取整函数" class="headerlink" title="常用的取整函数"></a>常用的取整函数</h2><p>round： 四舍五入<br>ceil：  向上取整<br>floor： 向下取整</p>
<h2 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h2><p>upper： 转大写<br>lower： 转小写<br>length： 长度<br>trim：  前后去空格<br>lpad： 向左补齐，到指定长度<br>rpad：  向右补齐，到指定长度<br>regexp_replace： SELECT regexp_replace(‘100-200’, ‘(\d+)’, ‘num’) ；使用正则表达式匹配目标字符串，匹配成功后替换！</p>
<h2 id="常用的集合操作"><a href="#常用的集合操作" class="headerlink" title="常用的集合操作"></a>常用的集合操作</h2><p>size： 集合中元素的个数<br>map_keys： 返回map中的key<br>map_values: 返回map中的value<br>array_contains: 判断array中是否包含某个元素<br>sort_array： 将array中的元素排序</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>记录一次mapJoin导致的副作用</title>
    <url>/2020/10/28/hive/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1mapJoin%E5%AF%BC%E8%87%B4%E7%9A%84%E5%89%AF%E4%BD%9C%E7%94%A8/</url>
    <content><![CDATA[<h1 id="记录一次mapJoin导致的副作用"><a href="#记录一次mapJoin导致的副作用" class="headerlink" title="记录一次mapJoin导致的副作用"></a>记录一次mapJoin导致的副作用</h1><p>写在前面的话:今天在优化一个sql的时候。发现了mapJoin导致了sql运行的异常缓慢。但是没有找到导致这个原因的地方。<br>也算是记录了一个坑吧。<br>优化时间：原先任务2小时——现在10分钟<br>由于涉及一些机密，代码中一些关键信息将被省略。</p>
<h2 id="原sql"><a href="#原sql" class="headerlink" title="原sql"></a>原sql</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">24</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> lyh_aaa_user_takelook_raw_tmp01;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> lyh_aaa_user_takelook_raw_tmp01 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">      a.id</span><br><span class="line">     ,a.prop_id</span><br><span class="line">     ,a.broker_id</span><br><span class="line">     ,to_date(a.confirm_takelook_time) <span class="keyword">as</span> view_prop_date</span><br><span class="line">     ,a.user_id</span><br><span class="line">     ,<span class="number">1</span> <span class="keyword">as</span> type</span><br><span class="line"><span class="keyword">from</span> dw_takelook_prop_order a</span><br><span class="line"><span class="keyword">where</span> a.cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> a.status <span class="operator">=</span><span class="number">4</span> <span class="keyword">and</span> a.order_type <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">      a.id</span><br><span class="line">     ,b.prop_id</span><br><span class="line">     ,a.broker_id</span><br><span class="line">     ,to_date(a.confirm_takelook_time) <span class="keyword">as</span> view_prop_date</span><br><span class="line">     ,a.user_id</span><br><span class="line">     ,<span class="number">1</span> <span class="keyword">as</span> type</span><br><span class="line"><span class="keyword">from</span> dw_takelook_prop_order a </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span>(</span><br><span class="line">    <span class="keyword">select</span> rowkey,prop_id</span><br><span class="line">    <span class="keyword">from</span>(</span><br><span class="line">        <span class="keyword">select</span> regexp_replace(rowkey,<span class="string">&#x27;looked_esf&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> rowkey,<span class="keyword">value</span></span><br><span class="line">        <span class="keyword">from</span> dw_user_tooklook_route_line </span><br><span class="line">        <span class="keyword">where</span> cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> colkey<span class="operator">=</span><span class="string">&#x27;router_looked_props&#x27;</span></span><br><span class="line">    )a <span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(split(a.value,<span class="string">&#x27;,&#x27;</span>)) b <span class="keyword">as</span> prop_id </span><br><span class="line">) b <span class="keyword">on</span> a.id<span class="operator">=</span>b.rowkey</span><br><span class="line"><span class="keyword">where</span> a.cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> a.status <span class="operator">=</span><span class="number">4</span> <span class="keyword">and</span> a.order_type<span class="operator">=</span><span class="number">2</span> </span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> lyh_aaa_user_takelook_raw_tmp02;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> lyh_aaa_user_takelook_raw_tmp02 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">     a.id</span><br><span class="line">    ,<span class="built_in">coalesce</span>(c.house_id,a.prop_id) <span class="keyword">as</span> prop_id</span><br><span class="line">    ,a.prop_id  <span class="keyword">as</span> prop_id_aaa</span><br><span class="line">    ,<span class="number">0</span>          <span class="keyword">as</span> prop_id_bbb</span><br><span class="line">    ,a.broker_id</span><br><span class="line">    ,a.view_prop_date</span><br><span class="line">    ,a.user_id</span><br><span class="line">    ,a.type</span><br><span class="line">    ,t.broker_evaluate_id</span><br><span class="line">    ,t.comm_evaluate_id</span><br><span class="line">    ,t.create_time</span><br><span class="line"><span class="keyword">from</span> lyh_aaa_user_takelook_raw_tmp01 a </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> dw_new_prop_base_esf c <span class="keyword">on</span> c.cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> a.prop_id<span class="operator">=</span>c.info_id_aaa</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">         take_look_id</span><br><span class="line">        ,regexp_replace(get_json_object(evaluate_relation,<span class="string">&#x27;$.broker_evaluate_id&#x27;</span>),<span class="string">&#x27;\\[|\\]&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> broker_evaluate_id</span><br><span class="line">        ,regexp_replace(get_json_object(evaluate_relation,<span class="string">&#x27;$.comm_evaluate_id&#x27;</span>),<span class="string">&#x27;\\[|\\]&#x27;</span>,<span class="string">&#x27;&#x27;</span>)   <span class="keyword">as</span> comm_evaluate_id</span><br><span class="line">        ,get_json_object(evaluate_relation,<span class="string">&#x27;$.create_time&#x27;</span>)                                     <span class="keyword">as</span> create_time</span><br><span class="line">        ,evaluate_relation</span><br><span class="line">    <span class="keyword">from</span>(</span><br><span class="line">        <span class="keyword">select</span> </span><br><span class="line">            regexp_replace(regexp_replace(id,<span class="string">&#x27;esf&#x27;</span>,<span class="string">&#x27;&#x27;</span>),<span class="string">&#x27;-2&#x27;</span>,<span class="string">&#x27;&#x27;</span>)                                     <span class="keyword">as</span> take_look_id</span><br><span class="line">            ,regexp_replace(regexp_replace(evaluate_relation,<span class="string">&#x27;\\[\\&#123;&#x27;</span>,<span class="string">&#x27;\\&#123;&#x27;</span>),<span class="string">&#x27;\\&#125;\\]&#x27;</span>,<span class="string">&#x27;\\&#125;&#x27;</span>)        <span class="keyword">as</span> evaluate_relation</span><br><span class="line">        <span class="keyword">from</span> dw_user_takelook_prop_order</span><br><span class="line">        <span class="keyword">where</span> cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> length(evaluate_relation)<span class="operator">&gt;</span><span class="number">0</span> <span class="keyword">and</span> id <span class="keyword">not</span> <span class="keyword">like</span> <span class="string">&#x27;%-2&#x27;</span></span><br><span class="line">    ) t</span><br><span class="line">)t <span class="keyword">on</span> a.id<span class="operator">=</span>t.take_look_id</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">     a.id</span><br><span class="line">    ,<span class="built_in">coalesce</span>(c.house_id,a.prop_id) <span class="keyword">as</span> prop_id</span><br><span class="line">    ,<span class="number">0</span>          <span class="keyword">as</span> prop_id_aaa</span><br><span class="line">    ,a.prop_id  <span class="keyword">as</span> prop_id_bbb</span><br><span class="line">    ,a.broker_id</span><br><span class="line">    ,to_date(a.confirm_takelook_time) <span class="keyword">as</span> view_prop_date</span><br><span class="line">    ,a.user_id</span><br><span class="line">    ,<span class="number">3</span> <span class="keyword">as</span> type</span><br><span class="line">    ,t.broker_evaluate_id</span><br><span class="line">    ,t.comm_evaluate_id</span><br><span class="line">    ,t.create_time</span><br><span class="line"><span class="keyword">from</span> dw_bbb_takelook_prop_order a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> dw_new_prop_base_esf c <span class="keyword">on</span> c.cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> a.prop_id<span class="operator">=</span>c.info_id_</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">         take_look_id</span><br><span class="line">        ,regexp_replace(get_json_object(evaluate_relation,<span class="string">&#x27;$.broker_evaluate_id&#x27;</span>),<span class="string">&#x27;\\[|\\]&#x27;</span>,<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> broker_evaluate_id</span><br><span class="line">        ,regexp_replace(get_json_object(evaluate_relation,<span class="string">&#x27;$.comm_evaluate_id&#x27;</span>),<span class="string">&#x27;\\[|\\]&#x27;</span>,<span class="string">&#x27;&#x27;</span>)   <span class="keyword">as</span> comm_evaluate_id</span><br><span class="line">        ,get_json_object(evaluate_relation,<span class="string">&#x27;$.create_time&#x27;</span>)                                     <span class="keyword">as</span> create_time</span><br><span class="line">        ,evaluate_relation</span><br><span class="line">    <span class="keyword">from</span>(</span><br><span class="line">        <span class="keyword">select</span> </span><br><span class="line">            regexp_replace(regexp_replace(rowkey ,<span class="string">&#x27;esf&#x27;</span>,<span class="string">&#x27;&#x27;</span>),<span class="string">&#x27;-2&#x27;</span>,<span class="string">&#x27;&#x27;</span>)  <span class="keyword">as</span> take_look_id</span><br><span class="line">            ,regexp_replace(regexp_replace(evaluate_relation,<span class="string">&#x27;\\[\\&#123;&#x27;</span>,<span class="string">&#x27;\\&#123;&#x27;</span>),<span class="string">&#x27;\\&#125;\\]&#x27;</span>,<span class="string">&#x27;\\&#125;&#x27;</span>)        <span class="keyword">as</span> evaluate_relation</span><br><span class="line">        <span class="keyword">from</span> dw__user_takelook_order </span><br><span class="line">        <span class="keyword">where</span> cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> length(evaluate_relation)<span class="operator">&gt;</span><span class="number">0</span> <span class="keyword">and</span> rowkey  <span class="keyword">not</span> <span class="keyword">like</span> <span class="string">&#x27;%-2&#x27;</span></span><br><span class="line">    ) t</span><br><span class="line">)t <span class="keyword">on</span> a.id<span class="operator">=</span>t.take_look_id</span><br><span class="line"><span class="keyword">where</span> a.cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> a.order_type <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>) <span class="keyword">and</span> a.status<span class="operator">=</span><span class="number">4</span></span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dm_aaa_user_takelook_detail_daily <span class="keyword">drop</span> if <span class="keyword">exists</span> <span class="keyword">partition</span>(cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">table</span> dm_aaa_user_takelook_detail_daily <span class="keyword">partition</span>(cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span>)</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">     a.id</span><br><span class="line">    ,a.prop_id</span><br><span class="line">    ,a.prop_id_aaa</span><br><span class="line">    ,a.prop_id_bbb</span><br><span class="line">    ,a.broker_id</span><br><span class="line">    ,a.view_prop_date</span><br><span class="line">    ,a.user_id</span><br><span class="line">    ,a.type</span><br><span class="line"><span class="keyword">from</span> lyh_aaa_user_takelook_raw_tmp02  a </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> dw_aaa_comm_dianping_daily b <span class="keyword">on</span> b.cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> a.comm_evaluate_id <span class="operator">=</span>b.id <span class="keyword">and</span> b.relate_type<span class="operator">=</span><span class="number">1</span> <span class="keyword">and</span> b.audit_status<span class="operator">=</span><span class="number">2</span> <span class="keyword">and</span> b.status<span class="operator">=</span><span class="number">2</span></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span>(</span><br><span class="line">  <span class="keyword">select</span> orderId</span><br><span class="line">  <span class="keyword">from</span>(</span><br><span class="line">  <span class="keyword">select</span> get_json_object(extra,<span class="string">&#x27;$.orderId&#x27;</span>) <span class="keyword">as</span> orderId</span><br><span class="line">  <span class="keyword">from</span> dw_t_take_look_prop </span><br><span class="line">  <span class="keyword">where</span> cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> `<span class="keyword">from</span>`<span class="operator">=</span><span class="number">2</span> <span class="keyword">and</span> check_status <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span> <span class="keyword">and</span> check_status <span class="keyword">in</span> (<span class="number">101</span>,<span class="number">201</span>)</span><br><span class="line">  )a <span class="keyword">group</span> <span class="keyword">by</span> orderId</span><br><span class="line">)c <span class="keyword">on</span> a.id<span class="operator">=</span>c.orderId</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span> id </span><br><span class="line">    <span class="keyword">from</span> da_broker_andromeda_chat_online_detail_daily</span><br><span class="line">    <span class="keyword">where</span> cal_dt<span class="operator">&gt;=</span><span class="string">&#x27;2020-01-02&#x27;</span> <span class="keyword">and</span> cal_dt<span class="operator">&lt;=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> is_effective<span class="operator">=</span><span class="number">0</span> <span class="keyword">and</span> biz<span class="operator">=</span><span class="string">&#x27;daikan-user&#x27;</span></span><br><span class="line">) d <span class="keyword">on</span> a.broker_evaluate_id<span class="operator">=</span>d.id </span><br><span class="line"><span class="keyword">where</span> (b.id <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span> <span class="keyword">or</span> c.orderId <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span>) <span class="keyword">and</span> ((to_date(a.create_time)<span class="operator">&gt;=</span><span class="string">&#x27;2020-01-02&#x27;</span> <span class="keyword">and</span> d.id <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span> ) <span class="keyword">or</span> to_date(a.create_time)<span class="operator">&lt;</span><span class="string">&#x27;2020-01-02&#x27;</span>)</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
<p>首先总体看一下这个sql。<br>最开始开启了mapJoin和设置了并行度，与并行运算。<br>其次使用了多张临时表去join。最后汇总成最终的表</p>
<h2 id="第一次优化-子查询"><a href="#第一次优化-子查询" class="headerlink" title="第一次优化(子查询)"></a>第一次优化(子查询)</h2><p>在观察第二张临时表的时候。<br>两次用到了 <code>dw_new_prop_base_esf</code>c这张表。<br>结合业务，这张表是一张大表。<br>所以在</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> dw_new_prop_base_esf c <span class="keyword">on</span> c.cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> a.prop_id<span class="operator">=</span>c.info_id_aaa</span><br></pre></td></tr></table></figure>
<p>在这个语句中。当join c表全表的时候，会让join速度异常慢，<br>1.对于这张表就需要改成子查询。<br>2.同时，c表的info_id_aaa会存在值为0的无效字段。所以，在子查询的时候需要优先过滤掉。<br>子查询sql</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        info_id_aaa,</span><br><span class="line">        house_id</span><br><span class="line">    <span class="keyword">from</span> dw_new_prop_base_esf </span><br><span class="line">    <span class="keyword">where</span> cal_dt<span class="operator">=</span><span class="string">&#x27;2020-10-28&#x27;</span> <span class="keyword">and</span> info_id_aaa <span class="operator">&gt;</span> <span class="number">0</span>)c <span class="keyword">on</span> a.prop_id<span class="operator">=</span>c.info_id_ajk</span><br></pre></td></tr></table></figure>
<h2 id="第二次优化"><a href="#第二次优化" class="headerlink" title="第二次优化"></a>第二次优化</h2><p>在优化完所有的子查询的时候。重新运行sql。发现sql并没有快多少。<br>马上去观察MR的日志。发现了一个很奇怪的地方</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO  CommandLogger - Execution completed successfully</span><br><span class="line">INFO  CommandLogger - MapredLocal task succeeded</span><br><span class="line">INFO  CommandLogger - Launching Job 4 out of 8</span><br><span class="line">INFO  CommandLogger - Number of reduce tasks is set to 0 since there&#39;s no reduce operator</span><br><span class="line">INFO  CommandLogger - Starting Job &#x3D; job_1591776234013_43039258</span><br><span class="line">INFO  CommandLogger - Kill Command &#x3D; &#x2F;usr&#x2F;lib&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop job  -kill job_1591776234013_43039258</span><br><span class="line">INFO  CommandLogger - Hadoop job information for Stage-15: number of mappers: 1; number of reducers: 0</span><br><span class="line">INFO  CommandLogger - 2020-10-27 15:14:49,951 Stage-15 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class="line">INFO  CommandLogger - 2020-10-27 15:15:50,827 Stage-15 map &#x3D; 0%,  reduce &#x3D; 0%, Cumulative CPU 6.65 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 15:16:51,080 Stage-15 map &#x3D; 0%,  reduce &#x3D; 0%, Cumulative CPU 6.65 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 15:17:51,423 Stage-15 map &#x3D; 0%,  reduce &#x3D; 0%, Cumulative CPU 6.65 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 15:18:51,683 Stage-15 map &#x3D; 0%,  reduce &#x3D; 0%, Cumulative CPU 6.65 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 15:19:52,681 Stage-15 map &#x3D; 0%,  reduce &#x3D; 0%, Cumulative CPU 6.65 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 15:20:53,670 Stage-15 map &#x3D; 0%,  reduce &#x3D; 0%, Cumulative CPU 6.65 sec</span><br><span class="line">...</span><br><span class="line">INFO  CommandLogger - 2020-10-27 15:21:54,578 Stage-15 map &#x3D; 0%,  reduce &#x3D; 0%, Cumulative CPU 413.41 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 16:19:41,349 Stage-15 map &#x3D; 87%,  reduce &#x3D; 0%, Cumulative CPU 3802.22 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 16:19:44,428 Stage-15 map &#x3D; 90%,  reduce &#x3D; 0%, Cumulative CPU 3805.13 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 16:20:45,037 Stage-15 map &#x3D; 92%,  reduce &#x3D; 0%, Cumulative CPU 3865.91 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 16:21:45,632 Stage-15 map &#x3D; 95%,  reduce &#x3D; 0%, Cumulative CPU 3926.83 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 16:22:46,214 Stage-15 map &#x3D; 97%,  reduce &#x3D; 0%, Cumulative CPU 3987.8 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-27 16:23:46,814 Stage-15 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 4048.62 sec</span><br><span class="line">INFO  CommandLogger - MapReduce Total cumulative CPU time: 0 days 1 hours 7 minutes 28 seconds 620 msec</span><br><span class="line">INFO  CommandLogger - Ended Job &#x3D; job_1591776234013_43039258</span><br></pre></td></tr></table></figure>
<p>这是最后一个任务的job。<br>可以看到这个stage，这个stage只启动了一个map，但是运行时长达到了一个多小时！！！！<br>这是一个大问题，去yarn的history去查看日志信息。<br>在查看日志的过程中，发现了这么一段非常奇怪的日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2020-10-27 15:21:07,969 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: FS[2]: records written - 1</span><br><span class="line">2020-10-27 15:21:07,969 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: MAP[4]: records read - 1</span><br><span class="line">2020-10-27 15:21:08,034 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: FS[2]: records written - 10</span><br><span class="line">2020-10-27 15:21:08,034 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: MAP[4]: records read - 10</span><br><span class="line">2020-10-27 15:21:08,684 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: FS[2]: records written - 100</span><br><span class="line">2020-10-27 15:21:08,684 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: MAP[4]: records read - 100</span><br><span class="line">2020-10-27 15:21:14,937 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: FS[2]: records written - 1000</span><br><span class="line">2020-10-27 15:21:14,937 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: MAP[4]: records read - 1000</span><br><span class="line">2020-10-27 15:22:17,695 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: FS[2]: records written - 10000</span><br><span class="line">2020-10-27 15:22:17,695 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: MAP[4]: records read - 10000</span><br><span class="line">2020-10-27 15:23:28,469 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000013_0</span><br><span class="line">2020-10-27 15:25:14,599 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000016_0</span><br><span class="line">2020-10-27 15:27:13,863 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000000_0</span><br><span class="line">2020-10-27 15:28:48,836 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: FS[2]: records written - 100000</span><br><span class="line">2020-10-27 15:28:48,836 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: MAP[4]: records read - 100000</span><br><span class="line">2020-10-27 16:07:14,335 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000007_0</span><br><span class="line">2020-10-27 16:08:25,129 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000017_0</span><br><span class="line">2020-10-27 16:09:26,163 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000008_0</span><br><span class="line">2020-10-27 16:10:28,783 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000010_0</span><br><span class="line">2020-10-27 16:11:32,083 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000005_0</span><br><span class="line">2020-10-27 16:12:32,526 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000018_0</span><br><span class="line">2020-10-27 16:13:34,346 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000014_0</span><br><span class="line">2020-10-27 16:14:35,463 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000002_0</span><br><span class="line">2020-10-27 16:15:36,420 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000009_0</span><br><span class="line">2020-10-27 16:16:36,473 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000019_0</span><br><span class="line">2020-10-27 16:17:38,098 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000006_0</span><br><span class="line">2020-10-27 16:18:39,803 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000012_0</span><br><span class="line">2020-10-27 16:18:42,826 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: FS[2]: records written - 1000000</span><br><span class="line">2020-10-27 16:18:42,826 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: MAP[4]: records read - 1000000</span><br><span class="line">2020-10-27 16:19:41,930 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000004_0</span><br><span class="line">2020-10-27 16:20:43,092 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000001_0</span><br><span class="line">2020-10-27 16:21:43,749 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000003_0</span><br><span class="line">2020-10-27 16:22:44,503 INFO [main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file viewfs:&#x2F;&#x2F;middata&#x2F;hive&#x2F;scratch&#x2F;1201b53e-40e8-47fa-a024-89d67e63792b&#x2F;hive_2020-10-27_14-56-52_380_1712864422965502457-1&#x2F;-mr-10004&#x2F;000011_0</span><br><span class="line">2020-10-27 16:23:45,720 INFO [main] org.apache.hadoop.hive.ql.exec.MapOperator: 4 finished. closing...</span><br></pre></td></tr></table></figure>
<p>这一步是将这个表往缓存中写。一共写入了进100W条数据，用时1小时！！<br>结合HDFS看到这个主表的大小大概在70MB左右。但是因为是临时表，所以字段比较少。而mapjoin机制要将这表写进内存中。从而耗费了大量的时间。<br>所以在最后一个任务前，将MapJoin关闭，能节省大量的时间</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">false</span>;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>JAVA几道基础常见的面试题</title>
    <url>/2019/06/13/java/JAVA%E5%87%A0%E9%81%93%E5%B8%B8%E8%A7%81%E7%9A%84%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<h1 id="JAVA几道常见的面试题"><a href="#JAVA几道常见的面试题" class="headerlink" title="JAVA几道常见的面试题"></a>JAVA几道常见的面试题</h1><h2 id="Java自带哪几种线程池？"><a href="#Java自带哪几种线程池？" class="headerlink" title="Java自带哪几种线程池？"></a>Java自带哪几种线程池？</h2><ol>
<li>newCachedThreadPool<br>创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。这种类型的线程池特点是：<br>工作线程的创建数量几乎没有限制（其实也有限制的，数目为Interger. MAX_VALUE）, 这样可灵活的往线程池中添加线程。<br>如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间（默认为1分钟），则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。<br>在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪。</li>
<li>newFixedThreadPool<br>创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但是，在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。</li>
<li>newSingleThreadExecutor<br>创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序（FIFO, LIFO, 优先级）执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。</li>
<li>newScheduleThreadPool<br>创建一个定长的线程池，而且支持定时的以及周期性的任务执行，支持定时及周期性任务执行。延迟3秒执行。</li>
</ol>
<h2 id="HashMap和HashTable区别"><a href="#HashMap和HashTable区别" class="headerlink" title="HashMap和HashTable区别"></a>HashMap和HashTable区别</h2><ol>
<li>线程安全性不同<br>HashMap是线程不安全的，HashTable是线程安全的，其中的方法是Synchronize的，在多线程并发的情况下，可以直接使用HashTabl，但是使用HashMap时必须自己增加同步处理。</li>
<li>是否提供contains方法<br>HashMap只有containsValue和containsKey方法；HashTable有contains、containsKey和containsValue三个方法，其中contains和containsValue方法功能相同。</li>
<li>key和value是否允许null值<br>Hashtable中，key和value都不允许出现null值。HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。</li>
<li>数组初始化和扩容机制<br>HashTable在不指定容量的情况下的默认容量为11，而HashMap为16，Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。<br>Hashtable扩容时，将容量变为原来的2倍加1，而HashMap扩容时，将容量变为原来的2倍。</li>
</ol>
<h2 id="TreeSet和HashSet区别"><a href="#TreeSet和HashSet区别" class="headerlink" title="TreeSet和HashSet区别"></a>TreeSet和HashSet区别</h2><p>HashSet是采用hash表来实现的。其中的元素没有按顺序排列，add()、remove()以及contains()等方法都是复杂度为O(1)的方法。<br>TreeSet是采用树结构实现（红黑树算法）。元素是按顺序进行排列，但是add()、remove()以及contains()等方法都是复杂度为O(log (n))的方法。它还提供了一些方法来处理排序的set，如first()，last()，headSet()，tailSet()等等。</p>
<h2 id="String-buffer和String-build区别"><a href="#String-buffer和String-build区别" class="headerlink" title="String buffer和String build区别"></a>String buffer和String build区别</h2><ol>
<li>StringBuffer与StringBuilder中的方法和功能完全是等价的。</li>
<li>只是StringBuffer中的方法大都采用了 synchronized 关键字进行修饰，因此是线程安全的，而StringBuilder没有这个修饰，可以被认为是线程不安全的。 </li>
<li>在单线程程序下，StringBuilder效率更快，因为它不需要加锁，不具备多线程安全而StringBuffer则每次都需要判断锁，效率相对更低</li>
</ol>
<h2 id="Final、Finally、Finalize"><a href="#Final、Finally、Finalize" class="headerlink" title="Final、Finally、Finalize"></a>Final、Finally、Finalize</h2><p>final：修饰符（关键字）有三种用法：修饰类、变量和方法。修饰类时，意味着它不能再派生出新的子类，即不能被继承，因此它和abstract是反义词。修饰变量时，该变量使用中不被改变，必须在声明时给定初值，在引用中只能读取不可修改，即为常量。修饰方法时，也同样只能使用，不能在子类中被重写。<br>finally：通常放在try…catch的后面构造最终执行代码块，这就意味着程序无论正常执行还是发生异常，这里的代码只要JVM不关闭都能执行，可以将释放外部资源的代码写在finally块中。<br>finalize：Object类中定义的方法，Java中允许使用finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在销毁对象时调用的，通过重写finalize() 方法可以整理系统资源或者执行其他清理工作。</p>
]]></content>
      <categories>
        <category>java</category>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>java读取配置文件的几种方式</title>
    <url>/2019/06/15/java/java%E8%AF%BB%E5%8F%96%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="java读取配置文件的几种方式"><a href="#java读取配置文件的几种方式" class="headerlink" title="java读取配置文件的几种方式"></a>java读取配置文件的几种方式</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取Properties的方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GetProperties</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 1。使用java.util.Properties 类的load()方法</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 示例：</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String  <span class="title">getProperties_1</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">		InputStream in = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filePath)));</span><br><span class="line"></span><br><span class="line">		Properties p = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">		p.load(in);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> p.getProperty(<span class="string">&quot;username&quot;</span>);</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2。使用java.util.ResourceBundle类的getBundle()方法</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 示例：</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getProperties_2</span><span class="params">(String url)</span> </span>&#123;</span><br><span class="line">		ResourceBundle rb = ResourceBundle.getBundle(url);</span><br><span class="line">		Enumeration&lt;String&gt; keys = rb.getKeys();</span><br><span class="line">		<span class="keyword">while</span> (keys.hasMoreElements())&#123;</span><br><span class="line"></span><br><span class="line">			System.out.println(rb.getString(keys.nextElement()));</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 3。使用java.util.PropertyResourceBundle类的构造函数</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 示例：</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getProperties_3</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">		InputStream in = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(filePath));</span><br><span class="line"></span><br><span class="line">		ResourceBundle rb = <span class="keyword">new</span> PropertyResourceBundle(in);</span><br><span class="line">		Enumeration&lt;String&gt; keys = rb.getKeys();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span> (keys.hasMoreElements())&#123;</span><br><span class="line"></span><br><span class="line">			System.out.println(rb.getString(keys.nextElement()));</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 4。使用class.getClassLoader()所得到的java.lang.ClassLoader的getResourceAsStream()方法</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 示例：</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getProperties_4</span><span class="params">(String url)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">		InputStream in = GetProperties.class.getClassLoader().getResourceAsStream(url);</span><br><span class="line"></span><br><span class="line">		Properties p = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">		p.load(in);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> p.getProperty(<span class="string">&quot;username&quot;</span>);</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 5。使用java.lang.ClassLoader类的getSystemResourceAsStream()静态方法</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 示例：</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getProperties_5</span><span class="params">(String url)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">		InputStream in = ClassLoader.getSystemResourceAsStream(url);</span><br><span class="line"></span><br><span class="line">		Properties p = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">		p.load(in);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> p.getProperty(<span class="string">&quot;username&quot;</span>);</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 补充</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// Servlet中可以使用javax.servlet.ServletContext的getResourceAsStream()方法</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 示例：</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// public static Properties getProperties_8(String url) &#123;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// InputStream in = context.getResourceAsStream(url);</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// Properties p = new Properties ();</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// p.load(in);</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// &#125;</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		<span class="comment">//需要将文件放置到项目根目录下面</span></span><br><span class="line">		System.out.println(getProperties_1(<span class="string">&quot;E:\\liruoyin\\mysql_durid\\src\\main\\resources\\prop.properties&quot;</span>));</span><br><span class="line">		System.out.println(<span class="string">&quot;----------&quot;</span>);</span><br><span class="line">		<span class="comment">//需要将文件放置到class文件同级目录下面</span></span><br><span class="line">		getProperties_2(<span class="string">&quot;prop&quot;</span>);</span><br><span class="line">		System.out.println(<span class="string">&quot;----------&quot;</span>);</span><br><span class="line">		getProperties_3(<span class="string">&quot;E:\\liruoyin\\mysql_durid\\src\\main\\resources\\prop.properties&quot;</span>);</span><br><span class="line">		System.out.println(<span class="string">&quot;----------&quot;</span>);</span><br><span class="line">		System.out.println(getProperties_4(<span class="string">&quot;prop.properties&quot;</span>));</span><br><span class="line">		System.out.println(<span class="string">&quot;----------&quot;</span>);</span><br><span class="line">		System.out.println(getProperties_5(<span class="string">&quot;prop.properties&quot;</span>));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>关于java中的重写</title>
    <url>/2019/08/23/java/%E5%85%B3%E4%BA%8Ejava%E4%B8%AD%E7%9A%84%E9%87%8D%E5%86%99/</url>
    <content><![CDATA[<h1 id="关于java中的重写"><a href="#关于java中的重写" class="headerlink" title="关于java中的重写"></a>关于java中的重写</h1><p>首先贴一段简单的代码:看一下运行的结果</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.bryce.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Java05_overwrite</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		User2 user = <span class="keyword">new</span> User2();</span><br><span class="line">		System.out.println(<span class="string">&quot;user = &quot;</span> + user.sum());</span><br><span class="line">		Person user2 = <span class="keyword">new</span> User2();</span><br><span class="line">		System.out.println(<span class="string">&quot;user2 = &quot;</span> + user2.sum());</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">int</span> i = <span class="number">10</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sum</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> i + <span class="number">10</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User2</span> <span class="keyword">extends</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">int</span> i = <span class="number">20</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sum</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> i + <span class="number">20</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>选中看答案<br>问题一：user = <font color=#fffff>40 </font><br>问题二：user2 = <font color=#fffff>40 </font></p>
<hr>
<p>如果两题都对了，那就再看看下面的</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.bryce.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Java05_overwrite</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		Person user3 = <span class="keyword">new</span> User2();</span><br><span class="line">		System.out.println(<span class="string">&quot;user3 = &quot;</span> + user3.sum());</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">int</span> i = <span class="number">10</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sum</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> i + <span class="number">10</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User2</span> <span class="keyword">extends</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">int</span> i = <span class="number">20</span>;</span><br><span class="line"><span class="comment">//	public int sum()&#123;</span></span><br><span class="line"><span class="comment">//		return i + 20;</span></span><br><span class="line"><span class="comment">//	&#125;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>问题三：user3 = <font color=#fffff>20 </font></p>
<p><strong>方法的重写：</strong><br>1.存在父子类关系，（子类重写父类的方法）<br>2.如果子类和父类都有相同的方法，那么遵循动态绑定机制<br><img src="https://img-blog.csdnimg.cn/20200525210516621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyeWNlX0xvc2tp,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p><strong>动态绑定机制</strong>：再程序的执行过程中，如果调用了对象的<strong>成员方法</strong>，那么会将对象的方法和对象实际的内存绑定。<strong>与属性无关</strong> 也就是说。</p>
</blockquote>
<p>Java05_overwrite再调用sum方法时，会先去寻找User中的sum方法，没有再去寻找Person中的方法。但是对于其中的属性，就不遵循动态绑定机制。也就是那个方法调用，就用那个类中的属性。</p>
<hr>
<p>By the way</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.bryce.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Java05_overwrite</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		Person user4 = <span class="keyword">new</span> User2();</span><br><span class="line">		System.out.println(<span class="string">&quot;user4 = &quot;</span> + user4.sum());</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">int</span> i = <span class="number">10</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sum</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> getI() + i;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getI</span> <span class="params">()</span></span>&#123; <span class="keyword">return</span> i; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User2</span> <span class="keyword">extends</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">int</span> i = <span class="number">20</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getI</span> <span class="params">()</span></span>&#123; <span class="keyword">return</span> i; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>问题四：user4 = <font color=#fffff>30 </font></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>重写</tag>
      </tags>
  </entry>
  <entry>
    <title>关于sleep与wait的思考</title>
    <url>/2019/07/14/java/%E5%85%B3%E4%BA%8Esleep%E4%B8%8Ewait%E7%9A%84%E6%80%9D%E8%80%83/</url>
    <content><![CDATA[<h1 id="关于sleep与wait的思考"><a href="#关于sleep与wait的思考" class="headerlink" title="关于sleep与wait的思考"></a>关于sleep与wait的思考</h1><h2 id="写在前面的话"><a href="#写在前面的话" class="headerlink" title="写在前面的话"></a>写在前面的话</h2><p>一说到sleep和wait的区别，最先想到是的<br>1.时间    sleep可以设置时间，wait<strong>一般不设置时间</strong>需要唤醒<br>2.对象锁    sleep不释放对象锁，而wait释放对象锁</p>
<h2 id="sleep和wait的核心区别"><a href="#sleep和wait的核心区别" class="headerlink" title="sleep和wait的核心区别"></a>sleep和wait的核心区别</h2><p><strong>sleep方法是静态的</strong><br>** wait是成员方法**</p>
<h3 id="sleep"><a href="#sleep" class="headerlink" title="sleep"></a>sleep</h3><p>关于sleep在java源码中只有简单的一句话</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">sleep</span><span class="params">(<span class="keyword">long</span> millis)</span> <span class="keyword">throws</span> InterruptedException</span>;</span><br></pre></td></tr></table></figure>
<p>从这句话看到sleep是静态的，<br><strong>静态方法和类型相关，和成员无关</strong><br>所以说，sleep方法属于谁调用就去休眠谁。<br>下面看这样一段代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaSleep</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Thread t1 = <span class="keyword">new</span> Thread();</span><br><span class="line">t1.start;</span><br><span class="line">t1.sleep(<span class="number">1000</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们发出一个疑问,在这段代码中休眠的是谁？<br>如果你能理解上面那句话，这里就会知道，上面的那段sleep与t1没有任何关系！<br>因为t1是一个对象，而sleep又与对象无关。所以sleep不可能让t1休眠。<br><strong>那么，这段代码中的sleep让谁休眠了？</strong><br>这个sleep是让当前正在执行的线程休眠，哪一个线程调用了sleep，休会休眠那个线程。<br><strong>所以，这个sleep会让main休眠</strong></p>
<h3 id="wait"><a href="#wait" class="headerlink" title="wait"></a>wait</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">wait</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        wait(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>wait并没有用static修饰，所以wait是对象方法没有对象有关。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaWait</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Thread t2 = <span class="keyword">new</span> Thread();</span><br><span class="line">        t2.start;</span><br><span class="line">        t2.wait;</span><br></pre></td></tr></table></figure>
<p>所以这段代码中的wait会让t2去等待</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>sleep&amp;wait</tag>
      </tags>
  </entry>
  <entry>
    <title>权限访问clone的思考</title>
    <url>/2019/06/15/java/%E6%9D%83%E9%99%90%E8%AE%BF%E9%97%AEclone%E7%9A%84%E6%80%9D%E8%80%83/</url>
    <content><![CDATA[<p><strong>clone</strong>：这个方法估计都不陌生。object种的方法。</p>
<p><img src="https://img-blog.csdnimg.cn/20200525200317218.png#pic_center" alt="clone方法"><br>这个是object中的一个方法，我们都知道，任何类都会继承object这个类。<br>首先提出一个问题：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Java_Access</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Object user = <span class="keyword">new</span> User();</span><br><span class="line">		user.clone();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个clone的方法会报错吗？<br>如果你的答案时肯定的，且清晰的知道为什么，就可以愉快的跳过下面的内容。<br>如果你不知道会不会报错。那么就接着往下看。</p>
<p>答案：这个在编译的时候就会出错。</p>
<hr>
<p>分析：</p>
<p>这里就涉及到clone方法访问的权限：<br>protected:同类,同包或者子类<br>当我们访问一个方法的时候一共需要遵循两个原则：<br><strong>1.方法的提供者是否符合要求<br>2.方法的调用者是否符合要求</strong><br>首先抛出两个问题：</p>
<blockquote>
<p>1.clone方法的调用者是否是User<br>2.User所继承的父类Object是不是Java_Access的父类</p>
</blockquote>
<p>在这个方法中以clone为例：<br>①：方法的提供者：java.lang.Object<br>方法的调用者：xxx.xxx.<strong>Java_Access</strong><br>这里方法的调用者并不是User。</p>
<p><strong>而user.clone()的意思完整的来说：Java_Access的main方法中，创建了一个User对象，并且调用了user对象的clone方法。</strong><br>所以，clone并不符合同类或者同包的概念。</p>
<hr>
<p>②：这里clone方法的提供者Object和Java_Access并没有父子关系</p>
<p><img src="https://img-blog.csdnimg.cn/20200525203147371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyeWNlX0xvc2tp,size_16,color_FFFFFF,t_70#pic_center" alt="clone的图片"><br>这里用一张图来表示。java_Access和User都有一个名叫Object的父类，但是这两个Object的之间并没有意点关系。所以，Java_Access并不能调用User父类Object中clone的方法。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>clone</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka注意事项与优化手段</title>
    <url>/2020/05/18/kafka/kafka%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%E4%B8%8E%E4%BC%98%E5%8C%96%E6%89%8B%E6%AE%B5/</url>
    <content><![CDATA[<h1 id="kafka注意事项与优化手段"><a href="#kafka注意事项与优化手段" class="headerlink" title="kafka注意事项与优化手段"></a>kafka注意事项与优化手段</h1><p>kafka在大数据领域是一个很重要的框架。几乎随便什么地方都能看得到kafka的身影。所以，今天就来整理下kafka在生产配置时候的一些需要注意的地方。</p>
<h2 id="1-1-kafka机器数量"><a href="#1-1-kafka机器数量" class="headerlink" title="1.1 kafka机器数量"></a>1.1 kafka机器数量</h2><p>kafka机器数量=2<em>（峰值生产速度</em>副本数/100）+1<br>这是一个经验公式。<br>先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署Kafka的数量。<br>比如我们的峰值生产速度是50M/s。副本数为2。<br>Kafka机器数量=2<em>（50</em>2/100）+ 1=3台</p>
<h2 id="1-2副本数量的设定"><a href="#1-2副本数量的设定" class="headerlink" title="1.2副本数量的设定"></a>1.2副本数量的设定</h2><p>一般我们设置为2个或者3个，很多企业设置为2个。<br>一方面kafka的数据基本都会当天消耗完毕，还有备份数据在上游都会有一定量的备份。所以kafka的副本数量设置3个没有必要。<br>副本优势：提高可靠性。副本劣势：增加了网络IO传输</p>
<h2 id="1-3-Kafka压测"><a href="#1-3-Kafka压测" class="headerlink" title="1.3 Kafka压测"></a>1.3 Kafka压测</h2><p>Kafka官方自带压力测试脚本。<br>Kafka压测时，可以查看到那个地方出现了瓶颈(CPU,内存,网络IO)。一般都是网络IO达到瓶颈。</p>
<h3 id="1-3-1-Kafka-Producer压力测试"><a href="#1-3-1-Kafka-Producer压力测试" class="headerlink" title="1.3.1 Kafka Producer压力测试"></a>1.3.1 Kafka Producer压力测试</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh  --topic <span class="built_in">test</span> --record-size 1024 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br></pre></td></tr></table></figure>
<p>说明：<br>record-size是一条信息有多大，单位是字节。<br>num-records是总共发送多少条信息。<br>throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。</p>
<p><strong>Kafka会打印下面的信息</strong></p>
<blockquote>
<p>100000 records sent, 95877.277085 records/sec (<strong>9.14 MB/sec</strong>), 187.68 ms avg latency, 424.00 ms max latency, 155 ms 50th, 411 ms 95th, 423 ms 99th, 424 ms 99.9th.</p>
</blockquote>
<p>参数解析：本例中一共写入10w条消息，吞吐量为9.14 MB/sec，每次写入的平均延迟为187.68毫秒，最大的延迟为424.00毫秒。</p>
<h3 id="1-3-2-Kafka-Consumer压力测试"><a href="#1-3-2-Kafka-Consumer压力测试" class="headerlink" title="1.3.2 Kafka Consumer压力测试"></a>1.3.2 Kafka Consumer压力测试</h3><p>Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic <span class="built_in">test</span> --fetch-size 10000 --messages 10000000 --threads 1</span><br></pre></td></tr></table></figure>
<p>参数说明：<br>–zookeeper 指定zookeeper的链接信息<br>–topic 指定topic的名称<br>–fetch-size 指定每次fetch的数据的大小<br>–messages 总共要消费的消息个数</p>
<p>测试结果说明：</p>
<blockquote>
<p>start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec<br>2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714, 100010, 21722.4153</p>
</blockquote>
<p>开始测试时间，测试结束数据，共消费数据9.5368MB，吞吐量2.0714MB/s，共消费100010条，平均每秒消费21722.4153条。</p>
<h2 id="1-4-Kafka日志保存时间"><a href="#1-4-Kafka日志保存时间" class="headerlink" title="1.4 Kafka日志保存时间"></a>1.4 Kafka日志保存时间</h2><p>默认保存为7天，但是生产环境下一般保存3天节省磁盘空间</p>
<h2 id="1-5-Kafka中数据量计算"><a href="#1-5-Kafka中数据量计算" class="headerlink" title="1.5 Kafka中数据量计算"></a>1.5 Kafka中数据量计算</h2><p>每天数据总量Ng，每天产生日志数量M条。<br>平均每秒中：M/24/60/60<br>低谷每秒钟：1/10的平均值<br>高峰时间：2-20倍平均值<br>每条日志大小:0.5k-2k<br>每秒数据量:M/24/60/60/1024MB</p>
<h2 id="1-6Kafka的硬盘大小"><a href="#1-6Kafka的硬盘大小" class="headerlink" title="1.6Kafka的硬盘大小"></a>1.6Kafka的硬盘大小</h2><p>每天的数据量<em>2个副本</em>3天/70%</p>
<h2 id="1-7-Kafka监控"><a href="#1-7-Kafka监控" class="headerlink" title="1.7 Kafka监控"></a>1.7 Kafka监控</h2><p>开源的监控器：KafkaManager、KafkaMonitor、KafkaEagle</p>
<h2 id="1-8-Kafka分区数设置规则"><a href="#1-8-Kafka分区数设置规则" class="headerlink" title="1.8 Kafka分区数设置规则"></a>1.8 Kafka分区数设置规则</h2><ol>
<li>创建只有一个分区的topic</li>
<li>测试这个topic的producer的吞吐量与consumer的吞吐量</li>
<li>假设他们的值分别是Tp和Tc，单位MB/S</li>
<li>分区数=期望吞吐量/Min（Tp,Tc）<br>分区数一般设置为3-10个</li>
</ol>
<h2 id="1-9-Kafka设置多少个Topic"><a href="#1-9-Kafka设置多少个Topic" class="headerlink" title="1.9 Kafka设置多少个Topic"></a>1.9 Kafka设置多少个Topic</h2><p>通常情况下：多少个日志类型就设置多少个Topic。也有对日志类型进行合并</p>
<h2 id="1-10-Kafka分区分配策略设置"><a href="#1-10-Kafka分区分配策略设置" class="headerlink" title="1.10 Kafka分区分配策略设置"></a>1.10 Kafka分区分配策略设置</h2><p>Kafka有两种分配策略，一是roundrobin，一是range。</p>
<p>partition.assignment.strategy参数选择 range 或 roundrobin</p>
<h2 id="1-11-Kafka-Ack应答机制设置"><a href="#1-11-Kafka-Ack应答机制设置" class="headerlink" title="1.11 Kafka Ack应答机制设置"></a>1.11 Kafka Ack应答机制设置</h2><p>ack :<br>        0   ：发送过来数据就不管了，   效率高、  可靠性最差<br>        1   ：发送过来的数据leader应答， 效率中等、可靠性中等<br>        -1  ：发送过来的数据leader和follower共同应答。 效率最低、可靠性最高</p>
<p>如果是金融公司或者对钱准确度要求比较高，选-1<br>如果是普通的日志，丢几条数据无所谓，选择1</p>
<h2 id="1-12Kafka数据积压，Kafka消费能力不足的时候怎么处理"><a href="#1-12Kafka数据积压，Kafka消费能力不足的时候怎么处理" class="headerlink" title="1.12Kafka数据积压，Kafka消费能力不足的时候怎么处理"></a>1.12Kafka数据积压，Kafka消费能力不足的时候怎么处理</h2><ol>
<li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提高消费者的数量。注：消费者数=分区数</li>
<li>如果是下游的数据吹不及时：提高每批次的拉去数量。批次拉去数据过少，是的处理的数据小于生产的数据，也会造成数据积压。<h2 id="1-13Kafka的参数优化"><a href="#1-13Kafka的参数优化" class="headerlink" title="1.13Kafka的参数优化"></a>1.13Kafka的参数优化</h2></li>
<li>Broker参数配置（server.properties）<br>1、日志保留策略配置<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保留三天，也可以更短 （log.cleaner.delete.retention.ms）</span></span><br><span class="line">log.retention.hours=72</span><br></pre></td></tr></table></figure>
2、Replica相关配置<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">default.replication.factor:1 默认副本1个</span><br></pre></td></tr></table></figure>
3、网络通信延时<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">replica.socket.timeout.ms:30000 <span class="comment">#当集群之间网络不稳定时,调大该参数</span></span><br><span class="line">replica.lag.time.max.ms= 600000<span class="comment"># 如果网络不好,或者kafka集群压力较大,会出现副本丢失,然后会频繁复制副本,导致集群压力更大,此时可以调大该参数</span></span><br></pre></td></tr></table></figure>
2）Producer优化（producer.properties）<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">compression.type:none                 gzip  snappy  lz4  </span><br><span class="line"><span class="comment">#默认发送不进行压缩，推荐配置一种适合的压缩算法，可以大幅度的减缓网络压力和Broker的存储压力。</span></span><br></pre></td></tr></table></figure>
3）Kafka内存调整（kafka-server-start.sh）<br>默认内存1个G，生产环境尽量不要超过6个G。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-Xms4g -Xmx4g&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="1-14Kafka单条日志传输大小"><a href="#1-14Kafka单条日志传输大小" class="headerlink" title="1.14Kafka单条日志传输大小"></a>1.14Kafka单条日志传输大小</h2>kafka对于消息体的大小默认为单条最大值是1M但是在我们应用场景中, 常常会出现一条消息大于1M，如果不对kafka进行配置。则会出现生产者无法将消息推送到kafka或消费者无法去消费kafka里面的数据, 这时我们就要对kafka进行以下配置：server.properties<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">replica.fetch.max.bytes: 1048576  broker可复制的消息的最大字节数, 默认为1M</span><br><span class="line">message.max.bytes: 1000012   kafka 会接收单个消息size的最大限制， 默认为1M左右</span><br></pre></td></tr></table></figure>
注意：message.max.bytes必须小于等于replica.fetch.max.bytes，否则就会导致replica之间数据同步失败。</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>kylin构建原理与优化</title>
    <url>/2020/07/07/kylin/kylin%E6%9E%84%E5%BB%BA%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<h2 id="写在前面的话"><a href="#写在前面的话" class="headerlink" title="写在前面的话"></a>写在前面的话</h2><p>kylin是一个华人写的即席查询的框架。所以有中文的社区。对于英语困难户来说真的是天大的福利了。一站式安装到底真舒服~~~~~~~~~~~~~~</p>
<p>所以本文就介绍下Kylin的底层架构原理与优化吧。</p>
<h1 id="Kylin-Cube的构建原理"><a href="#Kylin-Cube的构建原理" class="headerlink" title="Kylin Cube的构建原理"></a>Kylin Cube的构建原理</h1><h2 id="维度与度量"><a href="#维度与度量" class="headerlink" title="维度与度量"></a>维度与度量</h2><p>在介绍Cube之前首先需要了解维度与度量的概念</p>
<h3 id="维度"><a href="#维度" class="headerlink" title="维度"></a>维度</h3><p>在数仓构建中，维度是一个很重要的指标，就是<font color=red>观察数据的角度</font><br>维度是一组离散的值，比如性别中男和女，时间中的每一个时间点。所以在统计时，也通常将多种维度聚合在一起分析。</p>
<h3 id="度量"><a href="#度量" class="headerlink" title="度量"></a>度量</h3><p><font color=red>被聚合（观察）的统计值，也就是聚合运算的结果</font>，度量值也就是维度的客观反映结果，如性别为男的员工的个数。</p>
<h2 id="Cube与Cuboid"><a href="#Cube与Cuboid" class="headerlink" title="Cube与Cuboid"></a>Cube与Cuboid</h2><p>有了维度与度量，一个数据表或者数据模型上所有的字段都可以分类了。所以，根据维度与度量值就可以做出与计算的Cube理论。<br>所以，给定一个模型，我们就可以对上面所有的维度进行聚合。对于一个N维来说，就有2<sup>n</sup>种。对于每一种组合，将度量值进行聚合计算，然后将计算结果保存为一个物化视图，就称为<font color=red>Cuboid</font>。所有的Cuboid作为一个整体称为<font color=red>Cube</font>。<br><img src="https://images2015.cnblogs.com/blog/399159/201603/399159-20160303170336627-705310588.png" alt="UA19eJ.png"><br>上面是一个简单的例子：假设这个电商项目的销售数据集，其中维度包括：时间[time]、商品[item]、地区[location]和供应商[supplier]，度量值为销售额。那么所有的维度组合一共有2<sup>4</sup>=16种<br>零维：1种<br>一维：[time]、[item]、[location]和[supplier]4种<br>二维：[time, item]、[time, location]、[time, supplier]、[item, location]、[item, supplier]、[location, supplier]6种<br>三维：[time,item,location]、[time,item,supplier]、[time,location,supplier]、[,item,location,supplier]4种<br>四维：[time,item,location,supplier]1种<br><font color=red>这16种组合称为Cuboid，16个Cuboid整体为一个Cube</font></p>
<h2 id="Cube的储存原理"><a href="#Cube的储存原理" class="headerlink" title="Cube的储存原理"></a>Cube的储存原理</h2><p>为了节省存储空间，Cube的存储采用维度字典表的形式进行存储。<br><img src="https://i.loli.net/2020/07/07/Jy85k7GzCut3FbU.png" alt="Kylin储存原理1.png"><br>以上是一个标准的表字段，但是在Cube中存储的时候是将Cube拆开来，以每个维度作为一个块单独存储，维度的每一个字段作为K，分配value值。<br>在对维度进行聚合时，就取value值作为RowKey<br><img src="https://i.loli.net/2020/07/07/ZFvGMlJY6BxcIRd.png" alt="Kylin储存原理2.png"><br>对于RowKey的设计，将所有维度值作为Row的前半段。按顺序，如果用到了就位1，没有用到就为0.后半段是每个维度所对应的纬度值。<br>value就储存度量值。<br>所以在以上的表格，如果统计北京地区，2019-01-09电子产品的销售值。对应的RowKey的设计就为：111000</p>
<h2 id="Cube构建算法"><a href="#Cube构建算法" class="headerlink" title="Cube构建算法"></a>Cube构建算法</h2><p>Cube构建算法一共有两种，一种为逐层构建算法，另一种为快速构建算法。而Kylin会根据集群资源与表结构情况自动选取对应的构建算法。<br>当然可以通过修改配置文件<code>kylin.cube.algorithm</code>进行自行选择参数值可选 <code>auto</code>，<code>layer</code> 和<code> inmem</code>， 默认值为 <code>auto</code></p>
<h3 id="逐层构建算法-layer"><a href="#逐层构建算法-layer" class="headerlink" title="逐层构建算法(layer)"></a>逐层构建算法(layer)</h3><p><img src="http://lizhenchao.oss-cn-shenzhen.aliyuncs.com/1552552014.png"><br>逐层构建算法的算例就是按照维度层数一层一层的减少来计算，如上图，从4维开始构建一个MR，在减去一个维度，基于4维的计算结果，构建4个三维的MR，再根据4个三维的计算结果，构建6个二维的MR，最后构建4个一维的MR。<br>每一轮的计算都是一个MapReduce任务，且串行执行；一个N维的Cube，至少需要N次MapReduce Job。<br>算法优点：1、代码逻辑清晰，易于维护<br>                    2、当集群资源紧张，或者计算量特别庞大时，也能保证能够最终完成计算</p>
<p>算法缺点：1、当Cube有特别多的维度时，反复提交MR会浪费很多资源</p>
<p>​                    2、每一次MR都会伴随着一次shuffle的操作，所以导致效率低下</p>
<p>​                    3、对HDFS的读写操作较多：由于每一层计算的输出会用做下一层计算的输入，这些Key-Value需要写到HDFS上；当所有计算都完成后，Kylin还需要额外的一轮任务将这些文件转成HBase的HFile格式，以导入到HBase中去；</p>
<h3 id="快速构建算法-inmem"><a href="#快速构建算法-inmem" class="headerlink" title="快速构建算法(inmem)"></a>快速构建算法(inmem)</h3><p><img src="https://img-blog.csdn.net/20180814123615682?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2M5Mjk4MzM2MjNsdmNoYQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"><br>也被称作“逐段”(By Segment) 或“逐块”(By Split) 算法，从1.5.x开始引入该算法，该算法的主要思想是，每个Mapper将其所分配到的数据块，计算成一个完整的小Cube 段（包含所有Cuboid，如有4个维度时在Mapper端就会计算出16个Cuboid）。每个Mapper将计算完的Cube段输出给Reducer做合并，生成大Cube，也就是最终结果。<br>优点：取消了大量的shuffle，同时在mapper端利用内存做了预聚合。<br>缺点：会消耗大量的资源去计算</p>
<h2 id="Kylin-Cube的构建优化"><a href="#Kylin-Cube的构建优化" class="headerlink" title="Kylin Cube的构建优化"></a>Kylin Cube的构建优化</h2><h3 id="使用衍生维度（derived-dimension）"><a href="#使用衍生维度（derived-dimension）" class="headerlink" title="使用衍生维度（derived dimension）"></a>使用衍生维度（derived dimension）</h3><p>衍生维度用于在有效维度内将维度表上的非主键维度排除掉，并使用维度表的主键（其实是事实表上相应的外键）来替代它们。Kylin会在底层记录维度表主键与维度表其他维度之间的映射关系，以便在查询时能够动态地将维度表的主键“翻译”成这些非主键维度，并进行实时聚合。<br><img src="https://i.loli.net/2020/07/07/K7ENPJ5p3GQjknF.png" alt="衍生维度.png"></p>
<h3 id="使用聚合组（Aggregation-group）"><a href="#使用聚合组（Aggregation-group）" class="headerlink" title="使用聚合组（Aggregation group）"></a>使用聚合组（Aggregation group）</h3><ol>
<li>强制维度<br>如果一个维度被定义为强制维度，那么这个分组产生的所有Cuboid中每一个Cuboid都会包含该维度。每个分组中都可以有0个、1个或多个强制维度。如果根据这个分组的业务逻辑，则相关的查询一定会在过滤条件或分组条件中，因此可以在该分组中把该维度设置为强制维度。</li>
<li>层级维度<br>每个层级包含两个或更多个维度。假设一个层级中包含D1，D2…Dn这n个维度，那么在该分组产生的任何Cuboid中， 这n个维度只会以（），（D1），（D1，D2）…（D1，D2…Dn）这n+1种形式中的一种出现。每个分组中可以有0个、1个或多个层级，不同的层级之间不应当有共享的维度。如果根据这个分组的业务逻辑，则多个维度直接存在层级关系，因此可以在该分组中把这些维度设置为层级维度。</li>
<li>联合维度<br>每个联合中包含两个或更多个维度，如果某些列形成一个联合，那么在该分组产生的任何Cuboid中，这些联合维度要么一起出现，要么都不出现。每个分组中可以有0个或多个联合，但是不同的联合之间不应当有共享的维度（否则它们可以合并成一个联合）。如果根据这个分组的业务逻辑，多个维度在查询中总是同时出现，则可以在该分组中把这些维度设置为联合维度。<br><img src="https://i.loli.net/2020/07/07/8lq7rgkLdMwXDyU.png" alt="聚合组.png"><h3 id="Row-Key优化"><a href="#Row-Key优化" class="headerlink" title="Row Key优化"></a>Row Key优化</h3>Kylin会把所有的维度按照顺序组合成一个完整的Rowkey，并且按照这个Rowkey升序排列Cuboid中所有的行。<br>设计良好的Rowkey将更有效地完成数据的查询过滤和定位，减少IO次数，提高查询速度，维度在rowkey中的次序，对查询性能有显著的影响。<br>Row key的设计原则如下：</li>
<li>被用作过滤的维度放在前边。<br><img src="https://i.loli.net/2020/07/07/xS46eDLGqdnaWJs.png" alt="过滤维度.png"></li>
<li>基数大的维度放在基数小的维度前边。<br><img src="https://i.loli.net/2020/07/07/NEVpzmZQOXxwkWI.png" alt="基数设置.png"><h3 id="并发粒度优化"><a href="#并发粒度优化" class="headerlink" title="并发粒度优化"></a>并发粒度优化</h3>当Segment中某一个Cuboid的大小超出一定的阈值时，系统会将该Cuboid的数据分片到多个分区中，以实现Cuboid数据读取的并行化，从而优化Cube的查询速度。具体的实现方式如下：构建引擎根据Segment估计的大小，以及参数“kylin.hbase.region.cut”的设置决定Segment在存储引擎中总共需要几个分区来存储，如果存储引擎是HBase，那么分区的数量就对应于HBase中的Region数量。kylin.hbase.region.cut的默认值是5.0，单位是GB，也就是说对于一个大小估计是50GB的Segment，构建引擎会给它分配10个分区。用户还可以通过设置kylin.hbase.region.count.min（默认为1）和kylin.hbase.region.count.max（默认为500）两个配置来决定每个Segment最少或最多被划分成多少个分区。</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>kylin</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title>shell脚本执行jps时：-bash: jps: command not found</title>
    <url>/2020/02/15/linux/command%20not%20found/</url>
    <content><![CDATA[<p>我构建了hadoop集群。我们一定会写一个shell脚本去每一个节点上去jps，查看每个节点的进程情况。</p>
<p>原先以为shell很简单：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment">#查看每个节点运行情况</span></span><br><span class="line"><span class="keyword">for</span>((host=<span class="number">101</span>;host&lt;<span class="number">108</span>;host++));<span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> ----------<span class="literal">-hadoop</span><span class="variable">$host</span>-------------</span><br><span class="line">	ssh hadoop<span class="variable">$host</span> <span class="string">&quot;jps&quot;</span></span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>这里默认服务器节点的名字是hadoop101-hadoop107</strong><br><strong>默认已经配置了ssh_key的公钥和私钥</strong><br>看是运行这段程序的时候会弹出一个错误：</p>
<h2 id="bash-jps-command-not-found"><a href="#bash-jps-command-not-found" class="headerlink" title="-bash: jps: command not found"></a>-bash: jps: command not found</h2><p>错误原因：<br>在shell脚本写的ssh到其他节点的时候默认是不加载配置文件的。linux并不能去找到java中jps的命令。</p>
<ul>
<li><strong>解决方案一</strong>：在ssh到其他节点的时候source 一下配置文件。<br>具体操作为：ssh hadoop$host “source /etc/profile;jps”</li>
<li><strong>解决方案二</strong>：在ssh到其他节点的时候输入jps命令下的绝对路径。<br>在笔者的linux的jdk的绝对路径为：/opt/module/jdk1.8.0_144/bin这个目录下就有jps的命令。<br>具体操作为：ssh hadoop$host “/opt/module/jdk1.8.0_144/bin/jps”</li>
<li><strong>解决方案三</strong>：在当前用户的家目录中输入命令<strong>ll -a</strong>会显示隐藏文件，修改配置文件.bashrc。<br>.bashrc 是当你登入shell时执行<br>在其中添加JDK的环境变量</li>
<li><strong>解决方案四</strong>：在/etc/profile.d目录下创建一个以sh结尾的文件。将profile配的环境变量拷贝到这个文件目录下</li>
</ul>
<p>运行结果：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">----------<span class="literal">-hadoop101</span>-------------</span><br><span class="line"><span class="number">10720</span> DataNode</span><br><span class="line"><span class="number">10993</span> NodeManager</span><br><span class="line"><span class="number">10573</span> NameNode</span><br><span class="line"><span class="number">19663</span> Jps</span><br><span class="line">----------<span class="literal">-hadoop102</span>-------------</span><br><span class="line"><span class="number">37744</span> Jps</span><br><span class="line"><span class="number">31282</span> NodeManager</span><br><span class="line"><span class="number">31154</span> ResourceManager</span><br><span class="line"><span class="number">31036</span> DataNode</span><br><span class="line">----------<span class="literal">-hadoop103</span>-------------</span><br><span class="line"><span class="number">30725</span> NodeManager</span><br><span class="line"><span class="number">30620</span> SecondaryNameNode</span><br><span class="line"><span class="number">30511</span> DataNode</span><br><span class="line"><span class="number">41135</span> Jps</span><br><span class="line">----------<span class="literal">-hadoop104</span>-------------</span><br><span class="line"><span class="number">30995</span> DataNode</span><br><span class="line"><span class="number">31109</span> NodeManager</span><br><span class="line"><span class="number">37483</span> Jps</span><br><span class="line">----------<span class="literal">-hadoop105</span>-------------</span><br><span class="line"><span class="number">30882</span> NodeManager</span><br><span class="line"><span class="number">30766</span> DataNode</span><br><span class="line"><span class="number">37358</span> Jps</span><br><span class="line">----------<span class="literal">-hadoop106</span>-------------</span><br><span class="line"><span class="number">8816</span> Jps</span><br><span class="line"><span class="number">2592</span> NodeManager</span><br><span class="line"><span class="number">2477</span> DataNode</span><br><span class="line">----------<span class="literal">-hadoop107</span>-------------</span><br><span class="line"><span class="number">37445</span> Jps</span><br><span class="line"><span class="number">31035</span> DataNode</span><br><span class="line"><span class="number">31151</span> NodeManager</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>groupByKey与ReduceByKey的区别</title>
    <url>/2020/05/23/spark/groupByKey%E4%B8%8EReduceByKey%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h1 id="groupByKey与ReduceByKey的区别"><a href="#groupByKey与ReduceByKey的区别" class="headerlink" title="groupByKey与ReduceByKey的区别"></a>groupByKey与ReduceByKey的区别</h1><p><strong>groupByKey</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure>
<p><strong>ReduceByKey</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>
<p>groupByKey面向的是整个数据集<br>但是无论是groupByKey还是ReduceByKey都会对数据进行一次shuffle操作。</p>
<h2 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h2><p>groupByKey对一个分区的数据进行分组后就不会执行后续操作，要等待所有的分区数据全部到达后，才会进行后续操作。<br>在早期，shuffle是在内存中进行的，但是，很快发现，如果shuffle在内存中进行很容易出现内存溢出的情况。所以这个等待的情况应该被安排在磁盘中进行</p>
<h2 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h2><p>reduceByKey方法可以在shuffle之前进行分区内的聚合操作，称之为预聚和，这样，shuffle时落盘的数据量就减少了，提高了shuffle的性能</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>算子</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark任务运行流程(基于yarn集群模式)源码分析（1）</title>
    <url>/2020/06/01/spark/spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(1)/</url>
    <content><![CDATA[<h1 id="Spark任务运行流程-基于yarn集群模式-源码分析（1）"><a href="#Spark任务运行流程-基于yarn集群模式-源码分析（1）" class="headerlink" title="Spark任务运行流程(基于yarn集群模式)源码分析（1）"></a>Spark任务运行流程(基于yarn集群模式)源码分析（1）</h1><p> <strong>写在前面的话</strong><br>  本文通过通俗易懂的方式，将以spark的yarn集群模式，通过源码层面去分析spark的任务调度流程。因为源码量巨大，所以只分析调度任务时所经历的主要流程。<br>  <strong>注：阅读前需要具备一点点scala基础</strong></p>
<h2 id="1-1-Spark核心组件"><a href="#1-1-Spark核心组件" class="headerlink" title="1.1 Spark核心组件"></a>1.1 Spark核心组件</h2><ul>
<li>Driver<br>Spark的驱动器节点，用于执行spark的main方法，负责实际代码的执行工作。<br>主要责任有：</li>
</ul>
<ol>
<li>将用户程序转化为作业(job)</li>
<li>在Executor之间调度任务(Task)</li>
<li>跟踪Executor的执行情况</li>
<li>通过UI展示查询运行情况</li>
</ol>
<ul>
<li> Executor<br>Spark Executor节点是负责在Spark作业中运行具体的任务，任务彼此之间互相独立。Spark应用启动时，Executor加点被同时启动，并且始终伴随着整个Spark用用的生命周期。<br>Executor自动启动HA机制，当某个节点运行故障，Spark应用会自动将出错节点上的任务调度到其他的节点上继续执行。<br>Executor的核心功能：</li>
</ul>
<ol>
<li>负责运行组成Spark的应用任务，并将结果返回Driver端</li>
<li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li>
</ol>
<ul>
<li>Spark通用运行流程概述<br><img src="https://s1.ax1x.com/2020/06/19/NuiIyT.png" alt="NuiIyT.png"><br>上图体现spark基本的运行流程：</li>
</ul>
<ol>
<li>在任务提交以后，Saprk会先启动Driver程序</li>
<li>随后Driver分出两条线，</li>
<li>1 一条从集群管理器去注册应用程序，因为有这条线，Spark才能与yarn结合。实现可插拔。</li>
<li>2 另一条执行main函数。  Spark的查询为懒执行，当执行到Action算子时开始反向推算，根据宽依赖进行Stage的划分，随后每一个Stage对于一个Taskset。</li>
<li>Task分发到执行的Executor去执行。<h1 id="Spark在yarn的提交流程"><a href="#Spark在yarn的提交流程" class="headerlink" title="Spark在yarn的提交流程"></a>Spark在yarn的提交流程</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2>准备两个依赖<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-yarn_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.4.5&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.4.5&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>
<h2 id="2-1-指令执行入口"><a href="#2-1-指令执行入口" class="headerlink" title="2.1 指令执行入口"></a>2.1 指令执行入口</h2>在yarn运行模式下。spark会通过一个指令，向yarn提交任务。本文以官方案例，spark PI为例：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">./examples/jars/spark-examples_2.12-2.4.5.jar \</span><br><span class="line">10</span><br><span class="line"></span><br></pre></td></tr></table></figure>
从指令中可以看到，这个指令式执行spark-submit，传入参数。<br><img src="https://s1.ax1x.com/2020/06/19/NKFNqO.png" alt="NKFNqO.png"><br>用文本文件打开这个文件，里面只有不到30行代码。主要带代码在最后一句。这个指令主要执行<br>文件bin目录下的spark-class的org.apache.spark.deploy.SparkSubmit类</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br></pre></td></tr></table></figure>
<p>打开spark-class可以找到<br><img src="https://s1.ax1x.com/2020/06/19/NKFcsf.png" alt="NKFcsf.png"><br>从最后一行开始。这个文件执行了</p>
<ol>
<li>CMD参数。</li>
<li>CMD参数用过while循环，将build_command封装进CMD</li>
<li>build_command配置了JVM的内存，前面的Runner封装了java的配置。<br><img src="https://s1.ax1x.com/2020/06/19/NKFHyV.png" alt="NKFHyV.png"><br>所以通过拼接，最后得出的指令为：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/java -Xmx128m -cp org.apache.spark.deploy.SparkSubmit</span><br></pre></td></tr></table></figure>
这条指令通过java的指定去启动一个进程，这个进程为spark-submit。<h2 id="2-2-SparkSubmit"><a href="#2-2-SparkSubmit" class="headerlink" title="2.2 SparkSubmit"></a>2.2 SparkSubmit</h2>通过上面这条指令。可以得出，sparkSubmit这个类中一定有一个<strong>main</strong>方法作为方法的入口，去执行。所以在idea中去寻找这个类，的伴生类对象。<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> submit = <span class="keyword">new</span> <span class="type">SparkSubmit</span>() &#123;</span><br><span class="line">      self =&gt;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">parseArguments</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">SparkSubmitArguments</span> = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args) &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = self.logInfo(msg)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = self.logWarning(msg)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = printMessage(msg)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = printMessage(<span class="string">s&quot;Warning: <span class="subst">$msg</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doSubmit</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">super</span>.doSubmit(args)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">SparkUserAppException</span> =&gt;</span><br><span class="line">            exitFn(e.exitCode)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    submit.doSubmit(args)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
进入main方法。可以看到main方法中前面所有的方法都是在声明类信息。只有最后一行去调用了SparkSubmit的doSubmit方法，并将args作为参数传入。</li>
</ol>
<p><strong>目前的结构是：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SparkSubmit</span><br><span class="line"></span><br><span class="line">    -- main</span><br><span class="line">    </span><br><span class="line">        &#x2F;&#x2F; 执行提交</span><br><span class="line">        &#x2F;&#x2F; args表示应用程序的命令行参数</span><br><span class="line">        -- submit.doSubmit(args)</span><br></pre></td></tr></table></figure>
<h3 id="2-2-1-doSubmit"><a href="#2-2-1-doSubmit" class="headerlink" title="2.2.1 doSubmit"></a>2.2.1 doSubmit</h3><p>点击进入doSubmit<br><img src="https://s1.ax1x.com/2020/06/19/NKkPOK.png" alt="NKkPOK.png"><br>代码量很少，关键的代码只有</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val appArgs &#x3D; parseArguments(args)</span><br></pre></td></tr></table></figure>
<p>从字面意思，这句话对传入的args进行参数的解析。</p>
<h4 id="2-2-1-1-parseArguments"><a href="#2-2-1-1-parseArguments" class="headerlink" title="2.2.1.1 parseArguments"></a>2.2.1.1 parseArguments</h4><p>进入 <strong>parseArguments -&gt; SparkSubmitArguments</strong><br><img src="https://s1.ax1x.com/2020/06/19/NKkMOf.png" alt="NKkMOf.png"><br>可以看到很熟悉的东西。这个类将各种参数进行封装，封装成自己的类。<br>在下文可以看到处理的过程。但是这个并不是本文的主要内容所以略过。<br><img src="https://s1.ax1x.com/2020/06/19/NKkYfs.png" alt="NKkYfs.png"><br>在本类中可以找到一个handle的方法：<br><img src="https://s1.ax1x.com/2020/06/19/NKkUlq.png" alt="NKkUlq.png"><br>这个方法通过模式匹配对命令行的参数进行了封装。如：<br>–class org.apache.spark.examples.SparkPi <br>–master yarn <br>将 org.apache.spark.examples.SparkPi =&gt; master<br>将yarn -&gt; master<br>所以目前的结构是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SparkSubmit</span><br><span class="line"></span><br><span class="line">    -- main</span><br><span class="line">    </span><br><span class="line">        &#x2F;&#x2F; 执行提交</span><br><span class="line">        &#x2F;&#x2F; args表示应用程序的命令行参数</span><br><span class="line">        -- submit.doSubmit(args)</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; 解析命令行参数</span><br><span class="line">        &#x2F;&#x2F; --master : yarn            &#x3D;&gt; master</span><br><span class="line">        &#x2F;&#x2F; --class  : xxxxx.WordCount &#x3D;&gt; mainClass</span><br><span class="line">        -- parseArguments</span><br></pre></td></tr></table></figure>
<p>sparkSubmit执行<strong>main</strong> -&gt; <strong>dosubmit</strong>  -&gt;  <strong>parseArguments</strong> -&gt;** new SparkSubmitArguments** -&gt; 通过<strong>handle</strong>方法，将命令行的各个参数传入到了spark程序中。</p>
<h4 id="2-2-1-2-action"><a href="#2-2-1-2-action" class="headerlink" title="2.2.1.2 action"></a>2.2.1.2 action</h4><p>返回doSubmit层，往下看可以看到一个action方法：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">    appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs, uninitLog)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">PRINT_VERSION</span> =&gt; printVersion()</span><br><span class="line">    &#125;</span><br><span class="line">````</span><br><span class="line">通过代码可以得知appargs通过模式匹配执行了action，由于没有<span class="keyword">case</span> _的选项，所以action一定在某个地方被赋值了。</span><br><span class="line">点击进入action所在的**<span class="type">SparkSubmitArguments</span>**，可以找到这样一行代码：</span><br><span class="line">![<span class="type">NKkB0U</span>.png](https:<span class="comment">//s1.ax1x.com/2020/06/19/NKkB0U.png)</span></span><br><span class="line">通过一定的scala基础，可以分析出action在这里被赋值上了<span class="type">SUBMIT</span></span><br><span class="line">最后action走的是</span><br><span class="line"></span><br><span class="line">```scala</span><br><span class="line"><span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs, uninitLog)</span><br></pre></td></tr></table></figure>
<p>这一行代码。</p>
<h3 id="2-2-2-submit-appArgs-uninitLog"><a href="#2-2-2-submit-appArgs-uninitLog" class="headerlink" title="2.2.2 submit(appArgs, uninitLog)"></a>2.2.2 submit(appArgs, uninitLog)</h3><p>进入action的submit函数<br><img src="https://s1.ax1x.com/2020/06/19/NKk40O.png" alt="NKk40O.png"><br>里面有一个doRunMain函数，由于没有调用这个函数，所以这个函数不被执行，先跳过，看下面的if-else方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (args.isStandaloneCluster &amp;&amp; args.useRest) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      logInfo(<span class="string">&quot;Running Spark using the REST application submission protocol.&quot;</span>)</span><br><span class="line">      doRunMain()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">// Fail over to use the legacy submission gateway</span></span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">SubmitRestConnectionException</span> =&gt;</span><br><span class="line">        logWarning(<span class="string">s&quot;Master endpoint <span class="subst">$&#123;args.master&#125;</span> was not a REST server. &quot;</span> +</span><br><span class="line">          <span class="string">&quot;Falling back to legacy submission gateway instead.&quot;</span>)</span><br><span class="line">        args.useRest = <span class="literal">false</span></span><br><span class="line">        submit(args, <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">// In all other modes, just run the main class as prepared</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    doRunMain()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>由于本文是基于yarn部署环境。所以直接查看else中的doRunMain方法</p>
<h4 id="2-2-2-1-doRunMain"><a href="#2-2-2-1-doRunMain" class="headerlink" title="2.2.2.1 doRunMain"></a>2.2.2.1 doRunMain</h4><p>doRunMain方法中，if函数判断是否有代理用户，当前执行命令并没有设定，所以执行else中的<strong>runMain</strong>方法</p>
<h3 id="2-2-3-runMain"><a href="#2-2-3-runMain" class="headerlink" title="2.2.3 runMain"></a>2.2.3 runMain</h3><p><img src="https://s1.ax1x.com/2020/06/19/NKkb9A.png" alt="NKkb9A.png"><br>这个函数中开始准备做提交的内容。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)</span><br></pre></td></tr></table></figure>
<p>这个函数关键信息也在第一句，准备提交环境。</p>
<p>加下来：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//设定类加载器。</span></span><br><span class="line"> <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//加载和获取指定名称的类信息</span></span><br><span class="line"> mainClass = <span class="type">Utils</span>.classForName(childMainClass)</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//动态(反射)创建对象</span></span><br><span class="line"> mainClass.newInstance().asInstanceOf[<span class="type">SparkApplication</span>]</span><br><span class="line"> <span class="comment">//同时在之后的代码调用start方法</span></span><br><span class="line"> app.start(childArgs.toArray, sparkConf)</span><br></pre></td></tr></table></figure>
<p><strong>小结回顾</strong><br>进入start方法，会发现start是一个抽象类。<br>所以，代码一定在之前就已经被实现。<br>回顾app方法，是从<strong>mainClass.newInstance().asInstanceOf[SparkApplication]<strong>这个方法过来，<br>mainClass是从</strong>mainClass = Utils.classForName(childMainClass)<strong>得到的类信息<br>childMainClass是</strong>val (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)<strong>准备提交环境的时候返回的结果。<br>所以以上的代码就连成了串，关键在于</strong>prepareSubmitEnvironment</strong></p>
<h3 id="2-2-4-prepareSubmitEnvironment"><a href="#2-2-4-prepareSubmitEnvironment" class="headerlink" title="2.2.4 prepareSubmitEnvironment"></a>2.2.4 prepareSubmitEnvironment</h3><ol>
<li>进入 prepareSubmitEnvironment方法<br><img src="https://s1.ax1x.com/2020/06/19/NKk7hd.png" alt="NKk7hd.png"><br>这是一个代码量非常大的函数</li>
<li>直接看末尾处，**748行处(childArgs, childClasspath, sparkConf, childMainClass)**这是函数的返回结果，从返回结果往上推，能找到赋值的地方。</li>
<li>查找<strong>childMainClass</strong>跟着提示往上走，会找到这样一个代码段<br><img src="https://s1.ax1x.com/2020/06/19/NKAi3n.png" alt="NKAi3n.png"><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">childMainClass = <span class="type">YARN_CLUSTER_SUBMIT_CLASS</span></span><br></pre></td></tr></table></figure>
这里是Yarn的cluster模式，进入YARN_CLUSTER_SUBMIT_CLASS可以看到完整的类对象为<strong>org.apache.spark.deploy.yarn.YarnClusterApplication</strong></li>
<li>继续查找会看到client模式的代码<br><img src="https://s1.ax1x.com/2020/06/19/NKAP9s.png" alt="NKAP9s.png"><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">childMainClass = args.mainClass</span><br></pre></td></tr></table></figure>
而mainClass在前面已经提到为–class的值<br>所以目前的结构是：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SparkSubmit</span><br><span class="line"></span><br><span class="line">    -- main</span><br><span class="line">    </span><br><span class="line">        &#x2F;&#x2F; 执行提交</span><br><span class="line">        &#x2F;&#x2F; args表示应用程序的命令行参数</span><br><span class="line">        -- submit.doSubmit(args)</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; 解析命令行参数</span><br><span class="line">        &#x2F;&#x2F; --master : yarn            &#x3D;&gt; master</span><br><span class="line">        &#x2F;&#x2F; --class  : xxxxx.WordCount &#x3D;&gt; mainClass</span><br><span class="line">        -- parseArguments</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F; </span><br><span class="line">        -- submit</span><br><span class="line">            &#x2F;&#x2F; 执行主程序</span><br><span class="line">            -- doRunMain</span><br><span class="line">            </span><br><span class="line">                &#x2F;&#x2F; 执行主程序</span><br><span class="line">                -- runMain</span><br><span class="line">                </span><br><span class="line">                    &#x2F;&#x2F; childArgs</span><br><span class="line">                    &#x2F;&#x2F; childClasspath</span><br><span class="line">                    &#x2F;&#x2F; sparkConf</span><br><span class="line">                    &#x2F;&#x2F; childMainClass</span><br><span class="line">                    &#x2F;&#x2F; 准备提交的环境</span><br><span class="line">                    -- prepareSubmitEnvironment(args)</span><br><span class="line">                    </span><br><span class="line">                        &#x2F;&#x2F; client        &#x3D;&gt; xxxxx.WordCount</span><br><span class="line">                        &#x2F;&#x2F; isYarnCluster &#x3D;&gt; org.apache.spark.deploy.yarn.YarnClusterApplication</span><br><span class="line">                        -- (childArgs, childClasspath, sparkConf, childMainClass)</span><br><span class="line">                    </span><br><span class="line">                    &#x2F;&#x2F; 设定类加载器</span><br><span class="line">                    -- Thread.currentThread.setContextClassLoader(loader)</span><br><span class="line">                    </span><br><span class="line">                    &#x2F;&#x2F; 加载和获取指定名称的类信息</span><br><span class="line">                    -- mainClass &#x3D; Utils.classForName(childMainClass)</span><br><span class="line">                    </span><br><span class="line">                    &#x2F;&#x2F; 动态(反射)创建对象</span><br><span class="line">                    -- app &#x3D; mainClass.newInstance().asInstanceOf[SparkApplication]</span><br><span class="line">                    </span><br><span class="line">                    -- app.start</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>到此SparkSubmit就已经结束</strong></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark任务运行流程(基于yarn集群模式)源码分析（2）</title>
    <url>/2020/06/01/spark/spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(2)/</url>
    <content><![CDATA[<h1 id="Spark任务运行流程-基于yarn集群模式-源码分析（2）"><a href="#Spark任务运行流程-基于yarn集群模式-源码分析（2）" class="headerlink" title="Spark任务运行流程(基于yarn集群模式)源码分析（2）"></a>Spark任务运行流程(基于yarn集群模式)源码分析（2）</h1><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>前文的sparkSubmit阶段已经结束。可以看到yarn集群最后是调用了command(Cluster) = bin/java org.apache.spark.deploy.yarn.ApplicationMaster<br>在源码中搜索 org.apache.spark.deploy.yarn.ApplicationMaster</p>
<h2 id="2-3-YarnClusterApplication"><a href="#2-3-YarnClusterApplication" class="headerlink" title="2.3 YarnClusterApplication"></a>2.3 YarnClusterApplication</h2><p><img src="https://s1.ax1x.com/2020/06/19/NKlq3T.png" alt="NKlq3T.png"><br>从图中可以看到一共有三个参数，<br>ClientArguments<br>Client<br>run</p>
<ol>
<li>new ClientArguments(args）<br>进入这个类很容易可以看到之前的命令参数再次被封装起来。</li>
<li>new Client<br><img src="https://s1.ax1x.com/2020/06/19/NK3EWV.png" alt="NK3EWV.png"><br>这里可以看到<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> yarnClient = <span class="type">YarnClient</span>.createYarnClient</span><br></pre></td></tr></table></figure>
这里Client创建了一个yarn的客户端。<br>所以在当前的环境中应该已经包含了yarn的服务器，ResourceManager与NodeManager</li>
<li>run<br><img src="https://s1.ax1x.com/2020/06/19/NK3X79.png" alt="NK3X79.png"><br>this.appId = submitApplication()此方法用于提交应用</li>
</ol>
<ul>
<li>进入submitApplication<br><img src="https://s1.ax1x.com/2020/06/19/NK85HH.png" alt="NK85HH.png"><br>这里初始化了yarnClient客户端，与启动了yarnClient。使sparkSubmit能与yarn开始通信<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//179行</span></span><br><span class="line">      <span class="keyword">val</span> containerContext = createContainerLaunchContext(newAppResponse)</span><br><span class="line">      <span class="keyword">val</span> appContext = createApplicationSubmissionContext(newApp, containerContext)</span><br><span class="line"></span><br><span class="line"> <span class="comment">//184行</span></span><br><span class="line"> yarnClient.submitApplication(appContext)</span><br></pre></td></tr></table></figure>
开始向yarnClient提交应用，参数appContext为提交的内容<br>createContainerLaunchContext:主要封装JVM的参数，与封装ApplicationMaster的参数<br>createApplicationSubmissionContext：主要封装框架的参数信息</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">YarnClusterApplication</span></span><br><span class="line">    -- start</span><br><span class="line">    </span><br><span class="line">        <span class="comment">// 封装参数</span></span><br><span class="line">        <span class="comment">// --class xxxxx.WordCount</span></span><br><span class="line">        -- <span class="keyword">new</span> <span class="type">ClientArguments</span></span><br><span class="line">    </span><br><span class="line">        -- <span class="keyword">new</span> <span class="type">Client</span></span><br><span class="line">            <span class="comment">// 建立和RM之间的联系</span></span><br><span class="line">            -- yarnClient = <span class="type">YarnClient</span>.createYarnClient</span><br><span class="line">        -- run</span><br><span class="line">            <span class="comment">// 提交应用</span></span><br><span class="line">            -- submitApplication</span><br><span class="line">            </span><br><span class="line">                -- yarnClient.init</span><br><span class="line">                -- yarnClient.start</span><br><span class="line">                 <span class="comment">// 环境准备</span></span><br><span class="line">                <span class="comment">// command(Cluster) = bin/java org.apache.spark.deploy.yarn.ApplicationMaster</span></span><br><span class="line">                <span class="comment">// command(Client)  = bin/java org.apache.spark.deploy.yarn.ExecutorLauncher</span></span><br><span class="line">                -- createContainerLaunchContext(newAppResponse)</span><br><span class="line">                -- createApplicationSubmissionContext(newApp, containerContext)</span><br><span class="line">                </span><br><span class="line">                <span class="comment">// 向Yarn提交应用</span></span><br><span class="line">                -- yarnClient.submitApplication(appContext)</span><br><span class="line">                </span><br><span class="line">                    -- rmClient.submitApplication</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark任务运行流程(基于yarn集群模式)源码分析（3）</title>
    <url>/2020/06/01/spark/spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(3)/</url>
    <content><![CDATA[<h1 id="Spark任务运行流程-基于yarn集群模式-源码分析（3）"><a href="#Spark任务运行流程-基于yarn集群模式-源码分析（3）" class="headerlink" title="Spark任务运行流程(基于yarn集群模式)源码分析（3）"></a>Spark任务运行流程(基于yarn集群模式)源码分析（3）</h1><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p><img src="https://s1.ax1x.com/2020/06/19/NKN3Yd.png" alt="NKN3Yd.png"></p>
<h2 id="2-4-ApplicationMaster"><a href="#2-4-ApplicationMaster" class="headerlink" title="2.4 ApplicationMaster"></a>2.4 ApplicationMaster</h2><p>在sparksubmit与yarn建立了连接后，RM需要建立AM，并在AM中执行命令。<br>搜索<strong>org.apache.spark.deploy.yarn.ApplicationMaster</strong>，查看伴生类对象的main方法。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="type">SignalUtils</span>.registerLogger(log)</span><br><span class="line">  <span class="keyword">val</span> amArgs = <span class="keyword">new</span> <span class="type">ApplicationMasterArguments</span>(args)</span><br><span class="line">  master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs)</span><br><span class="line">  <span class="type">System</span>.exit(master.run())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的主要方法为</p>
<ol>
<li>new ApplicationMasterArguments    封装了命令行参数</li>
<li>new ApplicationMaster</li>
<li>run()<h3 id="2-4-1-ApplicationMaster"><a href="#2-4-1-ApplicationMaster" class="headerlink" title="2.4.1 ApplicationMaster"></a>2.4.1 ApplicationMaster</h3>点击查看ApplicationMaster的代码：</li>
<li>sparkConf //63行</li>
<li>yarnConf  //83行</li>
<li>userClassLoader //85行</li>
<li>client = doAsUser { new YarnRMClient() }    //122行<br>从client可以看到，AM中含有Yarn的RM的连接<h3 id="2-4-2-run"><a href="#2-4-2-run" class="headerlink" title="2.4.2 run()"></a>2.4.2 run()</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//run的函数很简单</span></span><br><span class="line">  <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">    doAsUser &#123;</span><br><span class="line">      runImpl()</span><br><span class="line">    &#125;</span><br><span class="line">    exitCode</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
进入runImpl()<br>runImpl()中包含了两个重要的代码<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> appAttemptId = client.getAttemptId()</span><br><span class="line"></span><br><span class="line"><span class="comment">//304行</span></span><br><span class="line"><span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">        runDriver()</span><br><span class="line"> 	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    	runExecutorLauncher()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
appAttemptId为Yarn的全局ID，一个任务仅包含一个ID<br>集群模式执行了runDriver()<br>而客户端模式执行runExecutorLauncher()<h4 id="2-4-2-1-runDriver"><a href="#2-4-2-1-runDriver" class="headerlink" title="2.4.2.1 runDriver()"></a>2.4.2.1 runDriver()</h4>进入runDriver()<br><img src="https://s1.ax1x.com/2020/06/19/NK0hWQ.png" alt="NK0hWQ.png"></li>
</ol>
<ul>
<li>startUserApplication() 启动用户的方法<br>  进入 startUserApplication方法。很容易看到  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> mainMethod = userClassLoader.loadClass(args.userClass)</span><br><span class="line">     .getMethod(<span class="string">&quot;main&quot;</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br></pre></td></tr></table></figure>
<ol>
<li>这个方法通过类加载器，加载了–class的值，并获取了类中的main方法</li>
<li>val userThread = new Thread //这个userThread在之后被命名为Driver</li>
<li>userThread.setContextClassLoader(userClassLoader)<br>   userThread.setName(“Driver”)<pre><code> userThread.start()//run方法中判断main方法是否为静态。并调用main方法
     userThread
</code></pre>
这条线创建了Driver线程，执行应用程序</li>
</ol>
</li>
<li>val sc = ThreadUtils.awaitResult(sparkContextPromise.future,Duration(totalWaitTime, TimeUnit.MILLISECONDS))  这个线程阻塞，等待sparkcontext的创建</li>
</ul>
<h4 id="2-4-2-2-registerAM"><a href="#2-4-2-2-registerAM" class="headerlink" title="2.4.2.2 registerAM"></a>2.4.2.2 registerAM</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (sc != <span class="literal">null</span>) &#123;</span><br><span class="line">        rpcEnv = sc.env.rpcEnv</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> userConf = sc.getConf</span><br><span class="line">        <span class="keyword">val</span> host = userConf.get(<span class="string">&quot;spark.driver.host&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> port = userConf.get(<span class="string">&quot;spark.driver.port&quot;</span>).toInt</span><br><span class="line">        registerAM(host, port, userConf, sc.ui.map(_.webUrl))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> driverRef = rpcEnv.setupEndpointRef(</span><br><span class="line">          <span class="type">RpcAddress</span>(host, port),</span><br><span class="line">          <span class="type">YarnSchedulerBackend</span>.<span class="type">ENDPOINT_NAME</span>)</span><br><span class="line">        createAllocator(driverRef, userConf)</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br></pre></td></tr></table></figure>
<ol>
<li>运行Rpc通讯环境</li>
<li>registerAM(host, port, userConf, sc.ui.map(_.webUrl)) 向RM注册Driver，申请资源</li>
<li>rpcEnv.setupEndpointRef -&gt; RM向AM返回资源列表，并分配可用资源<br>小结：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 封装命令行参数</span><br><span class="line">&#x2F;&#x2F; --class xxxx.WordCount  &#x3D;》userClass</span><br><span class="line">	-- new ApplicationMasterArguments</span><br><span class="line"></span><br><span class="line">	-- new ApplicationMaster</span><br><span class="line"></span><br><span class="line">		-- sparkConf</span><br><span class="line">		-- yarnConf</span><br><span class="line">		-- userClassLoader</span><br><span class="line">		&#x2F;&#x2F; AM保有Yarn RM的客户端连接</span><br><span class="line">		-- client &#x3D; new YarnRMClient()</span><br><span class="line"></span><br><span class="line">	-- run</span><br><span class="line"></span><br><span class="line">		-- runImpl</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; 客户端模式执行此方法</span><br><span class="line">			-- runExecutorLauncher</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F; 集群模式执行此方法</span><br><span class="line">            -- runDriver</span><br><span class="line"></span><br><span class="line">				-- startUserApplication</span><br><span class="line"></span><br><span class="line">				&#x2F;&#x2F; 反射获取应用程序的main方法</span><br><span class="line">				-- val mainMethod &#x3D; userClassLoader.loadClass(args.userClass)</span><br><span class="line">.getMethod(&quot;main&quot;, classOf[Array[String]])</span><br><span class="line"></span><br><span class="line">				-- new Thread</span><br><span class="line"></span><br><span class="line">					--  userThread.setName(&quot;Driver&quot;)</span><br><span class="line">						userThread.start()</span><br><span class="line"></span><br><span class="line">					-- run </span><br><span class="line"></span><br><span class="line">						&#x2F;&#x2F; 创建运行线程，运行应用程序的main方法</span><br><span class="line">                        &#x2F;&#x2F; WordCount开始执行</span><br><span class="line">                        -- mainMethod.invoke</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; RPC 通信环境 </span><br><span class="line">		-- rpcEnv &#x3D; sc.env.rpcEnv</span><br><span class="line"></span><br><span class="line">		-- registerAM</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 获取可分配Yarn的资源</span><br><span class="line">            -- allocator.allocateResources()</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 分配Yarn的资源</span><br><span class="line">            -- handleAllocatedContainers</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 容器本地化 ：移动数据不如移动计算</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 运行选择后的容器资源</span><br><span class="line">            -- runAllocatedContainers</span><br><span class="line"></span><br><span class="line">                &#x2F;&#x2F; 根据容器的数量启动线程</span><br><span class="line">                &#x2F;&#x2F;</span><br><span class="line">                -- new ExecutorRunnable.run</span><br><span class="line"></span><br><span class="line">                -- nmClient &#x3D; NMClient.createNMClient()</span><br><span class="line"></span><br><span class="line">                -- startContainer</span><br><span class="line"></span><br><span class="line">                &#x2F;&#x2F; command &#x3D; bin&#x2F;java org.apache.spark.executor.CoarseGrainedExecutorBackend</span><br><span class="line">                -- prepareCommand</span><br><span class="line"></span><br><span class="line">            -- nmClient.startContainer</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark任务运行流程(基于yarn集群模式)源码分析（4）</title>
    <url>/2020/06/01/spark/spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(4)/</url>
    <content><![CDATA[<h1 id="Spark任务运行流程-基于yarn集群模式-源码分析（4）"><a href="#Spark任务运行流程-基于yarn集群模式-源码分析（4）" class="headerlink" title="Spark任务运行流程(基于yarn集群模式)源码分析（4）"></a>Spark任务运行流程(基于yarn集群模式)源码分析（4）</h1><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p><img src="https://s1.ax1x.com/2020/06/19/NKxE40.png" alt="NKxE40.png"><br>AM创建了Driver线程后，连接NM启动CoarseGrainedExecutorBackend</p>
<h2 id="2-5-CoarseGrainedExecutorBackend"><a href="#2-5-CoarseGrainedExecutorBackend" class="headerlink" title="2.5 CoarseGrainedExecutorBackend"></a>2.5 CoarseGrainedExecutorBackend</h2><p>搜索 CoarseGrainedExecutorBackend点击伴生类对象，寻找Main方法<br><img src="https://s1.ax1x.com/2020/06/19/NKxY8K.png" alt="NKxY8K.png"><br>Main方法总，分解了参数。<br>并启动了run方法</p>
<h3 id="2-5-1-run"><a href="#2-5-1-run" class="headerlink" title="2.5.1 run"></a>2.5.1 run</h3><p><img src="https://s1.ax1x.com/2020/06/19/NKxwbd.png" alt="NKxwbd.png"></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//RpcEnv创建了一个RPC的通讯环境</span></span><br><span class="line"><span class="keyword">val</span> fetcher = <span class="type">RpcEnv</span>.create(</span><br><span class="line">        <span class="string">&quot;driverPropsFetcher&quot;</span>,</span><br><span class="line">        hostname,</span><br><span class="line">        <span class="number">-1</span>,</span><br><span class="line">        executorConf,</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SecurityManager</span>(executorConf),</span><br><span class="line">        clientMode = <span class="literal">true</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment">//得到driver</span></span><br><span class="line"><span class="keyword">val</span> driver = fetcher.setupEndpointRefByURI(driverUrl)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建执行器环境</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">SparkEnv</span>.createExecutorEnv(</span><br><span class="line">        driverConf, executorId, hostname, cores, cfg.ioEncryptionKey, isLocal = <span class="literal">false</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment">//创建一个名叫Excutor的终端，并与Driver互相通信</span></span><br><span class="line">env.rpcEnv.setupEndpoint(<span class="string">&quot;Executor&quot;</span>, <span class="keyword">new</span> <span class="type">CoarseGrainedExecutorBackend</span>(</span><br><span class="line">env.rpcEnv, driverUrl, executorId, hostname, cores, userClassPath, env))</span><br></pre></td></tr></table></figure>
<h3 id="2-5-2-inbox"><a href="#2-5-2-inbox" class="headerlink" title="2.5.2 inbox"></a>2.5.2 inbox</h3><p>进入setupEndpoint为一个抽象方法<br>获取实现类NettyRpcEnv找到实现setupEndpoint的方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//注册通讯终端</span></span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setupEndpoint</span></span>(name: <span class="type">String</span>, endpoint: <span class="type">RpcEndpoint</span>): <span class="type">RpcEndpointRef</span> = &#123;</span><br><span class="line">    dispatcher.registerRpcEndpoint(name, endpoint)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>进入registerRpcEndpoint</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//通讯终端中有一个终端的集合，里面存放了终端的消息</span></span><br><span class="line"><span class="keyword">if</span> (endpoints.putIfAbsent(name, <span class="keyword">new</span> <span class="type">EndpointData</span>(name, endpoint, endpointRef)) != <span class="literal">null</span>) &#123;<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s&quot;There is already an RpcEndpoint called <span class="subst">$name</span>&quot;</span>)&#125;</span><br></pre></td></tr></table></figure>
<p>进入EndpointData</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//inbox作为收件箱</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">EndpointData</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">      val name: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      val endpoint: <span class="type">RpcEndpoint</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">      val ref: <span class="type">NettyRpcEndpointRef</span></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> inbox = <span class="keyword">new</span> <span class="type">Inbox</span>(ref, endpoint)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>进入inbox</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//将消息放入LinkedList的收件箱</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">&quot;this&quot;</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> messages = <span class="keyword">new</span> java.util.<span class="type">LinkedList</span>[<span class="type">InboxMessage</span>]()</span><br><span class="line"></span><br><span class="line"><span class="comment">//当注册完毕，通讯终端应该启动</span></span><br><span class="line">inbox.synchronized &#123;</span><br><span class="line">    messages.add(<span class="type">OnStart</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//调用通讯终端的onStart方法启动</span></span><br><span class="line"><span class="keyword">case</span> <span class="type">OnStart</span> =&gt; endpoint.onStart()</span><br></pre></td></tr></table></figure>
<h3 id="2-5-3-onStrart"><a href="#2-5-3-onStrart" class="headerlink" title="2.5.3 onStrart"></a>2.5.3 onStrart</h3><p>在通讯终端(CoarseGrainedExecutorBackend)搜索onStrat方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>() &#123;</span><br><span class="line">    logInfo(<span class="string">&quot;Connecting to driver: &quot;</span> + driverUrl)</span><br><span class="line">    rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap &#123; ref =&gt;</span><br><span class="line">      <span class="comment">// This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot;</span></span><br><span class="line">      <span class="comment">//拿到Driver的引用</span></span><br><span class="line">      driver = <span class="type">Some</span>(ref)</span><br><span class="line">      <span class="comment">//想Driver请求，已经注册执行器</span></span><br><span class="line">      ref.ask[<span class="type">Boolean</span>](<span class="type">RegisterExecutor</span>(executorId, self, hostname, cores, extractLogUrls))</span><br><span class="line">    &#125;(<span class="type">ThreadUtils</span>.sameThread).onComplete &#123;</span><br><span class="line">      <span class="comment">// This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot;</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Success</span>(msg) =&gt;</span><br><span class="line">        <span class="comment">// Always receive `true`. Just ignore it</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt;</span><br><span class="line">        exitExecutor(<span class="number">1</span>, <span class="string">s&quot;Cannot register with driver: <span class="subst">$driverUrl</span>&quot;</span>, e, notifyDriver = <span class="literal">false</span>)</span><br><span class="line">    &#125;(<span class="type">ThreadUtils</span>.sameThread)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-3-Driver回复-SchedulerBackend"><a href="#2-5-3-Driver回复-SchedulerBackend" class="headerlink" title="2.5.3 Driver回复(SchedulerBackend)"></a>2.5.3 Driver回复(SchedulerBackend)</h3><p>Driver响应Executor的回复在sparkContext中。<br>里面有一个字段为</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> _schedulerBackend: <span class="type">SchedulerBackend</span> = _</span><br></pre></td></tr></table></figure>
<p>SchedulerBackend为用来做后台通讯。<br>进入SchedulerBackend</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">SchedulerBackend</span> </span>&#123;</span><br></pre></td></tr></table></figure>
<p>通过查找特质类找到了CoarseGrainedSchedulerBackend这个类<br>在类种搜索Executor发送的消息RegisterExecutor<br>找到了</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">RegisterExecutor</span>(executorId, executorRef, hostname, cores, logUrls) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (executorDataMap.contains(executorId)) &#123;</span><br><span class="line">          executorRef.send(<span class="type">RegisterExecutorFailed</span>(<span class="string">&quot;Duplicate executor ID: &quot;</span> + executorId))</span><br><span class="line">          context.reply(<span class="literal">true</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (scheduler.nodeBlacklist.contains(hostname)) &#123;</span><br><span class="line">          <span class="comment">// If the cluster manager gives us an executor on a blacklisted node (because it</span></span><br><span class="line">          <span class="comment">// already started allocating those resources before we informed it of our blacklist,</span></span><br><span class="line">          <span class="comment">// or if it ignored our blacklist), then we reject that executor immediately.</span></span><br><span class="line">          logInfo(<span class="string">s&quot;Rejecting <span class="subst">$executorId</span> as it has been blacklisted.&quot;</span>)</span><br><span class="line">          executorRef.send(<span class="type">RegisterExecutorFailed</span>(<span class="string">s&quot;Executor is blacklisted: <span class="subst">$executorId</span>&quot;</span>))</span><br><span class="line">          context.reply(<span class="literal">true</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// If the executor&#x27;s rpc env is not listening for incoming connections, `hostPort`</span></span><br><span class="line">          <span class="comment">// will be null, and the client connection should be used to contact the executor.</span></span><br><span class="line">          <span class="keyword">val</span> executorAddress = <span class="keyword">if</span> (executorRef.address != <span class="literal">null</span>) &#123;</span><br><span class="line">              executorRef.address</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              context.senderAddress</span><br><span class="line">            &#125;</span><br><span class="line">          logInfo(<span class="string">s&quot;Registered executor <span class="subst">$executorRef</span> (<span class="subst">$executorAddress</span>) with ID <span class="subst">$executorId</span>&quot;</span>)</span><br><span class="line">          <span class="comment">//更新注册地址</span></span><br><span class="line">          addressToExecutorId(executorAddress) = executorId</span><br><span class="line">          <span class="comment">//更新总共的核数，跟新资源列表</span></span><br><span class="line">          totalCoreCount.addAndGet(cores)</span><br><span class="line">          totalRegisteredExecutors.addAndGet(<span class="number">1</span>)</span><br><span class="line">          <span class="keyword">val</span> data = <span class="keyword">new</span> <span class="type">ExecutorData</span>(executorRef, executorAddress, hostname,</span><br><span class="line">            cores, cores, logUrls)</span><br><span class="line">          <span class="comment">// This must be synchronized because variables mutated</span></span><br><span class="line">          <span class="comment">// in this block are read when requesting executors</span></span><br><span class="line">          <span class="type">CoarseGrainedSchedulerBackend</span>.<span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">            executorDataMap.put(executorId, data)</span><br><span class="line">            <span class="keyword">if</span> (currentExecutorIdCounter &lt; executorId.toInt) &#123;</span><br><span class="line">              currentExecutorIdCounter = executorId.toInt</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (numPendingExecutors &gt; <span class="number">0</span>) &#123;</span><br><span class="line">              numPendingExecutors -= <span class="number">1</span></span><br><span class="line">              logDebug(<span class="string">s&quot;Decremented number of pending executors (<span class="subst">$numPendingExecutors</span> left)&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">//回复Executor，注册完毕(不需要回复)</span></span><br><span class="line">          executorRef.send(<span class="type">RegisteredExecutor</span>)</span><br><span class="line">          <span class="comment">// Note: some tests expect the reply to come after we put the executor in the map</span></span><br><span class="line">          context.reply(<span class="literal">true</span>)</span><br><span class="line">          listenerBus.post(</span><br><span class="line">            <span class="type">SparkListenerExecutorAdded</span>(<span class="type">System</span>.currentTimeMillis(), executorId, data))</span><br><span class="line">          makeOffers()</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-4-Executor收到Driver的回应"><a href="#2-5-4-Executor收到Driver的回应" class="headerlink" title="2.5.4 Executor收到Driver的回应"></a>2.5.4 Executor收到Driver的回应</h3><p>在CoarseGrainedExecutorBackend中寻找RegisteredExecutor</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">verride <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisteredExecutor</span> =&gt;</span><br><span class="line">      logInfo(<span class="string">&quot;Successfully registered with driver&quot;</span>)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//开始注册Executor</span></span><br><span class="line">        executor = <span class="keyword">new</span> <span class="type">Executor</span>(executorId, hostname, env, userClassPath, isLocal = <span class="literal">false</span>)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">          exitExecutor(<span class="number">1</span>, <span class="string">&quot;Unable to create executor due to &quot;</span> + e.getMessage, e)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://s1.ax1x.com/2020/06/19/NMCsc6.png" alt="NMCsc6.png"></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark任务运行流程(基于yarn集群模式)源码分析（5）</title>
    <url>/2020/06/01/spark/spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(5)/</url>
    <content><![CDATA[<h1 id="Spark任务运行流程-基于yarn集群模式-源码分析（5）"><a href="#Spark任务运行流程-基于yarn集群模式-源码分析（5）" class="headerlink" title="Spark任务运行流程(基于yarn集群模式)源码分析（5）"></a>Spark任务运行流程(基于yarn集群模式)源码分析（5）</h1><h2 id="2-6-任务执行"><a href="#2-6-任务执行" class="headerlink" title="2.6 任务执行"></a>2.6 任务执行</h2><p>任务的执行取决于行动算子，行动算子触发了才会执行任务。<br>所以在Driver中的Job应该有runJob</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;SparkContext has been shutdown&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">    <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">    logInfo(<span class="string">&quot;Starting job: &quot;</span> + callSite.shortForm)</span><br><span class="line">    <span class="keyword">if</span> (conf.getBoolean(<span class="string">&quot;spark.logLineage&quot;</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">      logInfo(<span class="string">&quot;RDD&#x27;s recursive dependencies:\n&quot;</span> + rdd.toDebugString)</span><br><span class="line">    &#125;</span><br><span class="line">     <span class="comment">//执行dagScheduler的runJob</span></span><br><span class="line">    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">    progressBar.foreach(_.finishAll())</span><br><span class="line">    rdd.doCheckpoint()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>runJob -&gt; submitJob </p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">      <span class="type">SerializationUtils</span>.clone(properties)))</span><br></pre></td></tr></table></figure>
<p>在类中搜索handleJobSubmitted<br>在其代码的1028行划分了阶段。<br><img src="https://s1.ax1x.com/2020/06/19/NMPAUJ.png" alt="NMPAUJ.png"><br>在最后提交阶段<br><img src="https://s1.ax1x.com/2020/06/19/NMP3UH.png" alt="NMP3UH.png"></p>
<p>-&gt; submitMissingTasks<br><img src="https://s1.ax1x.com/2020/06/19/NMPyPs.png" alt="NMPyPs.png"><br>确定了最后的任务分区</p>
<h2 id="2-7-总结"><a href="#2-7-总结" class="headerlink" title="2.7 总结"></a>2.7 总结</h2><p><img src="https://s1.ax1x.com/2020/06/19/NMCmnS.png" alt="NMCmnS.png"></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark源码</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkStream优雅的关闭</title>
    <url>/2020/06/04/spark/spark%E4%BC%98%E9%9B%85%E7%9A%84%E5%85%B3%E9%97%AD/</url>
    <content><![CDATA[<h1 id="SparkStream优雅的关闭"><a href="#SparkStream优雅的关闭" class="headerlink" title="SparkStream优雅的关闭"></a>SparkStream优雅的关闭</h1><p>当业务升级的场合，或者逻辑发送变化。需要关闭sparkStream的数据流的时候，就需要一种优雅的关闭。<br>但是Stop方法一般不会放置在main线程完成，所以需要一种线程的stop。<br><strong>关闭时机</strong>：关闭的时机判断一般不会使用业务操作，一般曹勇第三方的程序，或者存储来判断（HDFS,ZK,MYSQL,REDIS）等等</p>
<p>下面演示采用HDFS来监控来停止SparkStream</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sparkConf.set(<span class="string">&quot;spark.streaming.stopGracefullyOnShutdown&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动新的线程，希望在特殊的场合关闭SparkStreaming</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">Thread</span>(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> ( <span class="literal">true</span> ) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> ex : <span class="type">Exception</span> =&gt; println(ex)</span><br><span class="line">            &#125;</span><br><span class="line">    </span><br><span class="line">            <span class="comment">// 监控HDFS文件的变化</span></span><br><span class="line">            <span class="keyword">val</span> fs: <span class="type">FileSystem</span> = <span class="type">FileSystem</span>.get(<span class="keyword">new</span> <span class="type">URI</span>(<span class="string">&quot;hdfs://linux1:9000&quot;</span>), <span class="keyword">new</span> <span class="type">Configuration</span>(), <span class="string">&quot;root&quot;</span>)</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">val</span> state: <span class="type">StreamingContextState</span> = context.getState()</span><br><span class="line">            <span class="comment">// 如果环境对象处于活动状态，可以进行关闭操作</span></span><br><span class="line">            <span class="keyword">if</span> ( state == <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span> ) &#123;</span><br><span class="line">    </span><br><span class="line">                <span class="comment">// 判断路径是否存在</span></span><br><span class="line">                <span class="keyword">val</span> flg: <span class="type">Boolean</span> = fs.exists(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;hdfs://linux1:9000/stopSpark2&quot;</span>))</span><br><span class="line">                <span class="keyword">if</span> ( flg ) &#123;</span><br><span class="line">                    context.stop(<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">                    <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">                &#125;</span><br><span class="line">    </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;).start()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark常用算子总结</title>
    <url>/2020/05/25/spark/spark%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="spark常用算子总结"><a href="#spark常用算子总结" class="headerlink" title="spark常用算子总结"></a>spark常用算子总结</h1><p><strong>本文总结了spark中常用的几个算子重要程度用★标出</strong></p>
<h2 id="转换算子"><a href="#转换算子" class="headerlink" title="转换算子"></a>转换算子</h2><h3 id="★★★map"><a href="#★★★map" class="headerlink" title="★★★map"></a>★★★map</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">def map[U: ClassTag](f: T =&gt; U): RDD[U]</span><br></pre></td></tr></table></figure>
<p>将处理的数据进行逐条转换，这里的转换可以是任意转换。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">val dataRDD: RDD[Int] = sparkContext.makeRDD(List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">val dataRDD1: RDD[Int] = dataRDD.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        num * <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line">val dataRDD2: RDD[String] = dataRDD1.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        <span class="string">&quot;&quot;</span> + num</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="★★mapPartitions"><a href="#★★mapPartitions" class="headerlink" title="★★mapPartitions"></a>★★mapPartitions</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="行动算子"><a href="#行动算子" class="headerlink" title="行动算子"></a>行动算子</h2>]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>算子</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark通讯架构</title>
    <url>/2020/06/05/spark/spark%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h1 id="Spark通讯架构"><a href="#Spark通讯架构" class="headerlink" title="Spark通讯架构"></a>Spark通讯架构</h1><p>在Spark2系列中，Spark抛弃了Akka，转而视同了Netty通讯框架作为内部通讯组件。<br>Spark通讯框架中各个组件（Client/Master/Worker）可以认为是一个个独立的实体，各个实体之间通过消息来进行通信。<br>Endpoint（Client/Master/Worker）有1个InBox和N个OutBox（N&gt;=1，N取决于当前Endpoint与多少其他的Endpoint进行通信，一个与其通讯的其他Endpoint对应一个OutBox），Endpoint接收到的消息被写入InBox，发送出去的消息写入OutBox并被发送到其他Endpoint的InBox中。</p>
<h2 id="Spark通讯架构解析"><a href="#Spark通讯架构解析" class="headerlink" title="Spark通讯架构解析"></a>Spark通讯架构解析</h2><p>Spark通讯架构如下图所示：<br><img src="https://i.loli.net/2020/07/27/rfcHGKC86AyY4Js.png" alt="Spark通讯架构2.png"><br><img src="https://i.loli.net/2020/07/27/cldi247bCGovWrN.png" alt="spark通信架构.png"></p>
<ul>
<li>RpcEndpoint：RPC通信终端。Spark针对每个节点（Client/Master/Worker）都称之为一个RPC终端，且都实现RpcEndpoint接口，内部根据不同端点的需求，设计不同的消息和不同的业务处理，如果需要发送（询问）则调用Dispatcher。在Spark中，所有的终端都存在生命周期：<br>  Constructor<br>  onStart<br>  receive*<br>  onStop</li>
<li>RpcEnv：RPC上下文环境，每个RPC终端运行时依赖的上下文环境称为RpcEnv；在把当前Spark版本中使用的NettyRpcEnv</li>
<li>Dispatcher：消息调度（分发）器，针对于RPC终端需要发送远程消息或者从远程RPC接收到的消息，分发至对应的指令收件箱（发件箱）。如果指令接收方是自己则存入收件箱，如果指令接收方不是自己，则放入发件箱；</li>
<li>Inbox：指令消息收件箱。一个本地RpcEndpoint对应一个收件箱，Dispatcher在每次向Inbox存入消息时，都将对应EndpointData加入内部ReceiverQueue中，另外Dispatcher创建时会启动一个单独线程进行轮询ReceiverQueue，进行收件箱消息消费；</li>
<li>RpcEndpointRef：RpcEndpointRef是对远程RpcEndpoint的一个引用。当我们需要向一个具体的RpcEndpoint发送消息时，一般我们需要获取到该RpcEndpoint的引用，然后通过该应用发送消息。</li>
<li>OutBox：指令消息发件箱。对于当前RpcEndpoint来说，一个目标RpcEndpoint对应一个发件箱，如果向多个目标RpcEndpoint发送信息，则有多个OutBox。当消息放入Outbox后，紧接着通过TransportClient将消息发送出去。消息放入发件箱以及发送过程是在同一个线程中进行；</li>
<li>RpcAddress：表示远程的RpcEndpointRef的地址，Host + Port。</li>
<li>TransportClient：Netty通信客户端，一个OutBox对应一个TransportClient，TransportClient不断轮询OutBox，根据OutBox消息的receiver信息，请求对应的远程TransportServer；</li>
<li>TransportServer：Netty通信服务端，一个RpcEndpoint对应一个TransportServer，接受远程消息后调用Dispatcher分发消息至对应收发件箱；</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>算子</tag>
      </tags>
  </entry>
  <entry>
    <title>用数组和链表分别实现栈</title>
    <url>/2019/10/23/java/DataStructures/%E7%94%A8%E6%95%B0%E7%BB%84%E5%92%8C%E9%93%BE%E8%A1%A8%E5%88%86%E5%88%AB%E5%AE%9E%E7%8E%B0%E6%A0%88/</url>
    <content><![CDATA[<h1 id="用数组和链表分别实现栈"><a href="#用数组和链表分别实现栈" class="headerlink" title="用数组和链表分别实现栈"></a>用数组和链表分别实现栈</h1><h2 id="数组版"><a href="#数组版" class="headerlink" title="数组版"></a>数组版</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArrayStrakDemo</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		<span class="comment">//TODO 用数组模拟栈的实现</span></span><br><span class="line">		ArrayStark stark = <span class="keyword">new</span> ArrayStark(<span class="number">4</span>);</span><br><span class="line">		String key;</span><br><span class="line">		<span class="keyword">boolean</span> loop = <span class="keyword">true</span>;</span><br><span class="line">		Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span>(loop)&#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;show&quot;</span>);</span><br><span class="line">			System.out.println(<span class="string">&quot;exit&quot;</span>);</span><br><span class="line">			System.out.println(<span class="string">&quot;push&quot;</span>);</span><br><span class="line">			System.out.println(<span class="string">&quot;pop&quot;</span>);</span><br><span class="line">			System.out.println(<span class="string">&quot;Enter yor choose&quot;</span>);</span><br><span class="line">			key = scanner.next();</span><br><span class="line">			<span class="keyword">switch</span> (key)&#123;</span><br><span class="line">				<span class="keyword">case</span> <span class="string">&quot;show&quot;</span>:</span><br><span class="line">					stark.list();</span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">				<span class="keyword">case</span> <span class="string">&quot;push&quot;</span>:</span><br><span class="line">					System.out.println(<span class="string">&quot;Enter the number&quot;</span>);</span><br><span class="line">					stark.push(scanner.nextInt());</span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">				<span class="keyword">case</span> <span class="string">&quot;pop&quot;</span>:</span><br><span class="line">					<span class="keyword">try</span> &#123;</span><br><span class="line">						<span class="keyword">int</span> pop = stark.pop();</span><br><span class="line">						System.out.printf(<span class="string">&quot;The number is %d\n&quot;</span>,pop);</span><br><span class="line">					&#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">						System.out.println(e.getMessage());</span><br><span class="line">					&#125;</span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">				<span class="keyword">case</span> <span class="string">&quot;exit&quot;</span>:</span><br><span class="line">					scanner.close();</span><br><span class="line">					loop = <span class="keyword">false</span>;</span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		System.out.println(<span class="string">&quot;程序退出&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//表示栈结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArrayStark</span></span>&#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> maxSize; <span class="comment">//栈的大小</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span>[] stark;<span class="comment">//数组，模拟栈，数据存在数组中</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">int</span> top = -<span class="number">1</span>;<span class="comment">//表示栈顶，初始值为-1表示没有数据</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//构造器</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">ArrayStark</span><span class="params">(<span class="keyword">int</span> maxSize)</span></span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.maxSize = maxSize;</span><br><span class="line">		stark = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="keyword">this</span>.maxSize];</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//栈满</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isFull</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> top == maxSize-<span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//栈空</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isEmpty</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> top == -<span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//入栈</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> value)</span></span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(isFull())&#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;栈满&quot;</span>);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		top++;</span><br><span class="line">		stark[top] = value;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//出栈</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pop</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(isEmpty())&#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;空的&quot;</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> stark[top--];</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//显示栈的情况,从栈顶往下遍历</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">list</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(isEmpty())&#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;没有数据&quot;</span>);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = top;i &gt;= <span class="number">0</span>; i--)&#123;</span><br><span class="line">			System.out.printf(<span class="string">&quot;stark[%d]=%d\n&quot;</span>,i,stark[i]);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="链表版"><a href="#链表版" class="headerlink" title="链表版"></a>链表版</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LinkedStrakDemo</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		LinkedStark stark = <span class="keyword">new</span> LinkedStark(<span class="number">4</span>);</span><br><span class="line">		<span class="keyword">boolean</span> loop = <span class="keyword">true</span>;</span><br><span class="line">		String key;</span><br><span class="line">		Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span>(loop)&#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;show&quot;</span>);</span><br><span class="line">			System.out.println(<span class="string">&quot;exit&quot;</span>);</span><br><span class="line">			System.out.println(<span class="string">&quot;push&quot;</span>);</span><br><span class="line">			System.out.println(<span class="string">&quot;pop&quot;</span>);</span><br><span class="line">			System.out.println(<span class="string">&quot;Enter yor choose&quot;</span>);</span><br><span class="line">			key = scanner.next();</span><br><span class="line">			<span class="keyword">switch</span> (key)&#123;</span><br><span class="line">				<span class="keyword">case</span> <span class="string">&quot;show&quot;</span>:</span><br><span class="line">					stark.list();</span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">				<span class="keyword">case</span> <span class="string">&quot;push&quot;</span>:</span><br><span class="line">					System.out.println(<span class="string">&quot;Enter the number&quot;</span>);</span><br><span class="line">					stark.push(scanner.nextInt());</span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">				<span class="keyword">case</span> <span class="string">&quot;pop&quot;</span>:</span><br><span class="line">					<span class="keyword">try</span> &#123;</span><br><span class="line">						<span class="keyword">int</span> pop = stark.pop();</span><br><span class="line">						System.out.printf(<span class="string">&quot;The number is %d\n&quot;</span>,pop);</span><br><span class="line">					&#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">						System.out.println(e.getMessage());</span><br><span class="line">					&#125;</span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">				<span class="keyword">case</span> <span class="string">&quot;exit&quot;</span>:</span><br><span class="line">					scanner.close();</span><br><span class="line">					loop = <span class="keyword">false</span>;</span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		System.out.println(<span class="string">&quot;程序退出&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span></span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">int</span> num;</span><br><span class="line">	<span class="keyword">public</span> Node next;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(<span class="keyword">int</span> num)</span></span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.num = num;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkedStark</span></span>&#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> maxSize; <span class="comment">//栈的容量</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">int</span> nums;   <span class="comment">//栈当前的大小</span></span><br><span class="line">	<span class="keyword">private</span> Node top;<span class="comment">//表示栈顶的元素</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">LinkedStark</span><span class="params">(<span class="keyword">int</span> maxSize)</span></span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.maxSize = maxSize;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//栈满</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPull</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> nums == maxSize;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//栈空</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isEmpty</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> nums == <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//添加元素</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(isPull())&#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;栈满&quot;</span>);</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">//将栈顶的旧元素节点保存</span></span><br><span class="line">			Node oldNode = top;</span><br><span class="line">			<span class="comment">//替换新的栈顶的元素</span></span><br><span class="line">			top = <span class="keyword">new</span> Node(n);</span><br><span class="line">			top.next = oldNode;</span><br><span class="line">			nums++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//缩减元素</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">pop</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(isEmpty())&#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;栈空&quot;</span>);</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">//取出当前栈顶元素的值</span></span><br><span class="line">			<span class="keyword">int</span> value = top.num;</span><br><span class="line">			<span class="comment">//舍弃当前栈顶的节点</span></span><br><span class="line">			top = top.next;</span><br><span class="line">			nums--;</span><br><span class="line">			<span class="keyword">return</span> value;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//显示栈的情况</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">list</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(isEmpty())&#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;栈空&quot;</span>);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span>(Node node = top;node.next != <span class="keyword">null</span>; node = node.next)&#123;</span><br><span class="line">			System.out.println(node.num);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>数据结构</category>
        <category>栈</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title>Hashmap扩容机制</title>
    <url>/2019/12/14/java/hash/Hashmap%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h1 id="HashMap-扩容机制"><a href="#HashMap-扩容机制" class="headerlink" title="HashMap 扩容机制"></a>HashMap 扩容机制</h1><p>首先hashmap中有一个重要的参数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The number of key-value mappings contained in this map.</span></span><br><span class="line"><span class="comment">     */</span>    </span><br><span class="line">   <span class="keyword">transient</span> <span class="keyword">int</span> size;</span><br></pre></td></tr></table></figure>
<p>这个数值记录了当前的数据量，当数据量达到阈值，就会触发扩容机制。</p>
<p><strong>扩容算法</strong>：</p>
<h2 id="扩容长度"><a href="#扩容长度" class="headerlink" title="扩容长度"></a>扩容长度</h2><p>在hashmap中，table数组的长度必须是2^n数。所以，扩容的长度是根据上一次的table长度经过位移运算获取。<br>假设当前的tableSize的长度为16.那么扩容后的长度为16&lt;&lt;1 == 32。<br>之所以不采用乘数运算，也是因为性能原因。在底层，cpu不支持乘法运算，所以最终还是会转化为加法运算。但是采用位运算，效率会大大提高。</p>
<h2 id="hashmap扩容为什么是2倍，而不是1-5倍？"><a href="#hashmap扩容为什么是2倍，而不是1-5倍？" class="headerlink" title="hashmap扩容为什么是2倍，而不是1.5倍？"></a>hashmap扩容为什么是2倍，而不是1.5倍？</h2><p>首先hashmap通过indexFor来计算出当前数值应该放置于数值的哪一个位置。<br>这个运算方法是**h &amp; (length-1)**。<br>第一个数值是当前数的hash值。<br>假设数值为 01010101 &amp; 00001111 == 00000101<br>所以这个计算出来的结果5.放置与第5个格子<br>对于这个位运算，不管怎么计算，其结果只能在[0,15]之间。</p>
<p>了解了数值的放置规则，我们在来看看hashmap中数值的长度为什么一定要是2^n。<br>首先假设数组长度为5。来看看经过运算，数据会出现什么样的分布。<br>5-1==4转化为二进制位=&gt; 00000100  可以看到高4位都是0，低4位只有第三位是1.按照位运算&amp;的规则 只有第三位能得到0或者1.其余的位数只能是0.<br>所以，如果数组长度为5，数组经过计算，只能放置在索引为0或者4的位置上。这就造成了严重的资源浪费。<br>所以，为了不造成资源浪费。数组必须保证低4位都是1才能保证数据能分布在数组的每个索引位上。</p>
<h2 id="hashmap的冷知识：Hashmap-JDK1-8-在极限情况下，单个索引位能放置多少个元素才会变成红黑树"><a href="#hashmap的冷知识：Hashmap-JDK1-8-在极限情况下，单个索引位能放置多少个元素才会变成红黑树" class="headerlink" title="hashmap的冷知识：Hashmap(JDK1.8)在极限情况下，单个索引位能放置多少个元素才会变成红黑树"></a>hashmap的冷知识：Hashmap(JDK1.8)在极限情况下，单个索引位能放置多少个元素才会变成红黑树</h2><p>一般我们认为，链表长度大于8，数组长度大于64就会转化为红黑树。<br>所以，在极限情况下，假设所有的数据都放置在同一个索引上。<br>对于数据同一个索引：<br>在数据量达到9时，数组会发送第一次扩容16 &gt;&gt; 32<br>在数据量达到10时，数组发送第二次扩容32 &gt;&gt; 64<br>在数据量达到11时，数组就不会发送扩容，而是转化为红黑树。<br><strong>在极限情况下，hashmap在极限情况下，单个索引位能放置11个元素才会变成红黑树</strong></p>
]]></content>
      <categories>
        <category>java</category>
        <category>hashmap</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title>数仓治理与建设</title>
    <url>/2022/01/17/dataMall/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E6%B2%BB%E7%90%86/</url>
    <content><![CDATA[<h1 id="数仓治理与建设"><a href="#数仓治理与建设" class="headerlink" title="数仓治理与建设"></a><strong>数仓治理与建设</strong></h1><p>本博客适合还未对公司数仓建设与公司已经完成数仓建设将要重构</p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h2><p>&ensp; &ensp; 随着5G时代的来临,数据的体量每年都在成倍扩张,传统的数据库已经不能支撑起庞大的数据量.越来越多的公司开始构建自己的数据仓库.但是,随着企业数字化程度的日益加深，数据对企业增长的作用越来越重要，如何利用数据解决并帮助业务实现可持续增长，就成为了当下企业面临的主要问题。同时在构建数据仓库的过程中延伸出的问题:</p>
<ul>
<li><p>数据标准不统一：各业务团队的数据标准不统一，缺少对指标与模型的管理；</p>
</li>
<li><p>数据资产难管理：各类数据库表和服务缺少统一管理，难以真正发挥价值；</p>
</li>
<li><p>数据质量无保障：数据质量参差不齐，且存在重复存储、重复计算等问题。<br>因此，消除数据的不一致性，实现数据安全共享，进而发挥数据资产价值变得尤为迫切和重要。本文的目标也在于:让数据成为新的生产力，为业务发展赋能。</p>
<h2 id="数据仓库分层规范和表命名规范白皮书"><a href="#数据仓库分层规范和表命名规范白皮书" class="headerlink" title="数据仓库分层规范和表命名规范白皮书"></a><strong>数据仓库分层规范和表命名规范白皮书</strong></h2><h3 id="1-分层规范"><a href="#1-分层规范" class="headerlink" title="1 分层规范"></a>1 <strong>分层规范</strong></h3><h4 id="1-1-数仓分层"><a href="#1-1-数仓分层" class="headerlink" title="1.1 数仓分层"></a>1.1 <strong>数仓分层</strong></h4><p>&ensp; &ensp; 目前绝大多数公司的数仓都会分为五层.自下而上分别为:原始数据层（RAW，RAW DATA）、数据存储层（ODS，Operation Data Store）、模型层（DWD，Data Warehouse Detail）、汇总层（DWS，Data Warehouse Summary）、应用层（APP，Application).少量公司会在汇总层和应用层中间混入一个主题层(DWT,Data Warehouse Topic)<br>但是本文博主并不建议加入这个层,原因:1:主题层的分层和汇总层的区分并不明显,在开发的时候对于开发同学来说对一个需求在汇总层和主题层中间区分难免会犹豫.2:过多的分层也不利于数仓的模型管理</p>
<h4 id="1-2-RAW层"><a href="#1-2-RAW层" class="headerlink" title="1.2 RAW层"></a>1.2 <strong>RAW层</strong></h4><p>&ensp; &ensp; RAW（Raw Data）层存放这从业务系统获取的最原始的数据，是其他上层数据的源数据,结构上与源系统保持一致或者没有丢失数据信息。业务数据系统中的数据通常为非常细节的数据，经过长时间累积，切访问频率很高，是面向业务应用的数据。为了满足历史数据分析需求，可以在RAW层表中添加<strong>时间维度</strong>作为分区字段。实际应用中，可以选择采用<strong>增量</strong>、<strong>全量</strong>存储的方式。<br>&ensp; &ensp; 对于日志类数据源而言，是传输到存储介质上的原始数据，待进入数仓的数据；<br>&ensp; &ensp; 对于数据库类的数据，是指抽取到存储介质上的与业务库数据结构一致，没有丢失数据信息。<br>&ensp; &ensp; 它相当于一个数据准备区，同时又承担着基础数据的记录以及历史变化。</p>
<h4 id="1-3-ODS层"><a href="#1-3-ODS层" class="headerlink" title="1.3 ODS层"></a>1.3 <strong>ODS层</strong></h4><p>&ensp; &ensp; ODS层主要是对原始数据进行ETL处理，这些处理包括编码转换、清洗、统一格式、去重、脱敏。<br>&ensp; &ensp; 部分公司会将ODS层与RAW层两层和在一起,但是博主认为,在公司对于存储压力不是很大的情况下还是建议将两成分离.1是原始数据层可以作为最后的备份数据,是数据安全的最后的屏障.2:将两层和在一起,对于下游将数据进行分层的工作量太大</p>
<h4 id="1-4-DWD层"><a href="#1-4-DWD层" class="headerlink" title="1.4 DWD层"></a>1.4 <strong>DWD层</strong></h4><p>&ensp; &ensp; 模型层DWD以业务过程作为建模驱动，基于每个具体的业务过程特点，形成主题概念，构建最细粒度的明细层事实表。可以结合业务的数据使用特点，将明细事实表的维表属性字段做适当冗余，即宽表化处理。模型层的表通常也被称为逻辑事实表。<br>&ensp; &ensp; DWD层的事实表作为数据仓库维度建模的核心，需紧绕业务过程来设计。通过获取描述业务过程的度量来描述业务过程，包括引用的维度和与业务过程有关的度量。度量通常为数值型数据，作为事实逻辑表的依据。事实逻辑表的描述信息是事实属性，事实属性中的外键字段通过对应维度进行关联。事实表中一条记录所表达的业务细节程度被称为粒度。通常粒度可以通过两种方式来表述：一种是维度属性组合所表示的细节程度，一种是所表示的具体业务含义。事实表相对维表通常更加细长，行增加速度也更快。维度属性可以存储到事实表中，这种存储到事实表中的维度列称为维度退化，可加快查询速度。与其他存储在维表中的维度一样，维度退化可以用来进行事实表的过滤查询、实现聚合操作等。<br>&ensp; &ensp; DWD层通常分为三种：事务事实表、周期快照事实表和累积快照事实表。</p>
</li>
<li><p>事务事实表用来描述业务过程，跟踪空间或时间上某点的度量事件，保存的是最原子的数据，也称为原子事实表。</p>
</li>
<li><p>周期快照事实表以具有规律性的、可预见的时间间隔记录事实。</p>
</li>
<li><p>累积快照事实表用来表述过程开始和结束之间的关键步骤事件，覆盖过程的整个生命周期，通常具有多个日期字段来记录关键时间点。当累积快照事实表随着生命周期不断变化时，记录也会随着过程的变化而被修改。<br>明细粒度事实表整体设计流程如下图所示。<br><img src="%E4%BA%8B%E5%AE%9E%E8%A1%A8.png" alt="事实表"><br>&ensp; &ensp; 在一致性度量中已定义好了交易业务过程及其度量。明细事实表注意针对业务过程进行模型设计。明细事实表的设计可以分为四个步骤：选择业务过程、确定粒度、选择维度、确定事实（度量）。粒度主要是在维度未展开的情况下记录业务活动的语义描述。在您建设明细事实表时，需要选择基于现有的表进行明细层数据的开发，清楚所建表记录存储的是什么粒度的数据。</p>
<h4 id="1-5-DWS层"><a href="#1-5-DWS层" class="headerlink" title="1.5 DWS层"></a>1.5 <strong>DWS层</strong></h4><p>&ensp; &ensp; 汇总层DWS以分析的主题对象作为建模驱动，基于上层的应用和产品的指标需求，构建公共粒度的汇总指标事实表，构建命名规范、口径一致的统计指标，为上层提供公共指标，建立轻度汇总表、重度汇总表，以宽表化手段物理化模型。<br>&ensp; &ensp; DWS层是面向分析对象的主题聚集建模。聚集是指针对原始明细粒度的数据进行汇总。聚集是不跨越事实的。聚集是针对原始星形模型进行的汇总。为获取和查询与原始模型一致的结果，聚集的维度和度量必须与原始模型保持一致，因此聚集是不跨越事实的。聚集会带来查询性能的提升，但聚集也会增加ETL维护的难度。</p>
<h4 id="1-6-DIM层"><a href="#1-6-DIM层" class="headerlink" title="1.6 DIM层"></a>1.6 <strong>DIM层</strong></h4><p>&ensp; &ensp; DIM维度层，当不同的维度表的属性具有相同的列名和领域内容时候，称为维度具有一致性，建立公司的一致数据分析维表，可以降低数据计算口径和算法不统一风险。<br>&ensp; &ensp; 以维度作为建模驱动，基于每个维度的业务含义，通过定义维度及维度主键，添加维度属性、关联维度等，完成属性定义的过程并建立一致的数据分析维表。相同维度属性在不同物理表中的字段名称、数据类型、数据内容须保持一致。</p>
</li>
</ul>
<h4 id="1-7-分层数据流向规则"><a href="#1-7-分层数据流向规则" class="headerlink" title="1.7 分层数据流向规则"></a><strong>1.7 分层数据流向规则</strong></h4><p>&ensp; &ensp; 1. 标准数据流向：RAW-&gt;ODS-&gt;DWD-&gt;DWS-&gt;APP；ODS-&gt;DWD-&gt;APP；<br>&ensp; &ensp; &ensp; &ensp; -   RAW 只能被 ODS 调用；<br>&ensp; &ensp; &ensp; &ensp; -   ODS 只能被 DWD 调用；<br>&ensp; &ensp; &ensp; &ensp; -   DWD 可以被 DWS 和 APP调用；<br>&ensp; &ensp; &ensp; &ensp; -   DWS 只能被 APP调用；<br>&ensp; &ensp; &ensp; &ensp; -  数据应用可以调用 DWD、DWS、APP，建议优先考虑使用汇总度高的数据。<br>&ensp;&ensp;2. 禁止反向调用，即底层数据不可以从高层数据中获取数据；<br>&ensp;&ensp;示例：<br>&ensp;&ensp;APP/DWS直接调用ODS层数据，这是不允许的，正确用法是调用经过设计加工的DWD模型层表，避免重复加工计算。故DWD模型层应该积极了解应用层数据的建设需求，将公用的数据沉淀到模型层，为其他数据层次提供数据服务。同时，APP应用层也需积极配合DWD模型层进行持续的数据模型改造。避免出现过度的ODS层引用、不合理的数据复制和子集合冗余。</p>
<h4 id="1-8-分层数据权限规则"><a href="#1-8-分层数据权限规则" class="headerlink" title="1.8 分层数据权限规则"></a><strong>1.8 分层数据权限规则</strong></h4><p>&ensp;&ensp;1、原则上RAW、ODS层数据不对外提供，申请ODS、RAW层数据需严格遵循审批流程<br>&ensp;&ensp;2、原则上对外只开通DWD、DWS、APP层数据；<br>&ensp;&ensp;示例：<br>&ensp;&ensp;BG-A想要调用BG-B的ODS层数据，BG-B不能直接提供ODS层数据，应该提供加工计算后的DWD或者DWS模型提供BG-A使用。如若上层模型缺失导致暂时无法提供对应的上层模型，则需要遵循公司严格的模型审批流程，避免带来重复加工计算量。并且后续BG-B积极了解相关模型数据需求，不断完善DWD/DWS层模型建设。</p>
<h4 id="1-9-分层部门间依赖规则"><a href="#1-9-分层部门间依赖规则" class="headerlink" title="1.9 分层部门间依赖规则"></a><strong>1.9 分层部门间依赖规则</strong></h4><p>&ensp;&ensp;部门之间有数据层级依赖时，遵循1.7分层数据流向规则；<br>&ensp;&ensp;示例：<br>&ensp;&ensp;&ensp;&ensp;A部门DWD层依赖B部门的某一层数据，那么B部门提供的数据是DWD层或者是经过审批流程的ODS、RAW层；<br>&ensp;&ensp;&ensp;&ensp;A部门DWS层依赖B部门的某一层数据，那么B部门提供的数据是DWD\DWS层或者是经过审批流程的ODS、RAW层；<br>&ensp;&ensp;&ensp;&ensp;A部门APP层依赖B部门的某一层数据，那么B部门提供的数据是DWD\DWS\APP层或者是经过审批流程的ODS、RAW层；</p>
<h4 id="1-10-分层数据存储周期规则"><a href="#1-10-分层数据存储周期规则" class="headerlink" title="1.10 分层数据存储周期规则"></a><strong>1.10 分层数据存储周期规则</strong></h4><p>&ensp;&ensp;1. RAW层表数据，从业务系统获取的最原始数据最终会流转到ODS 层，ODS 层数据作为原始数据保留下来，从而使得RAW上游数据成为临时数据。这类数据不建议保留很长时间，生命周期限制最长保存3年，可以根据实际情况适当减少保留天数。<br>&ensp;&ensp;2. ODS，最长存储周期为3年，特殊存储周期，需要说明，重要的业务表及需要保留历史分区数据的场景。<br>&ensp;&ensp;&ensp;&ensp;示例：金融交易类数据需要合规监查，按照合规要求的期限存储。<br>&ensp;&ensp;3.  DWD模型层以及DWS汇总层的表的类型为事实表，存储方式为按天分区。最长存储周期为3年，如有特殊存储周期，需要说明。<br>模型层的表的类型为事实表，存储方式为按天分区。</p>
<p>&ensp;&ensp;模型设计时根据自身业务需求设置表的生命周期管理。可依据3个月内的最大需要访问的跨度设置保留策略，具体计算方式如下：<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于4天时，建议将保留天数设为7天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于12天时，建议将保留天数设为15天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于30天时， 建议将保留天数设为33天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于90天时，建议将保留天数设为93天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于180天时， 建议将保留天数设为183天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于365天时，建议将保留天数设为368天。<br>&ensp;&ensp;4. APP应用层存放的是面向需求的统计指标数据，最长存储周期为5年，如有特殊存储周期，需要说明。<br>&ensp;&ensp;5. DIM维度层的表的类型为维度表，存储方式建议按天分区，最长存储周期为5年，如有特殊存储周期，需要说明。<br>&ensp;&ensp;模型设计时根据自身业务需求设置表的生命周期管理。可依据3个月内的最大需要访问的跨度设置保留策略，具体计算方式如下：<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于4天时，建议将保留天数设为7天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于12天时，建议将保留天数设为15天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于30天时， 建议将保留天数设为33天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于90天时，建议将保留天数设为93天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于180天时， 建议将保留天数设为183天。<br>&ensp;&ensp;&ensp;&ensp; - 当3个月内的最大访问跨度小于或等于365天时，建议将保留天数设为368天。<br>&ensp;&ensp;6. TMP层临时表，是指ETL 处理过程中产生的临时表数据，一般不建议保留，最多7天。</p>
<h3 id="2-表和字段命名规范"><a href="#2-表和字段命名规范" class="headerlink" title="2. 表和字段命名规范"></a>2. 表和字段命名规范</h3><h4 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h4><p>&ensp;&ensp;所有的数据都是以表的形式存在的，表中的数据是以行列的方式组织。在Hive中的各种不同类型任务的数据输入和输出对象都是表，表可以使用SQL的create table和drop table进行创建以及删除，对表的进一步描述。<br>可以参考 hive_table<br>表创建示例<br><strong>外部表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> `dwd_zp_husky_meta_table_info`(</span><br><span class="line">`db_name` string COMMENT <span class="string">&#x27;库名称&#x27;</span>,</span><br><span class="line">`tbl_name` string COMMENT <span class="string">&#x27;表名称&#x27;</span>,</span><br><span class="line">`tbl_type` string COMMENT <span class="string">&#x27;表类型 EXTERNAL_TABLE, MANAGED_TABLE&#x27;</span>,</span><br><span class="line">`location` string COMMENT <span class="string">&#x27;存储位置&#x27;</span>,</span><br><span class="line">`is_partition` <span class="type">int</span> COMMENT <span class="string">&#x27;是否分区表&#x27;</span>,</span><br><span class="line">`dt_partition` string COMMENT <span class="string">&#x27;分区字段默认:dt&#x27;</span>,</span><br><span class="line">`lifecycle` <span class="type">bigint</span> COMMENT <span class="string">&#x27;生命周期，0:未设置，-1:永久， 默认33天&#x27;</span></span><br><span class="line">)COMMENT <span class="string">&#x27;元数据表信息&#x27;</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (`dt` string)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span> COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> LINES TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE LOCATION <span class="string">&#x27;/home/hdp_lbg_supin/resultdata/hdp_lbg_supin_zplisting/dwd/dwd_zp_husky_meta_table_info&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p><strong>内部表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `dwd_zp_husky_meta_table_info`(</span><br><span class="line">`db_name` string COMMENT <span class="string">&#x27;库名称&#x27;</span>,</span><br><span class="line">`tbl_name` string COMMENT <span class="string">&#x27;表名称&#x27;</span>,</span><br><span class="line">`tbl_type` string COMMENT <span class="string">&#x27;表类型 EXTERNAL_TABLE, MANAGED_TABLE&#x27;</span>,</span><br><span class="line">`location` string COMMENT <span class="string">&#x27;存储位置&#x27;</span>,</span><br><span class="line">`is_partition` <span class="type">int</span> COMMENT <span class="string">&#x27;是否分区表&#x27;</span>,</span><br><span class="line">`dt_partition` string COMMENT <span class="string">&#x27;分区字段默认:dt&#x27;</span>,</span><br><span class="line">`lifecycle` <span class="type">bigint</span> COMMENT <span class="string">&#x27;生命周期，0:未设置，-1:永久， 默认33天&#x27;</span></span><br><span class="line">)COMMENT <span class="string">&#x27;元数据表信息&#x27;</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (`dt` string)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span> COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> LINES TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE LOCATION</span><br><span class="line"><span class="string">&#x27;/home/hdp_lbg_supin/resultdata/hdp_lbg_supin_zplisting/dwd/dwd_zp_husky_meta_table_info&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h4 id="2-2表名命名"><a href="#2-2表名命名" class="headerlink" title="2.2表名命名"></a>2.2表名命名</h4><p>&ensp;&ensp;【定义】<br>&ensp;&ensp;表名必须由小写字母、数字、下划线组成，且以小写字母开头，使用下划线”<em>“把多个单词分开。<br>&ensp;&ensp;1.长度规范：整个表名的字符长度不能超过50,包含多个单词的表需要舍去不必要的单词；<br>&ensp;&ensp;2.拼写规范：只能使用小写英文单词及其缩写、下划线(‘</em>‘)、阿拉伯数字，字段只能英文字母开头；<br>&ensp;&ensp;3.分隔规范：采用下划线命名法，字段中包含多个单词的每个单词之间使用单个下划线分隔(‘_’)；<br>&ensp;&ensp;4.表意要求：表名应该尽量能够望文生义，反映本身的含义，禁止使用类似a,b,c这样没有意义的表名；<br>&ensp;&ensp;【约束级别】: 强制<br>&ensp;&ensp;【检查方法】: 每小时扫描<br>&ensp;&ensp;【处置方法】: TBD（To Be Drop）, NO(Notify Owner）, RPT(按团队汇总、报告)<br>&ensp;&ensp;【示例说明】:<br>&ensp;&ensp;good: zpb_algo_n_cfop3_model_step1_wenxun<br>&ensp;&ensp;bad: My_table,myTable,my_TABLE</p>
<h4 id="2-3表命名组成"><a href="#2-3表命名组成" class="headerlink" title="2.3表命名组成"></a>2.3表命名组成</h4><p>&ensp;&ensp;【定义】 表名必须是以下几种pattern之一<br>&ensp;&ensp;Pattern1.{数据层次}<em>&lt;一级业务线&gt;</em>&lt;二级业务线&gt;_&lt;三级业务线&gt;[_应用名称]{_自定义}[<em>时间维度]<br>&ensp;&ensp;Pattern2.{数据层次}</em>&lt;业务线缩写&gt;[_应用名称]{_自定义}[_时间维度]<br>&ensp;&ensp;业务线名称、业务线缩写必须出自码表定义（各个BG会自定义一套）<br>&ensp;&ensp;时间维度是否需要添加，_h,_d,_w,<em>y<br>数据层次包括raw、 ods、dim、dwd、dws、dim、app（注：如果每个部分包含多个独立单词可&ensp;&ensp;以使用下划线(‘</em>‘)来分隔），如果是视图表则：rawv、 odsv、dimv、dwdv、dwsv、dimv、appv；<br>&ensp;&ensp;【示例说明】<br>&ensp;&ensp;good： dwd_zp_st_n_cfop3_model_step1_wenxun、 ods_cfop3_model_st1_wenxun<br>&ensp;&ensp;bad： dwd_p4p_kgb_kkk_cfop3_model(kkk不在三级业务线码表中）、 ods_word_count_in(前缀均不在码表中）、dwd_p4p_cfop3_model(二级、三级业务线不在码表中，需要用n占位）<br>&ensp;&ensp;最佳实践： 尽量按Pattern1命名表，业务线不存在请联系【BG的负责人】</p>
<h4 id="2-4生命周期"><a href="#2-4生命周期" class="headerlink" title="2.4生命周期"></a>2.4生命周期</h4><p>&ensp;&ensp; 表的整个生命周期的规则以分层规范的存储周期为基准，以下是更细化的规则。<br>&ensp;&ensp; 【定义】<br>&ensp;&ensp; 1. 表/表分区DDL定义生命周期不超过3个月（以表元数据为准）<br>&ensp;&ensp; 2. tmp表/表分区DDL定义生命周期不超过7天<br>&ensp;&ensp; 3. 应用最大使用空间不超过meta（指申请登记）声名使用量<br>&ensp;&ensp; 4.声名使用量超过30T的应用，需经过PE、架构师Review<br>&ensp;&ensp; 5.因不合规发布造成元数据丢失无法跟踪责任的表，生命周期不超过7天，负责人为当期生产账号对应PE<br>&ensp;&ensp; 6.超过2T的表/表分区实际生存时间不超过1个月（以表CreateTime为准，2T指物理空间）<br>&ensp;&ensp;【检查方式】：每天扫描<br>&ensp;&ensp;【处理方式】： TBD 、NO、RPT<br>&ensp;&ensp;【最佳实践】：<br>&ensp;&ensp;生命周期不符的表，会被系统加上TBD前缀，更改生命周期为1周，并通知owner<br>&ensp;&ensp;TBD表一周无任何数据写入，将被husky自动回收<br>&ensp;&ensp;预研任务，有大表需要长期保留，请转成生产任务<br>&ensp;&ensp;被标记为TBD（to be drop）的表，请联系团队接口认领<br>&ensp;&ensp;加tbd_后命名冲突，表名后自动加时间戳（毫秒）、工号区分<br>&ensp;&ensp;特殊需求请与团队接口人沟通</p>
<h4 id="2-5-字段命名"><a href="#2-5-字段命名" class="headerlink" title="2.5 字段命名"></a>2.5 字段命名</h4><p>&ensp;&ensp;【定义】<br>&ensp;&ensp;字段名必须由小写字母、数字、下划线组成，且以小写字母开头，使用下划线”<em>“把多个单词分开。<br>&ensp;&ensp;1.命名规范<br>&ensp;&ensp;1）命名组成：字段名尽量与依赖的底表的相应字段相同，不要创造新的列名，可参考词根表（团队接口人维护）<br>&ensp;&ensp;2）长度规范：整个字段的字符长度不要超过30;每个单词的长度不要超过10个字符，最好是对长单词进行缩写。<br>&ensp;&ensp;3）拼写规范：只能使用小写英文单词及其缩写、下划线(‘</em>‘)、阿拉伯数字、尽量不用阿拉伯数字，最好不要使用拼音和拼音的简写<br>&ensp;&ensp;4）分隔规范：采用下划线的命名法，所有单独的单词都必须要用下划线(‘<em>‘)分隔<br>&ensp;&ensp;5）冲突限制：不能使用与hive或者mysql关键字、函数名相同的命名，不能使用’</em>‘开头<br>&ensp;&ensp;6）断意要求：字段名尽量要做到望文生义，反映字段本身的含义，禁止使用类似于a,b,c这样没有意义的字段名，也不推荐使用类似’id’这样短而泛的命名<br>&ensp;&ensp;7）区分要求：同一个表内有性质比较相近的字段时，需要明确区别，比如下单时间和支付时间，不能使用time1和time2这样。<br>&ensp;&ensp;2.字段类型<br>&ensp;&ensp;对于上游表和目标表属于同一数据源类型的，类型尽量和原始字段保持一致，hive中新加字段可以按照实际情况来设定。<br>&ensp;&ensp;hive表字段类型规定如下：<br>&ensp;&ensp;1）整形字段：除非有可能超过bigint范围，否则不管取值范围大小，统一采用bigint。<br>&ensp;&ensp;2）浮点字段：根据对精度的要求进行合理的设置，统一采用decimal(n,m)<br>&ensp;&ensp;3）字符型字段：统一采用string<br>&ensp;&ensp;4）时间类型字段：统一采用string<br>&ensp;&ensp;【约束级别】： 建议<br>&ensp;&ensp;【检查方式】：每小时扫描<br>&ensp;&ensp;【处理方式】：NO 、RPT<br>&ensp;&ensp;【示例说明】<br>&ensp;&ensp;good：uv_cnt, user_name, user_age<br>&ensp;&ensp;bad：My_cnt,myCnt,my_Count,mycnt<br>&ensp;&ensp;表字段类型说明：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>常量定义</th>
<th>描述</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>1Y、-127Y</td>
<td>8位有符号整形。范围：-128~127。</td>
<td>不推荐</td>
</tr>
<tr>
<td>SMALLINT</td>
<td>32767S、-100S</td>
<td>16位有符号整形。 范围：-32768~32767。</td>
<td>不推荐</td>
</tr>
<tr>
<td>INT</td>
<td>1000、-15645787</td>
<td>32位有符号整形。范围：-2^31^~2^31^ -1。</td>
<td>推荐</td>
</tr>
<tr>
<td>BIGINT</td>
<td>100000000000L、-1L</td>
<td>64位有符号整形。范围：-2^63^+1~2^63^-1。</td>
<td>推荐</td>
</tr>
<tr>
<td>FLOAT</td>
<td>无</td>
<td>32位二进制浮点型。</td>
<td>不推荐</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>3.1415926 1E+7</td>
<td>8字节双精度浮点数或64位二进制浮点型。</td>
<td>不推荐</td>
</tr>
<tr>
<td>DECIMAL</td>
<td>3.5BD、99999999999.9999999BD</td>
<td>10进制精确数字类型。整形部分范围：-1036+1~1036-1。小数部分精确到10-18 。</td>
<td>推荐</td>
</tr>
<tr>
<td>VARCHAR</td>
<td>无</td>
<td>变长字符类型，n为长度。取值范围：1~65535。</td>
<td>不推荐</td>
</tr>
<tr>
<td>STRING</td>
<td>“abc”、’bcd’、”alibaba”、”inc”</td>
<td>字符串类型。长度限制为8 MB。</td>
<td>推荐</td>
</tr>
<tr>
<td>BINARY</td>
<td>无</td>
<td>二进制数据类型。长度限制为8 MB。</td>
<td>不推荐</td>
</tr>
<tr>
<td>DATETIME</td>
<td>DATETIME ‘2017-11-11 00:00:00’</td>
<td>日期时间类型，使用东八区时间作为系统标准时间。<br />范围：0000年1月1日~9999年12月31日，精确到毫秒。</td>
<td>不推荐</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td>TIMESTAMP ‘2017-11-11 00:00:00.123456789’</td>
<td>与时区无关的时间戳类型。<br />范围：0000年1月1日~9999年12月31日23.59:59.999999999，精确到纳秒。</td>
<td>不推荐</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>TRUE、FALSE</td>
<td>布尔类型。值为TRUE或FALSE。</td>
<td>不推荐</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>复杂数据类型</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>类型</td>
<td>定义方法</td>
<td>构造方法</td>
<td>备注</td>
</tr>
<tr>
<td>ARRAY</td>
<td>array&lt;int&gt;、array&lt;struct&lt;a:int，b:string&gt;&gt;</td>
<td>array(1, 2, 3)、array(array(1, 2)、array(3, 4))</td>
<td>不推荐</td>
</tr>
<tr>
<td>MAP</td>
<td>map&lt;string, string&gt;、map&lt;smallint, array&lt;string&gt;&gt;</td>
<td>map(“k1”, “v1”, “k2”, “v2”)、map(1S, array(‘a’, ‘b’), 2S, array(‘x’, ‘y’))</td>
<td>不推荐</td>
</tr>
<tr>
<td>STRUCT</td>
<td>struct&lt;x:int, y:int&gt;、struct&lt;field1:bigint, field2:array&lt;int&gt;, field3:map&lt;int, int&gt;&gt;</td>
<td>named_struct(‘x’, 1, ‘y’, 2)、 named_struct(‘field1’, 100L, ‘field2’, array(1, 2), ‘field3’, map(1, 100, 2, 200))</td>
<td>不推荐</td>
</tr>
</tbody></table>
<h4 id="2-6-分区命名"><a href="#2-6-分区命名" class="headerlink" title="2.6 分区命名"></a>2.6 分区命名</h4><p>&ensp;&ensp;【定义】<br>&ensp;&ensp;所有分区表，一级分区名必须为“dt”，分区值为yyyyMMdd<br>&ensp;&ensp;小时、分钟分区的表，采用二级分区方式，一级分区名必须为“dt”，分区值为yyyyMMdd，二级分区采用具体的小时、分钟；<br>&ensp;&ensp;【约束级别】： 建议<br>&ensp;&ensp;【检查方式】：每小时扫描<br>&ensp;&ensp;【处理方式】： NO、RPT<br>&ensp;&ensp;【示例说明】：<br>&ensp;&ensp;good： dt=20130331/hour=23 、 dt=20130422<br>&ensp;&ensp;bad： dt=abc、 pt=20130422、ds=20130422</p>
<h4 id="2-7注释命名"><a href="#2-7注释命名" class="headerlink" title="2.7注释命名"></a>2.7注释命名</h4><p>&ensp;&ensp;【定义】<br>&ensp;&ensp;1.表必须有注释，且长度超过5个中文字符，注释要说明表的主题、类型以及用途。<br>&ensp;&ensp;2.字段必须有注释，且长度超过2个字符，注释要解释下字段含义，取值范围，没有在字典表维护的话，要给出每个取值或是取值范围代表的意义。（字段字典有团队接口人统一维护）<br>&ensp;&ensp;【约束级别】：建议<br>&ensp;&ensp;【检查方式】：每小时扫描<br>&ensp;&ensp;【处理方式】：NO、RPT<br>&ensp;&ensp;【最佳实践】：<br>&ensp;&ensp;解释用途，让使用者、维护者对表和字段有清晰的认识<br>&ensp;&ensp;注释禁止单纯copy表名、字段名，填写无意义描述</p>
<h4 id="2-8特殊表"><a href="#2-8特殊表" class="headerlink" title="2.8特殊表"></a>2.8特殊表</h4><p>&ensp;&ensp;【定义】<br>&ensp;&ensp;1.表只有一个字段时，且存储复合内容，名称必须为“content”，类型为string<br>&ensp;&ensp;content内容要在注释中描述<br>&ensp;&ensp;a.Protobuf注释内容：PB,&lt;pb定义svn地址&gt;<br>&ensp;&ensp;b.分隔符文本注释内容：TEXT,层次,分隔符列表,&lt;参考描述&gt;<br>&ensp;&ensp;2.表有两个字段key，value表，两个字段名称分别为“key”，“value”<br>&ensp;&ensp;【约束级别】：建议<br>&ensp;&ensp;【检查方式】：每小时扫描<br>&ensp;&ensp;【处理方式】： NO、RPT<br>&ensp;&ensp;【最佳实践】：<br>&ensp;&ensp;统一写成content方便工具扫描，识别<br>&ensp;&ensp;注释内容，方便后续维护，工具自动反解</p>
]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>建设</tag>
      </tags>
  </entry>
  <entry>
    <title>flink流处理API总结</title>
    <url>/2020/07/13/flink/flink%E6%B5%81%E5%A4%84%E7%90%86API%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Flink-流处理API"><a href="#Flink-流处理API" class="headerlink" title="Flink 流处理API"></a>Flink 流处理API</h1><p><img src="https://i.loli.net/2020/08/07/Y28O3ArjxXmBRDb.png" alt="227688edd9adf621dbfcea8bb000d6be.png"></p>
<h2 id="1-1-Environment"><a href="#1-1-Environment" class="headerlink" title="1.1 Environment"></a>1.1 Environment</h2><h3 id="1-1-1-getExecutionEnvironment"><a href="#1-1-1-getExecutionEnvironment" class="headerlink" title="1.1.1 getExecutionEnvironment"></a>1.1.1 getExecutionEnvironment</h3><p>创建一个执行环境，表示当前执行程序的上下文。<br>如果程序是独立调用的，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回此集群的执行环境，也就是说，getExecutionEnvironment会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> env: <span class="type">ExecutionEnvironment</span> = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br></pre></td></tr></table></figure>
<p>如果没有设置并行度，会以flink-conf.yaml中的配置为准，默认是1。</p>
<p><img src="https://i.loli.net/2020/08/07/bWMSDK1c86vPurl.png" alt="13a5b8d11f323cb86c7fc6cd8ab0150c.png"></p>
<h3 id="1-1-2-createLocalEnvironment"><a href="#1-1-2-createLocalEnvironment" class="headerlink" title="1.1.2 createLocalEnvironment"></a>1.1.2 createLocalEnvironment</h3><p>返回本地执行环境，需要在调用时指定默认的并行度。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.createLocalEnvironment(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<h3 id="1-1-3-createRemoteEnvironment"><a href="#1-1-3-createRemoteEnvironment" class="headerlink" title="1.1.3 createRemoteEnvironment"></a>1.1.3 createRemoteEnvironment</h3><p>返回集群执行环境，将Jar提交到远程服务器。需要在调用时指定JobManager的IP和端口号，并指定要在集群中运行的Jar包。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.createRemoteEnvironment(<span class="string">&quot;jobmanage-hostname&quot;</span>, <span class="number">6123</span>,<span class="string">&quot;YOURPATH//wordcount.jar&quot;</span>) </span><br></pre></td></tr></table></figure>

<h2 id="1-2-Source"><a href="#1-2-Source" class="headerlink" title="1.2 Source"></a>1.2 Source</h2><h3 id="1-2-1-从集合读取数据"><a href="#1-2-1-从集合读取数据" class="headerlink" title="1.2.1 从集合读取数据"></a>1.2.1 从集合读取数据</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义样例类，传感器id，时间戳，温度*  </span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SensorReading</span>(<span class="params">id: <span class="type">String</span>, timestamp: <span class="type">Long</span>, temperature: <span class="type">Double</span></span>)  </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">Sensor</span> </span>&#123;  </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;  </span><br><span class="line">		<span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.*getExecutionEnvironment*  </span><br><span class="line">        <span class="keyword">val</span> stream1 = env  </span><br><span class="line">        .fromCollection(<span class="type">List</span>(  </span><br><span class="line">         <span class="type">SensorReading</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">1547718199</span>, <span class="number">35.8</span>),  </span><br><span class="line">       	 <span class="type">SensorReading</span>(<span class="string">&quot;sensor_6&quot;</span>, <span class="number">1547718201</span>, <span class="number">15.4</span>),  </span><br><span class="line">         <span class="type">SensorReading</span>(<span class="string">&quot;sensor_7&quot;</span>, <span class="number">1547718202</span>, <span class="number">6.7</span>),  </span><br><span class="line">         <span class="type">SensorReading</span>(<span class="string">&quot;sensor_10&quot;</span>, <span class="number">1547718205</span>, <span class="number">38.1</span>)  </span><br><span class="line">       ))  </span><br><span class="line"></span><br><span class="line"> stream1.print(<span class="string">&quot;stream1:&quot;</span>).setParallelism(<span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line"> env.execute()  </span><br><span class="line"> &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-2-2-从文件读取数据"><a href="#5-2-2-从文件读取数据" class="headerlink" title="5.2.2 从文件读取数据"></a>5.2.2 从文件读取数据</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> stream2 = env.readTextFile(<span class="string">&quot;YOUR_FILE_PATH&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-2-3-以kafka消息队列的数据作为来源"><a href="#5-2-3-以kafka消息队列的数据作为来源" class="headerlink" title="5.2.3 以kafka消息队列的数据作为来源"></a>5.2.3 以kafka消息队列的数据作为来源</h3><p>需要引入kafka连接器的依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span>                                                             </span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>                                 </span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span>                                                       </span><br></pre></td></tr></table></figure>


<p>具体代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()  </span><br><span class="line"></span><br><span class="line">properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>)  </span><br><span class="line"></span><br><span class="line">properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)  </span><br><span class="line"></span><br><span class="line">properties.setProperty(<span class="string">&quot;key.deserializer&quot;</span>,</span><br><span class="line"><span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)  </span><br><span class="line"></span><br><span class="line">properties.setProperty(<span class="string">&quot;value.deserializer&quot;</span>,</span><br><span class="line"><span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>) </span><br><span class="line"></span><br><span class="line">properties.setProperty(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;latest&quot;</span>)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stream3 = env.addSource(<span class="keyword">new</span></span><br><span class="line"><span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](<span class="string">&quot;sensor&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(),</span><br><span class="line">properties))</span><br></pre></td></tr></table></figure>
<h3 id="5-2-4-自定义Source"><a href="#5-2-4-自定义Source" class="headerlink" title="5.2.4 自定义Source"></a>5.2.4 自定义Source</h3><p>除了以上的source数据来源，我们还可以自定义source。需要做的，只是传入一个SourceFunction就可以。具体调用如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> stream4 = env.addSource( <span class="keyword">new</span> <span class="type">MySensorSource</span>() )</span><br></pre></td></tr></table></figure>
<p>我们希望可以随机生成传感器数据，MySensorSource具体的代码实现如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySensorSource</span> <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// flag: 表示数据源是否还在正常运行</span></span><br><span class="line">  <span class="keyword">var</span> running: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    running = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 初始化一个随机数发生器</span></span><br><span class="line">    <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> curTemp = <span class="number">1.</span>to(<span class="number">10</span>).map(</span><br><span class="line">      i =&gt; (<span class="string">&quot;sensor_&quot;</span> + i, <span class="number">65</span> + rand.nextGaussian() * <span class="number">20</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (running) &#123;</span><br><span class="line">      <span class="comment">// 更新温度值</span></span><br><span class="line">      curTemp = curTemp.map(</span><br><span class="line">        t =&gt; (t._1, t._2 + rand.nextGaussian())</span><br><span class="line">      )</span><br><span class="line">      <span class="comment">// 获取当前时间戳</span></span><br><span class="line">      <span class="keyword">val</span> curTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"></span><br><span class="line">      curTemp.foreach(</span><br><span class="line">        t =&gt; ctx.collect(<span class="type">SensorReading</span>(t._1, curTime, t._2))</span><br><span class="line">      )</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">100</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="1-3-Transform"><a href="#1-3-Transform" class="headerlink" title="1.3 Transform"></a>1.3 Transform</h2><p><strong>转换算子</strong></p>
<h3 id="1-3-1-map"><a href="#1-3-1-map" class="headerlink" title="1.3.1 map"></a>1.3.1 map</h3><p><img src="https://i.loli.net/2020/08/07/CG9weZEbvgT6tox.png" alt="193055f74622201b2896a571b2ed96c9.png"></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> streamMap = stream.map &#123; x =&gt; x * <span class="number">2</span> &#125;</span><br></pre></td></tr></table></figure>

<h3 id="1-3-2-flatMap"><a href="#1-3-2-flatMap" class="headerlink" title="1.3.2 flatMap"></a>1.3.2 flatMap</h3><p>flatMap的函数签名：def flatMap[A,B](as: List[A])(f: A ⇒ List[B]): List[B]</p>
<p>例如: flatMap(List(1,2,3))(i ⇒ List(i,i))</p>
<p>结果是List(1,1,2,2,3,3), </p>
<p>而List(“a b”, “c d”).flatMap(line ⇒ line.split(“ “))</p>
<p>结果是List(a, b, c, d)。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> streamFlatMap = stream.flatMap&#123; x =&gt; x.split(<span class="string">&quot; &quot;</span>) &#125;</span><br></pre></td></tr></table></figure>

<h3 id="1-3-3-Filter"><a href="#1-3-3-Filter" class="headerlink" title="1.3.3 Filter"></a>1.3.3 Filter</h3><p><img src="https://i.loli.net/2020/08/07/BEwvZMLz8d7g1PG.png" alt="14f35444a633b3a5d83f9ac03351f0af.png"></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> streamFilter = stream.filter&#123; x =&gt; x == <span class="number">1</span> &#125;</span><br></pre></td></tr></table></figure>

<h3 id="1-3-4-KeyBy"><a href="#1-3-4-KeyBy" class="headerlink" title="1.3.4 KeyBy"></a>1.3.4 KeyBy</h3><p><img src="https://i.loli.net/2020/08/07/HVzjeFnGprSabOK.png" alt="344c4ea11966068d7e48a19ff3fa139e.png"></p>
<p><strong>DataStream</strong> → <strong>KeyedStream</strong>：<br>逻辑地将一个流拆分成不相交的分区，每个分区包含具有相同key的元素，在内部以hash的形式实现的。</p>
<h3 id="1-3-5-滚动聚合算子（Rolling-Aggregation）"><a href="#1-3-5-滚动聚合算子（Rolling-Aggregation）" class="headerlink" title="1.3.5 滚动聚合算子（Rolling Aggregation）"></a>1.3.5 滚动聚合算子（Rolling Aggregation）</h3><p>这些算子可以针对KeyedStream的每一个支流做聚合。</p>
<ul>
<li><p>  sum()</p>
</li>
<li><p>  min()</p>
</li>
<li><p>  max()</p>
</li>
<li><p>  minBy()</p>
</li>
<li><p>  maxBy()</p>
</li>
</ul>
<h3 id="1-3-6-Reduce"><a href="#1-3-6-Reduce" class="headerlink" title="1.3.6 Reduce"></a>1.3.6 Reduce</h3><p>KeyedStream →<br>DataStream：一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> stream2 = env.readTextFile(<span class="string">&quot;YOUR_PATH\\sensor.txt&quot;</span>)</span><br><span class="line">  .map(data =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> dataArray = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    <span class="type">SensorReading</span>(dataArray(<span class="number">0</span>).trim, dataArray(<span class="number">1</span>).trim.toLong, dataArray(<span class="number">2</span>).trim.toDouble)</span><br><span class="line">  &#125;)</span><br><span class="line">  .keyBy(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">  .reduce((x, y) =&gt; <span class="type">SensorReading</span>(x.id, x.timestamp + <span class="number">1</span>, y.temperature))</span><br></pre></td></tr></table></figure>
<h3 id="1-3-7-Split-和-Select"><a href="#1-3-7-Split-和-Select" class="headerlink" title="1.3.7 Split 和 Select"></a>1.3.7 Split 和 Select</h3><p><strong>Split</strong></p>
<p><img src="https://i.loli.net/2020/08/07/UHdBc94Y7Jk2Roh.png" alt="de0d07d920e4825e62e7c0a555f6a1b7.png"></p>
<p><strong>DataStream</strong> → <strong>SplitStream</strong>：</p>
<p>根据某些特征把一个DataStream拆分成两个或者多个DataStream。</p>
<p><strong>Select</strong></p>
<p><img src="https://i.loli.net/2020/08/07/PHrxqTbGDgVAOyJ.png" alt="图 Select"></p>
<p><strong>SplitStream→DataStream</strong>：从一个SplitStream中获取一个或者多个DataStream。</p>
<p>需求：传感器数据按照温度高低（以30度为界），拆分成两个流。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> splitStream = stream2</span><br><span class="line">      .split(sensorData =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (sensorData.temperature &gt; <span class="number">30</span>) <span class="type">Seq</span>(<span class="string">&quot;high&quot;</span>) <span class="keyword">else</span> <span class="type">Seq</span>(<span class="string">&quot;low&quot;</span>)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> high = splitStream.select(<span class="string">&quot;high&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> low = splitStream.select(<span class="string">&quot;low&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> all = splitStream.select(<span class="string">&quot;high&quot;</span>, <span class="string">&quot;low&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-3-8-Connect和-CoMap"><a href="#1-3-8-Connect和-CoMap" class="headerlink" title="1.3.8 Connect和 CoMap"></a>1.3.8 Connect和 CoMap</h3><p><img src="https://i.loli.net/2020/08/07/Mg5PdJtTqvZ7yCl.png" alt="74e8e724fc9eac1bd307c49fe837e8e4.png"></p>
<p><strong>DataStream,DataStream →ConnectedStreams</strong>：</p>
<p>连接两个保持他们类型的数据流，两个数据流被Connect之后，只是被放在了一个同一个流中，内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。</p>
<p><strong>CoMap,CoFlatMap</strong></p>
<p>!<img src="https://i.loli.net/2020/08/07/PHrxqTbGDgVAOyJ.png" alt="fa0c9ca1ea815fdcfccd218e06dd5239.png"></p>
<p><strong>ConnectedStreams →DataStream</strong>：</p>
<p>作用于ConnectedStreams上，功能与map和flatMap一样，对ConnectedStreams中的每一个Stream分别进行map和flatMap处理。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> warning = high.map( sensorData =&gt; (sensorData.id, sensorData.temperature) )</span><br><span class="line"><span class="keyword">val</span> connected = warning.connect(low)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> coMap = connected.map(</span><br><span class="line">    warningData =&gt; (warningData._1, warningData._2, <span class="string">&quot;warning&quot;</span>),</span><br><span class="line">    lowData =&gt; (lowData.id, <span class="string">&quot;healthy&quot;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<h3 id="1-3-9-Union"><a href="#1-3-9-Union" class="headerlink" title="1.3.9 Union"></a>1.3.9 Union</h3><p><img src="https://i.loli.net/2020/08/07/Qu4bPgjmS9etNyC.png" alt="a4f8072eb4f2feca852be1ac25ffc659.png"></p>
<p><strong>DataStream →DataStream：</strong></p>
<p>对两个或者两个以上的DataStream进行union操作，产生一个包含所有DataStream元素的新DataStream。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//合并以后打印</span></span><br><span class="line"><span class="keyword">val</span> unionStream: <span class="type">DataStream</span>[<span class="type">StartUpLog</span>] = appStoreStream.union(otherStream)</span><br><span class="line">unionStream.print(<span class="string">&quot;union:::&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>Connect与 Union 区别：</strong></p>
<ol>
<li>Union之前两个流的类型必须是一样，Connect可以不一样，在之后的coMap中再去调整成为一样的。</li>
<li>Connect只能操作两个流，Union可以操作多个。</li>
</ol>
<h2 id="1-4-支持的数据类型"><a href="#1-4-支持的数据类型" class="headerlink" title="1.4 支持的数据类型"></a>1.4 支持的数据类型</h2><p>Flink流应用程序处理的是以数据对象表示的事件流。所以在Flink内部，我们需要能够处理这些对象。它们需要被序列化和反序列化，以便通过网络传送它们；或者从状态后端、检查点和保存点读取它们。为了有效地做到这一点，Flink需要明确知道应用程序所处理的数据类型。Flink使用类型信息的概念来表示数据类型，并为每个数据类型生成特定的序列化器、反序列化器和比较器。</p>
<p>Flink还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息，从而获得序列化器和反序列化器。但是，在某些情况下，例如lambda函数或泛型类型，需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</p>
<p>Flink支持Java和Scala中所有常见数据类型。使用最广泛的类型有以下几种。</p>
<h3 id="1-4-1-基础数据类型"><a href="#1-4-1-基础数据类型" class="headerlink" title="1.4.1 基础数据类型"></a>1.4.1 基础数据类型</h3><p>Flink支持所有的Java和Scala基础数据类型，Int, Double, Long, String, …</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.fromElements(<span class="number">1</span>L, <span class="number">2</span>L, <span class="number">3</span>L, <span class="number">4</span>L)</span><br><span class="line"></span><br><span class="line">numbers.map( n =&gt; n + <span class="number">1</span> )</span><br></pre></td></tr></table></figure>
<h3 id="1-4-2-Java和Scala元组（Tuples）"><a href="#1-4-2-Java和Scala元组（Tuples）" class="headerlink" title="1.4.2 Java和Scala元组（Tuples）"></a>1.4.2 Java和Scala元组（Tuples）</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Integer</span>)] = env.fromElements(</span><br><span class="line">  (<span class="string">&quot;Adam&quot;</span>, <span class="number">17</span>),</span><br><span class="line">  (<span class="string">&quot;Sarah&quot;</span>, <span class="number">23</span>))</span><br><span class="line">persons.filter(p =&gt; p._2 &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure>

<h3 id="1-4-3-Scala样例类（case-classes）"><a href="#1-4-3-Scala样例类（case-classes）" class="headerlink" title="1.4.3 Scala样例类（case classes）"></a>1.4.3 Scala样例类（case classes）</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">persons</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = env.fromElements(</span><br><span class="line">  <span class="type">Person</span>(<span class="string">&quot;Adam&quot;</span>, <span class="number">17</span>),</span><br><span class="line">  <span class="type">Person</span>(<span class="string">&quot;Sarah&quot;</span>, <span class="number">23</span>))</span><br><span class="line">persons.filter(p =&gt; p.age &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-4-4-Java简单对象（POJOs）"><a href="#1-4-4-Java简单对象（POJOs）" class="headerlink" title="1.4.4 Java简单对象（POJOs）"></a>1.4.4 Java简单对象（POJOs）</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  public <span class="type">String</span> name;</span><br><span class="line">  public int age;</span><br><span class="line">  public <span class="type">Person</span>() &#123;&#125;</span><br><span class="line">  public <span class="type">Person</span>(<span class="type">String</span> name, int age) &#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">DataStream</span>&lt;<span class="type">Person</span>&gt; persons = env.fromElements(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">&quot;Alex&quot;</span>, <span class="number">42</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">&quot;Wendy&quot;</span>, <span class="number">23</span>));</span><br></pre></td></tr></table></figure>

<h3 id="1-4-5-其它（Arrays-Lists-Maps-Enums-等等）"><a href="#1-4-5-其它（Arrays-Lists-Maps-Enums-等等）" class="headerlink" title="1.4.5 其它（Arrays, Lists, Maps, Enums, 等等）"></a>1.4.5 其它（Arrays, Lists, Maps, Enums, 等等）</h3><p>Flink对Java和Scala中的一些特殊目的的类型也都是支持的，比如Java的ArrayList，HashMap，Enum等等。</p>
<h2 id="1-5-实现UDF函数——更细粒度的控制流"><a href="#1-5-实现UDF函数——更细粒度的控制流" class="headerlink" title="1.5 实现UDF函数——更细粒度的控制流"></a>1.5 实现UDF函数——更细粒度的控制流</h2><h3 id="1-5-1-函数类（Function-Classes）"><a href="#1-5-1-函数类（Function-Classes）" class="headerlink" title="1.5.1 函数类（Function Classes）"></a>1.5.1 函数类（Function Classes）</h3><p>Flink暴露了所有udf函数的接口(实现方式为接口或者抽象类)。例如MapFunction,<br>FilterFunction, ProcessFunction等等。</p>
<p>下面例子实现了FilterFunction接口：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilterFilter</span> <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    value.contains(<span class="string">&quot;flink&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> flinkTweets = tweets.filter(<span class="keyword">new</span> <span class="type">FlinkFilter</span>)</span><br></pre></td></tr></table></figure>

<p>还可以将函数实现成匿名类</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> flinkTweets = tweets.filter(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">RichFilterFunction</span>[<span class="type">String</span>] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">      value.contains(<span class="string">&quot;flink&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们filter的字符串”flink”还可以当作参数传进去。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> tweets: <span class="type">DataStream</span>[<span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> flinkTweets = tweets.filter(<span class="keyword">new</span> <span class="type">KeywordFilter</span>(<span class="string">&quot;flink&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KeywordFilter</span>(<span class="params">keyWord: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    value.contains(keyWord)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-5-2-匿名函数（Lambda-Functions）"><a href="#5-5-2-匿名函数（Lambda-Functions）" class="headerlink" title="5.5.2 匿名函数（Lambda Functions）"></a>5.5.2 匿名函数（Lambda Functions）</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> tweets: <span class="type">DataStream</span>[<span class="type">String</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> flinkTweets = tweets.filter(_.contains(<span class="string">&quot;flink&quot;</span>))</span><br></pre></td></tr></table></figure>
<h3 id="5-5-3-富函数（Rich-Functions）"><a href="#5-5-3-富函数（Rich-Functions）" class="headerlink" title="5.5.3 富函数（Rich Functions）"></a>5.5.3 富函数（Rich Functions）</h3><p>“富函数”是DataStream<br>API提供的一个函数类的接口，所有Flink函数类都有其Rich版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。</p>
<ul>
<li><p>  RichMapFunction</p>
</li>
<li><p>  RichFlatMapFunction</p>
</li>
<li><p>  RichFilterFunction</p>
</li>
<li><p>  …​</p>
</li>
</ul>
<p>Rich Function有一个生命周期的概念。典型的生命周期方法有：</p>
<ul>
<li><p>open()方法是rich<br>  function的初始化方法，当一个算子例如map或者filter被调用之前open()会被调用。</p>
</li>
<li><p>  close()方法是生命周期中的最后一个调用的方法，做一些清理工作。</p>
</li>
<li><p>  getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，例如函数执行的并行度，任务的名字，以及state状态</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>[<span class="type">Int</span>, (<span class="type">Int</span>, <span class="type">Int</span>)] </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> subTaskIndex = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(configuration: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    subTaskIndex = getRuntimeContext.getIndexOfThisSubtask</span><br><span class="line">    <span class="comment">// 以下可以做一些初始化工作，例如建立一个和HDFS的连接</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(in: <span class="type">Int</span>, out: <span class="type">Collector</span>[(<span class="type">Int</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (in % <span class="number">2</span> == subTaskIndex) &#123;</span><br><span class="line">      out.collect((subTaskIndex, in))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 以下做一些清理工作，例如断开和HDFS的连接。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="5-6-Sink"><a href="#5-6-Sink" class="headerlink" title="5.6 Sink"></a>5.6 Sink</h2><p>Flink没有类似于spark中foreach方法，让用户进行迭代的操作。虽有对外的输出操作都要利用Sink完成。最后通过类似如下方式完成整个任务最终输出操作。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">stream.addSink(<span class="keyword">new</span> <span class="type">MySink</span>(xxxx))</span><br></pre></td></tr></table></figure>

<p>官方提供了一部分的框架的sink。除此以外，需要用户自定义实现sink。</p>
<p><img src="media/1aa5dca34a0357600866098320d428d7.png"></p>
<p><img src="media/f1310383984206ee96747a1d817ee971.png"></p>
<h3 id="5-6-1-Kafka"><a href="#5-6-1-Kafka" class="headerlink" title="5.6.1 Kafka"></a>5.6.1 Kafka</h3><p>pom.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p>主函数中添加sink：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> union = high.union(low).map(_.temperature.toString)</span><br><span class="line"></span><br><span class="line">union.addSink(<span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>](<span class="string">&quot;localhost:9092&quot;</span>, <span class="string">&quot;test&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="5-6-2-Redis"><a href="#5-6-2-Redis" class="headerlink" title="5.6.2 Redis"></a>5.6.2 Redis</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.bahir/flink-connector-redis --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.bahir<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>定义一个redis的mapper类，用于定义保存到redis时调用的命令：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">extends</span> <span class="title">RedisMapper</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">RedisCommandDescription</span>(<span class="type">RedisCommand</span>.<span class="type">HSET</span>, <span class="string">&quot;sensor_temperature&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(t: <span class="type">SensorReading</span>): <span class="type">String</span> = t.temperature.toString</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(t: <span class="type">SensorReading</span>): <span class="type">String</span> = t.id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>在主函数中调用：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">FlinkJedisPoolConfig</span>.<span class="type">Builder</span>().setHost(<span class="string">&quot;localhost&quot;</span>).setPort(<span class="number">6379</span>).build()</span><br><span class="line">dataStream.addSink( <span class="keyword">new</span> <span class="type">RedisSink</span>[<span class="type">SensorReading</span>](conf, <span class="keyword">new</span> <span class="type">MyRedisMapper</span>) )</span><br></pre></td></tr></table></figure>


<h3 id="5-6-3-Elasticsearch"><a href="#5-6-3-Elasticsearch" class="headerlink" title="5.6.3 Elasticsearch"></a>5.6.3 Elasticsearch</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>


<p>在主函数中调用：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> httpHosts = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">HttpHost</span>]()</span><br><span class="line">httpHosts.add(<span class="keyword">new</span> <span class="type">HttpHost</span>(<span class="string">&quot;localhost&quot;</span>, <span class="number">9200</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> esSinkBuilder = <span class="keyword">new</span> <span class="type">ElasticsearchSink</span>.<span class="type">Builder</span>[<span class="type">SensorReading</span>](httpHosts, <span class="keyword">new</span> <span class="type">ElasticsearchSinkFunction</span>[<span class="type">SensorReading</span>] &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(t: <span class="type">SensorReading</span>, runtimeContext: <span class="type">RuntimeContext</span>, requestIndexer: <span class="type">RequestIndexer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;saving data: &quot;</span> + t)</span><br><span class="line">    <span class="keyword">val</span> json = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line">    json.put(<span class="string">&quot;data&quot;</span>, t.toString)</span><br><span class="line">    <span class="keyword">val</span> indexRequest = <span class="type">Requests</span>.indexRequest().index(<span class="string">&quot;sensor&quot;</span>).`<span class="class"><span class="keyword">type</span>`(<span class="params">&quot;readingData&quot;</span>).<span class="title">source</span>(<span class="params">json</span>)</span></span><br><span class="line"><span class="class">    <span class="title">requestIndexer</span>.<span class="title">add</span>(<span class="params">indexRequest</span>)</span></span><br><span class="line"><span class="class">    <span class="title">println</span>(<span class="params">&quot;saved successfully&quot;</span>)</span></span><br><span class="line"><span class="class">  &#125;</span></span><br><span class="line"><span class="class">&#125;)</span></span><br><span class="line"><span class="class"><span class="title">dataStream</span>.<span class="title">addSink</span>(<span class="params">esSinkBuilder.build(</span>))</span></span><br></pre></td></tr></table></figure>

<h3 id="5-6-4-JDBC-自定义sink"><a href="#5-6-4-JDBC-自定义sink" class="headerlink" title="5.6.4 JDBC 自定义sink"></a>5.6.4 JDBC 自定义sink</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.44<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>添加MyJdbcSink</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyJdbcSink</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">SensorReading</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> conn: <span class="type">Connection</span> = _</span><br><span class="line">  <span class="keyword">var</span> insertStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line">  <span class="keyword">var</span> updateStmt: <span class="type">PreparedStatement</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">// open 主要是创建连接</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">super</span>.open(parameters)</span><br><span class="line"></span><br><span class="line">    conn = <span class="type">DriverManager</span>.getConnection(<span class="string">&quot;jdbc:mysql://localhost:3306/test&quot;</span>, <span class="string">&quot;root&quot;</span>, <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">    insertStmt = conn.prepareStatement(<span class="string">&quot;INSERT INTO temperatures (sensor, temp) VALUES (?, ?)&quot;</span>)</span><br><span class="line">    updateStmt = conn.prepareStatement(<span class="string">&quot;UPDATE temperatures SET temp = ? WHERE sensor = ?&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 调用连接，执行sql</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: <span class="type">SensorReading</span>, context: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    updateStmt.setDouble(<span class="number">1</span>, value.temperature)</span><br><span class="line">    updateStmt.setString(<span class="number">2</span>, value.id)</span><br><span class="line">    updateStmt.execute()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (updateStmt.getUpdateCount == <span class="number">0</span>) &#123;</span><br><span class="line">      insertStmt.setString(<span class="number">1</span>, value.id)</span><br><span class="line">      insertStmt.setDouble(<span class="number">2</span>, value.temperature)</span><br><span class="line">      insertStmt.execute()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    insertStmt.close()</span><br><span class="line">    updateStmt.close()</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在main方法中增加，把明细保存到mysql中</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dataStream.addSink(<span class="keyword">new</span> <span class="type">MyJdbcSink</span>())</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Hashmap源码解析</title>
    <url>/2019/12/14/java/hash/Hashmap%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h1 id="HashMap-源码剖析"><a href="#HashMap-源码剖析" class="headerlink" title="HashMap 源码剖析"></a>HashMap 源码剖析</h1><ul>
<li><em><strong>如果你是要面对面试，可以直接去看下基本概念与总结</strong></em></li>
</ul>
<h2 id="1-hashmap的基本概念"><a href="#1-hashmap的基本概念" class="headerlink" title="1.hashmap的基本概念"></a>1.hashmap的基本概念</h2><ul>
<li>hash的基本概念：把一个任意长度的基本输入，通过一系列的hash算法映射成一个固定长度的输出。有时候两个不同的输入，映射出一个相同的输出，这种情况呗称为hash冲突。</li>
<li>hashmap的存储结构按JDK8来说是：数组+链表+红黑树构成的。<br><img src="https://s1.ax1x.com/2020/06/04/twI1bR.png" alt="twI1bR.png"></li>
<li>hashmap的每一个存储单元称为一个node结构。node中包含了：<br>key字段：map中key的字段<br>value字段：map中value的字段<br>next字段：当发生hash冲突的时候，当前桶中的node与发生冲突的node形成编标要用到的字段<br>hash字段：存储key的hash值，但是要经过一次扰动</li>
</ul>
<h2 id="2-hashmap类"><a href="#2-hashmap类" class="headerlink" title="2.hashmap类"></a>2.hashmap类</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;, <span class="title">Cloneable</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-hashmap基本属性"><a href="#3-hashmap基本属性" class="headerlink" title="3.hashmap基本属性"></a>3.hashmap基本属性</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">362498820763181265L</span>;<span class="comment">//序列化版本号</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_INITIAL_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">4</span>; <span class="comment">// 初始化长度为16，切必须是2的N次方</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAXIMUM_CAPACITY = <span class="number">1</span> &lt;&lt; <span class="number">30</span>;<span class="comment">//最大的容量为2^30，一般用于自定义初始化容量</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">float</span> DEFAULT_LOAD_FACTOR = <span class="number">0.75f</span>;<span class="comment">//默认负载因子</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TREEIFY_THRESHOLD = <span class="number">8</span>;<span class="comment">//数组单个单元要转化为红黑树节点的阈值</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> UNTREEIFY_THRESHOLD = <span class="number">6</span>;<span class="comment">//反树化时，节点的阈值</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MIN_TREEIFY_CAPACITY = <span class="number">64</span>;<span class="comment">//树化时数组长度的阈值</span></span><br></pre></td></tr></table></figure>
<h2 id="4-hashmap-Node属性"><a href="#4-hashmap-Node属性" class="headerlink" title="4.hashmap Node属性"></a>4.hashmap Node属性</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//hashmap中节点的基本属性，实现get，set方法。重写了hashCode、toString、equals方法。 属性在上文有介绍</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> hash;</span><br><span class="line">        <span class="keyword">final</span> K key;</span><br><span class="line">        V value;</span><br><span class="line">        Node&lt;K,V&gt; next;</span><br><span class="line"></span><br><span class="line">        Node(<span class="keyword">int</span> hash, K key, V value, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">            <span class="keyword">this</span>.hash = hash;</span><br><span class="line">            <span class="keyword">this</span>.key = key;</span><br><span class="line">            <span class="keyword">this</span>.value = value;</span><br><span class="line">            <span class="keyword">this</span>.next = next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> K <span class="title">getKey</span><span class="params">()</span>        </span>&#123; <span class="keyword">return</span> key; &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">getValue</span><span class="params">()</span>      </span>&#123; <span class="keyword">return</span> value; &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> key + <span class="string">&quot;=&quot;</span> + value; &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> Objects.hashCode(key) ^ Objects.hashCode(value);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">setValue</span><span class="params">(V newValue)</span> </span>&#123;</span><br><span class="line">            V oldValue = value;</span><br><span class="line">            value = newValue;</span><br><span class="line">            <span class="keyword">return</span> oldValue;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (o == <span class="keyword">this</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">if</span> (o <span class="keyword">instanceof</span> Map.Entry) &#123;</span><br><span class="line">                Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o;</span><br><span class="line">                <span class="keyword">if</span> (Objects.equals(key, e.getKey()) &amp;&amp;</span><br><span class="line">                    Objects.equals(value, e.getValue()))</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//扰动函数，用于计算hash值，在之后专门专题讲解。</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> h;</span><br><span class="line">        <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在发送hash冲突的时候。用于比较两个node。</span></span><br><span class="line">    <span class="keyword">static</span> Class&lt;?&gt; comparableClassFor(Object x) &#123;</span><br><span class="line">        <span class="keyword">if</span> (x <span class="keyword">instanceof</span> Comparable) &#123;</span><br><span class="line">            Class&lt;?&gt; c; Type[] ts, as; Type t; ParameterizedType p;</span><br><span class="line">            <span class="keyword">if</span> ((c = x.getClass()) == String.class) <span class="comment">// bypass checks</span></span><br><span class="line">                <span class="keyword">return</span> c;</span><br><span class="line">            <span class="keyword">if</span> ((ts = c.getGenericInterfaces()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; ts.length; ++i) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (((t = ts[i]) <span class="keyword">instanceof</span> ParameterizedType) &amp;&amp;</span><br><span class="line">                        ((p = (ParameterizedType)t).getRawType() ==</span><br><span class="line">                         Comparable.class) &amp;&amp;</span><br><span class="line">                        (as = p.getActualTypeArguments()) != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                        as.length == <span class="number">1</span> &amp;&amp; as[<span class="number">0</span>] == c) <span class="comment">// type arg is c</span></span><br><span class="line">                        <span class="keyword">return</span> c;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">compareComparables</span><span class="params">(Class&lt;?&gt; kc, Object k, Object x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (x == <span class="keyword">null</span> || x.getClass() != kc ? <span class="number">0</span> :</span><br><span class="line">                ((Comparable)k).compareTo(x));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-hashmap-构造器原理与字段"><a href="#5-hashmap-构造器原理与字段" class="headerlink" title="5.hashmap 构造器原理与字段"></a>5.hashmap 构造器原理与字段</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">//用于寻找大于或等于capacity的最小2的幂</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">tableSizeFor</span><span class="params">(<span class="keyword">int</span> cap)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = cap - <span class="number">1</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">1</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">2</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">4</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">8</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">16</span>;</span><br><span class="line">        <span class="keyword">return</span> (n &lt; <span class="number">0</span>) ? <span class="number">1</span> : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment">//					字段</span></span><br><span class="line"><span class="keyword">transient</span> Node&lt;K,V&gt;[] table; <span class="comment">//hashMap数组的表示</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">transient</span> Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; <span class="comment">//entry节点</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> size; <span class="comment">//数组长度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> modCount; <span class="comment">//添加的元素个数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> threshold; <span class="comment">//合理的初始化数组长度，根据tableSizeFor()得到，用于手动设置时使用</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">float</span> loadFactor; <span class="comment">//负载因子，用于手动设置时使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//					构造器</span></span><br><span class="line"><span class="comment">//构造器一：定义Node[]数组初始长度，与负载因子</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Illegal initial capacity: &quot;</span> +</span><br><span class="line">                                               initialCapacity);</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &gt; MAXIMUM_CAPACITY)</span><br><span class="line">            initialCapacity = MAXIMUM_CAPACITY;</span><br><span class="line">        <span class="keyword">if</span> (loadFactor &lt;= <span class="number">0</span> || Float.isNaN(loadFactor))</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Illegal load factor: &quot;</span> +</span><br><span class="line">                                               loadFactor);</span><br><span class="line">        <span class="comment">//手动设定负载因子</span></span><br><span class="line">        <span class="keyword">this</span>.loadFactor = loadFactor;</span><br><span class="line">        <span class="comment">//自动设置合适的数组长度</span></span><br><span class="line">        <span class="keyword">this</span>.threshold = tableSizeFor(initialCapacity);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment">//构造器二：定义Node[]数组初始长度，默认负载因子</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(initialCapacity, DEFAULT_LOAD_FACTOR);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//构造器三：仅创建HashMap对象，并初始化负债因子为0.75f</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.loadFactor = DEFAULT_LOAD_FACTOR; <span class="comment">// all other fields defaulted</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment">//构造器四：转化hashmap的父类</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(Map&lt;? extends K, ? extends V&gt; m)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.loadFactor = DEFAULT_LOAD_FACTOR;</span><br><span class="line">        <span class="comment">//这个方法为，将map中所有数据插入到hashmap中，此文不再描述</span></span><br><span class="line">        putMapEntries(m, <span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="6-hashmap树节点-简要分析"><a href="#6-hashmap树节点-简要分析" class="headerlink" title="6.hashmap树节点(简要分析)"></a>6.hashmap树节点(简要分析)</h2><p>hashmap树节点比较复杂，之后做专门的分析</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">LinkedHashMap</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">        TreeNode&lt;K,V&gt; parent;  <span class="comment">// 父节点</span></span><br><span class="line">        TreeNode&lt;K,V&gt; left;	<span class="comment">//左节点</span></span><br><span class="line">        TreeNode&lt;K,V&gt; right;<span class="comment">//右节点</span></span><br><span class="line">        TreeNode&lt;K,V&gt; prev;    <span class="comment">// 记录上一个节点</span></span><br><span class="line">        <span class="keyword">boolean</span> red;<span class="comment">//节点红黑判断</span></span><br><span class="line">        TreeNode(<span class="keyword">int</span> hash, K key, V val, Node&lt;K,V&gt; next) &#123;</span><br><span class="line">            <span class="keyword">super</span>(hash, key, val, next);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-hashmap-get方法"><a href="#7-hashmap-get方法" class="headerlink" title="7.hashmap get方法"></a>7.hashmap get方法</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//调用的GET方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">return</span> (e = getNode(hash(key), key)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//实际执行的GET方法</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">getNode</span><span class="params">(<span class="keyword">int</span> hash, Object key)</span> </span>&#123;</span><br><span class="line">	Node&lt;K,V&gt;[] tab; </span><br><span class="line">    Node&lt;K,V&gt; first, e; </span><br><span class="line">    <span class="keyword">int</span> n; K k;</span><br><span class="line">    <span class="comment">// table不为空 &amp;&amp; table长度大于0 &amp;&amp; table索引位置(根据hash值计算出)节点不为空</span></span><br><span class="line">    <span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp; (first = tab[(n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// first的key等于传入的key则返回first对象</span></span><br><span class="line">        <span class="keyword">if</span> (first.hash == hash &amp;&amp; ((k = first.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">            <span class="keyword">return</span> first;</span><br><span class="line">        <span class="comment">//first的key不等于传入的key则说明是链表，向下遍历</span></span><br><span class="line">        <span class="keyword">if</span> ((e = first.next) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 判断是否为TreeNode，是则为红黑树</span></span><br><span class="line">            <span class="comment">// 如果是红黑树节点，则调用红黑树的查找目标节点方法getTreeNode</span></span><br><span class="line">            <span class="keyword">if</span> (first <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                <span class="keyword">return</span> ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);</span><br><span class="line">            <span class="comment">//走下列步骤表示是链表，循环至节点的key与传入的key值相等</span></span><br><span class="line">            <span class="keyword">do</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                    <span class="keyword">return</span> e;</span><br><span class="line">            &#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//找不到符合的返回空</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-hashmap-put方法"><a href="#7-hashmap-put方法" class="headerlink" title="7.hashmap put方法"></a>7.hashmap put方法</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//掉用的PUT方法，hash(key)调用本例中的hash()方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> putVal(hash(key), key, value, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 实际执行的PUT方法 </span></span><br><span class="line"><span class="comment"> * Implements Map.put and related methods</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> hash hash for key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> key the key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value the value to put</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> onlyIfAbsent if true, don&#x27;t change existing value</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> evict if false, the table is in creation mode.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> previous value, or null if none</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function"><span class="keyword">final</span> V <span class="title">putVal</span><span class="params">(<span class="keyword">int</span> hash, K key, V value, <span class="keyword">boolean</span> onlyIfAbsent, <span class="keyword">boolean</span> evict)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab;</span><br><span class="line">    Node&lt;K,V&gt; p;</span><br><span class="line">    <span class="keyword">int</span> n, i;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// table是否为空或者length等于0, 如果是则调用resize方法进行初始化</span></span><br><span class="line">    <span class="comment">// table是一个 （Node&lt;K,V&gt;[] table;） Node类型的数组</span></span><br><span class="line">    <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)</span><br><span class="line">        n = (tab = resize()).length;</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 通过hash值计算索引位置, 如果table表该索引位置节点为空则新增一个</span></span><br><span class="line">    <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>) <span class="comment">// 将索引位置的头节点赋值给p</span></span><br><span class="line">        tab[i] = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span> &#123; <span class="comment">// table表该索引位置不为空</span></span><br><span class="line">        Node&lt;K,V&gt; e; K k;</span><br><span class="line">        <span class="comment">//判断p节点的hash值和key值是否跟传入的hash值和key值相等</span></span><br><span class="line">        <span class="keyword">if</span> (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">            e = p; <span class="comment">// 如果相等, 则p节点即为要查找的目标节点，赋值给e</span></span><br><span class="line">        <span class="comment">// 判断p节点是否为TreeNode, 如果是则调用红黑树的putTreeVal方法查找目标节点</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">            e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, key, value);</span><br><span class="line">        <span class="comment">// 走到这代表p节点为普通链表节点</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 遍历此链表, binCount用于统计节点数</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> binCount = <span class="number">0</span>; ; ++binCount) &#123;</span><br><span class="line">                <span class="comment">//p.next为空代表目标节点不存在</span></span><br><span class="line">                <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="comment">//新增一个节点插入链表尾部</span></span><br><span class="line">                    p.next = newNode(hash, key, value, <span class="keyword">null</span>);</span><br><span class="line">                    <span class="comment">//如果节点数目超过8个，调用treeifyBin方法将该链表转换为红黑树</span></span><br><span class="line">                    <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span></span><br><span class="line">                        treeifyBin(tab, hash);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//e节点的hash值和key值都与传入的相等, 则e即为目标节点,跳出循环</span></span><br><span class="line">                <span class="keyword">if</span> (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                p = e;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// e不为空则代表根据传入的hash值和key值查找到了节点,将该节点的value覆盖,返回oldValue</span></span><br><span class="line">        <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123; <span class="comment">// existing mapping for key</span></span><br><span class="line">            V oldValue = e.value;</span><br><span class="line">            <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)</span><br><span class="line">                e.value = value;</span><br><span class="line">            afterNodeAccess(e); <span class="comment">// 用于LinkedHashMap</span></span><br><span class="line">            <span class="keyword">return</span> oldValue;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//map修改次数加1</span></span><br><span class="line">    ++modCount;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//map节点数加1，如果超过阀值，则扩容</span></span><br><span class="line">    <span class="keyword">if</span> (++size &gt; threshold)</span><br><span class="line">        resize();</span><br><span class="line">    afterNodeInsertion(evict); <span class="comment">// 用于LinkedHashMap</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="8-hashmap-resize-方法"><a href="#8-hashmap-resize-方法" class="headerlink" title="8.hashmap resize()方法"></a>8.hashmap resize()方法</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Node&lt;K,V&gt;[] resize() &#123;</span><br><span class="line">        <span class="comment">//oldTab保存未扩容的tab</span></span><br><span class="line">        Node&lt;K,V&gt;[] oldTab = table;</span><br><span class="line">        <span class="comment">//oldTab最大容量</span></span><br><span class="line">        <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;</span><br><span class="line">        <span class="comment">//oldTab阀值</span></span><br><span class="line">        <span class="keyword">int</span> oldThr = threshold;</span><br><span class="line">        <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">//如果老map有值</span></span><br><span class="line">        <span class="keyword">if</span> (oldCap &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// 老table的容量超过最大容量值，设置阈值为Integer.MAX_VALUE，返回老表</span></span><br><span class="line">            <span class="keyword">if</span> (oldCap &gt;= MAXIMUM_CAPACITY) &#123;</span><br><span class="line">                threshold = Integer.MAX_VALUE;</span><br><span class="line">                <span class="keyword">return</span> oldTab;</span><br><span class="line">            <span class="comment">//老table的容量没有超过最大容量值，将新容量赋值为老容量*2，如果新容量&lt;最大容量并且老容量&gt;=16, 则将新阈值设置为原来的两倍</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((newCap = oldCap &lt;&lt; <span class="number">1</span>) &lt; MAXIMUM_CAPACITY &amp;&amp;</span><br><span class="line">                    oldCap &gt;= DEFAULT_INITIAL_CAPACITY)</span><br><span class="line">                newThr = oldThr &lt;&lt; <span class="number">1</span>; <span class="comment">// double threshold</span></span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (oldThr &gt; <span class="number">0</span>)&#123; <span class="comment">// 老表的容量为0, 老表的阈值大于0, 是因为初始容量被放入阈值</span></span><br><span class="line">            newCap = oldThr;    <span class="comment">// 则将新表的容量设置为老表的阈值</span></span><br><span class="line">        <span class="comment">//放第一个值时，对数组容量及阈值进行初始化。</span></span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;   <span class="comment">//老表的容量为0, 老表的阈值为0, 则为空表，设置默认容量和阈值</span></span><br><span class="line">            newCap = DEFAULT_INITIAL_CAPACITY; <span class="comment">//16</span></span><br><span class="line">            newThr = (<span class="keyword">int</span>)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); <span class="comment">//12</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果新阈值为空, 则通过新的容量*负载因子获得新阈值</span></span><br><span class="line">        <span class="keyword">if</span> (newThr == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">float</span> ft = (<span class="keyword">float</span>)newCap * loadFactor;</span><br><span class="line">            newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY ? (<span class="keyword">int</span>)ft : Integer.MAX_VALUE);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将当前阈值赋值为刚计算出来的新的阈值</span></span><br><span class="line">        threshold = newThr;</span><br><span class="line">        <span class="meta">@SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;)</span></span><br><span class="line">        <span class="comment">//初始化数组对象</span></span><br><span class="line">        Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap];</span><br><span class="line">        <span class="comment">//将当前的表赋值为新定义的表</span></span><br><span class="line">        table = newTab;  </span><br><span class="line">        <span class="comment">// 如果老表不为空, 则需遍历将节点赋值给新表</span></span><br><span class="line">        <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">//通过循环将老数组重新赋值给新数组</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) &#123;</span><br><span class="line">                Node&lt;K,V&gt; e;</span><br><span class="line">                <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) &#123; <span class="comment">// 将索引值为j的老表头节点赋值给e</span></span><br><span class="line">                    oldTab[j] = <span class="keyword">null</span>; <span class="comment">//将老表的节点设置为空, 以便垃圾收集器回收空间</span></span><br><span class="line">                    <span class="comment">// 如果e.next为空, 则代表老表的该位置只有1个节点,</span></span><br><span class="line">                    <span class="comment">// 通过hash值计算新表的索引位置, 直接将该节点放在该位置</span></span><br><span class="line">                    <span class="keyword">if</span> (e.next == <span class="keyword">null</span>) <span class="comment">//</span></span><br><span class="line">                        newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e;</span><br><span class="line">                    <span class="comment">//e.next不为空,判断是否是红黑树</span></span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                        ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap);</span><br><span class="line">                    <span class="comment">//是普通链表</span></span><br><span class="line">                    <span class="keyword">else</span> &#123; <span class="comment">// preserve order</span></span><br><span class="line">                        Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line">                        Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">                        Node&lt;K,V&gt; next;</span><br><span class="line">                        <span class="keyword">do</span> &#123;</span><br><span class="line">                            next = e.next;</span><br><span class="line">                            <span class="comment">//如果e的hash值与老表的容量进行与运算为0,则扩容后的索引位置跟老表的索引位置一样</span></span><br><span class="line">                            <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (loTail == <span class="keyword">null</span>)</span><br><span class="line">                                    loHead = e;</span><br><span class="line">                                <span class="keyword">else</span></span><br><span class="line">                                    loTail.next = e;</span><br><span class="line">                                loTail = e;</span><br><span class="line">                            &#125;</span><br><span class="line">                            <span class="comment">//如果e的hash值与老表的容量进行与运算为1,则扩容后的索引位置为:</span></span><br><span class="line">                            <span class="comment">//	老表的索引位置＋oldCap</span></span><br><span class="line">                            <span class="keyword">else</span> &#123;</span><br><span class="line">                                <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>)</span><br><span class="line">                                    hiHead = e;</span><br><span class="line">                                <span class="keyword">else</span></span><br><span class="line">                                    hiTail.next = e;</span><br><span class="line">                                hiTail = e;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125; <span class="keyword">while</span> ((e = next) != <span class="keyword">null</span>);</span><br><span class="line">                        <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            loTail.next = <span class="keyword">null</span>; <span class="comment">// 最后一个节点的next设为空</span></span><br><span class="line">                            newTab[j] = loHead; <span class="comment">// 将原索引位置的节点设置为对应的头结点</span></span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            hiTail.next = <span class="keyword">null</span>; <span class="comment">// 最后一个节点的next设为空</span></span><br><span class="line">                            newTab[j + oldCap] = hiHead; <span class="comment">// 将索引位置为原索引+oldCap的节点设置为对应的头结点</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> newTab;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="9-hashmap-remove-方法"><a href="#9-hashmap-remove-方法" class="headerlink" title="9.hashmap remove()方法"></a>9.hashmap remove()方法</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt; e;</span><br><span class="line">    <span class="keyword">return</span> (e = removeNode(hash(key), key, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">true</span>)) == <span class="keyword">null</span> ?</span><br><span class="line">        <span class="keyword">null</span> : e.value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">final</span> Node&lt;K,V&gt; <span class="title">removeNode</span><span class="params">(<span class="keyword">int</span> hash, Object key, Object value,</span></span></span><br><span class="line"><span class="function"><span class="params">                           <span class="keyword">boolean</span> matchValue, <span class="keyword">boolean</span> movable)</span> </span>&#123;</span><br><span class="line">    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="keyword">int</span> n, index;</span><br><span class="line">    <span class="comment">// 如果table不为空并且根据hash值计算出来的索引位置不为空, 将该位置的节点赋值给p</span></span><br><span class="line">    <span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">        (p = tab[index = (n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        Node&lt;K,V&gt; node = <span class="keyword">null</span>, e; K k; V v;</span><br><span class="line">        <span class="comment">// 如果p的hash值和key都与入参的相同, 则p即为目标节点, 赋值给node</span></span><br><span class="line">        <span class="keyword">if</span> (p.hash == hash &amp;&amp;</span><br><span class="line">            ((k = p.key) == key || (key != <span class="keyword">null</span> &amp;&amp; key.equals(k))))</span><br><span class="line">            node = p;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((e = p.next) != <span class="keyword">null</span>) &#123;    <span class="comment">// 否则向下遍历节点</span></span><br><span class="line">            <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)  <span class="comment">// 如果p是TreeNode则调用红黑树的方法查找节点</span></span><br><span class="line">                node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key);</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">do</span> &#123;    <span class="comment">// 遍历链表查找符合条件的节点</span></span><br><span class="line">                    <span class="comment">// 当节点的hash值和key与传入的相同,则该节点即为目标节点</span></span><br><span class="line">                    <span class="keyword">if</span> (e.hash == hash &amp;&amp;</span><br><span class="line">                        ((k = e.key) == key ||</span><br><span class="line">                         (key != <span class="keyword">null</span> &amp;&amp; key.equals(k)))) &#123;</span><br><span class="line">                        node = e;    <span class="comment">// 赋值给node, 并跳出循环</span></span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    p = e;  <span class="comment">// p节点赋值为本次结束的e</span></span><br><span class="line">                &#125; <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>); <span class="comment">// 指向像一个节点</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果node不为空(即根据传入key和hash值查找到目标节点)，则进行移除操作</span></span><br><span class="line">        <span class="keyword">if</span> (node != <span class="keyword">null</span> &amp;&amp; (!matchValue || (v = node.value) == value ||</span><br><span class="line">                             (value != <span class="keyword">null</span> &amp;&amp; value.equals(v)))) &#123; </span><br><span class="line">            <span class="keyword">if</span> (node <span class="keyword">instanceof</span> TreeNode)   <span class="comment">// 如果是TreeNode则调用红黑树的移除方法</span></span><br><span class="line">                ((TreeNode&lt;K,V&gt;)node).removeTreeNode(<span class="keyword">this</span>, tab, movable);</span><br><span class="line">            <span class="comment">// 走到这代表节点是普通链表节点</span></span><br><span class="line">            <span class="comment">// 如果node是该索引位置的头结点则直接将该索引位置的值赋值为node的next节点</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (node == p)</span><br><span class="line">                tab[index] = node.next;</span><br><span class="line">            <span class="comment">// 否则将node的上一个节点的next属性设置为node的next节点, </span></span><br><span class="line">            <span class="comment">// 即将node节点移除, 将node的上下节点进行关联(链表的移除)    </span></span><br><span class="line">            <span class="keyword">else</span> </span><br><span class="line">                p.next = node.next;</span><br><span class="line">            ++modCount; <span class="comment">// 修改次数+1</span></span><br><span class="line">            --size; <span class="comment">// table的总节点数-1</span></span><br><span class="line">            afterNodeRemoval(node); <span class="comment">// 供LinkedHashMap使用</span></span><br><span class="line">            <span class="keyword">return</span> node;    <span class="comment">// 返回被移除的节点</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="总结篇"><a href="#总结篇" class="headerlink" title="总结篇"></a>总结篇</h2><ol>
<li>在JDK8 HashMap使用的是懒加载模式，也就是说，在默认初始化hashmap的时候，并不会在内存中创建一个长度为16的数组。而是在第一次put数据的时候才会创建。</li>
<li>负载因子的作用：默认负载因子为0.75.也就是说，hashmap在put数据的时候，发现数组中75%的index中都有了数据，就会进行一次扩容。每一次扩容大小均为2^n。(默认情况下，首次初始化数组长度为16，那么扩容阈值就位12)</li>
<li>链表转化为红黑树的条件：1.单个index中的链表长度超过8。 2.当前散列表长度打到64。</li>
<li>put算法：<br>1、对比hash值。如果节点已经存在，则更新原值。<br>2、如果节点不存在，则插入数组中，如果数组已经有值，则判断是否是红黑树，如果是，则调用红黑树方法插入<br>3、如果插入的是链表，插入尾部，然后判断节点数是否超过8，如果超过，则转换为红黑树<br>4、先插入的数据，后面判断是否超过阀值再进行的扩容</li>
</ol>
]]></content>
      <categories>
        <category>java</category>
        <category>hashmap</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title>记录一次hive优化的全过程</title>
    <url>/2020/10/27/hive/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1hive%E4%BC%98%E5%8C%96%E7%9A%84%E5%85%A8%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="记录一次hive优化的全过程"><a href="#记录一次hive优化的全过程" class="headerlink" title="记录一次hive优化的全过程"></a>记录一次hive优化的全过程</h1><p>因为涉及公司隐私，本文所有的sql被精简化，加上脱敏处理。<br>第一版的sql：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="number">1073741824</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.text.output.compress.split<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> wangzy_broker_detail_daily_tmp01;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> wangzy_broker_detail_daily_tmp01 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">	a.broker_id,</span><br><span class="line">	b.broker_id,</span><br><span class="line">	c.broker_id,</span><br><span class="line">	d.broker_id</span><br><span class="line">	e.user_id,</span><br><span class="line">	f.broker_id,</span><br><span class="line">	g.broker_id,</span><br><span class="line">	h.broker_id,</span><br><span class="line">	i.broker_id</span><br><span class="line"><span class="keyword">from</span> dm_qqq_broker_detail_basis_daily a </span><br><span class="line">left outer join dm_qqq_broker_detail_combo_daily b on a.broker_id=b.broker_id and b.cal_dt = $&#123;dealDate&#125; and b.broker_id&gt;0</span><br><span class="line">left outer join dm_qqq_broker_detail_account_daily c on a.broker_id=c.broker_id and c.cal_dt = $&#123;dealDate&#125; and c.broker_id&gt;0</span><br><span class="line">left outer join dm_aaa_broker_detail_normal_daily d on a.broker_id=d.broker_id and d.cal_dt = $&#123;dealDate&#125; and d.broker_id&gt;0</span><br><span class="line">left outer join dm_bbb_broker_detail_normal_daily e on a.user_id=e.user_id and e.cal_dt = $&#123;dealDate&#125; and coalesce(e.user_id,0)&gt;0</span><br><span class="line">left outer join dm_qqq_broker_detail_link_daily f on a.broker_id=f.broker_id and f.cal_dt = $&#123;dealDate&#125; and f.broker_id&gt;0</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">	<span class="keyword">select</span> </span><br><span class="line">		user_id <span class="keyword">as</span> broker_id</span><br><span class="line">	<span class="keyword">from</span>(</span><br><span class="line">			<span class="keyword">select</span> </span><br><span class="line">				user_id</span><br><span class="line">			<span class="keyword">from</span> dw_new_prop_base_esf </span><br><span class="line">			where cal_dt=$&#123;dealDate&#125; and state in(1,2) and substr(add_date,1,10)=$&#123;dealDate&#125; and COALESCE(user_id,0)&gt;0</span><br><span class="line">			<span class="keyword">union</span> <span class="keyword">ALL</span></span><br><span class="line">			<span class="keyword">select</span> </span><br><span class="line">				user_id</span><br><span class="line">			<span class="keyword">from</span> dw_new_prop_base_zf </span><br><span class="line">			where cal_dt=$&#123;dealDate&#125; and state in(1,2) and substr(add_date,1,10)=$&#123;dealDate&#125; and COALESCE(user_id,0)&gt;0</span><br><span class="line">			<span class="keyword">union</span> <span class="keyword">ALL</span></span><br><span class="line">			<span class="keyword">select</span> </span><br><span class="line">				user_id</span><br><span class="line">			<span class="keyword">from</span> dw_new_prop_base_sydc </span><br><span class="line">			where cal_dt=$&#123;dealDate&#125; and state in(1,2) and substr(add_date,1,10)=$&#123;dealDate&#125; and COALESCE(user_id,0)&gt;0</span><br><span class="line">	)aa <span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line">) g <span class="keyword">on</span> a.broker_id<span class="operator">=</span>g.broker_id</span><br><span class="line"></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">	<span class="keyword">select</span> </span><br><span class="line">		broker_id</span><br><span class="line">	<span class="keyword">from</span> dm_aaa_user_coupon_daily </span><br><span class="line">	where cal_dt=$&#123;dealDate&#125;</span><br><span class="line">	<span class="keyword">group</span> <span class="keyword">by</span> broker_id</span><br><span class="line">) h <span class="keyword">on</span> a.broker_id<span class="operator">=</span>h.broker_id <span class="keyword">and</span> h.broker_id<span class="operator">&gt;</span><span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">	<span class="keyword">select</span> </span><br><span class="line">		 broker_id</span><br><span class="line">	<span class="keyword">from</span> dm_house_daily</span><br><span class="line">	where cal_dt=$&#123;dealDate&#125;</span><br><span class="line">	<span class="keyword">group</span> <span class="keyword">by</span> broker_id</span><br><span class="line">) i <span class="keyword">on</span> a.broker_id<span class="operator">=</span>i.broker_id <span class="keyword">and</span> i.broker_id<span class="operator">&gt;</span><span class="number">0</span></span><br><span class="line"></span><br><span class="line">left outer join da_qqq_broker_quality_daily j on a.broker_id = j.broker_id and j.cal_dt = $&#123;dealDate&#125;</span><br><span class="line"></span><br><span class="line">where a.cal_dt = $&#123;dealDate&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">use hdp_anjuke_dw_db;</span><br><span class="line">alter table dm_qqq_broker_detail_v2_daily drop partition(cal_dt = $&#123;dealDate&#125;);</span><br><span class="line"></span><br><span class="line">insert overwrite table dm_broker_detail_daily partition(cal_dt = $&#123;dealDate&#125;)</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">a.<span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> hdp_anjuke_dw_stage.wangzy_broker_detail_daily_tmp01 a</span><br><span class="line">join dw_qqq_account_city_config_daily cfg on a.city_id=cfg.city_id_ajk and cfg.cal_dt=$&#123;dealDate&#125;</span><br></pre></td></tr></table></figure>
<h2 id="第一次优化-开启mapjoin"><a href="#第一次优化-开启mapjoin" class="headerlink" title="第一次优化:开启mapjoin"></a>第一次优化:开启mapjoin</h2><p>本次sql运行时长在2小时46分，肯定是一个待优化的sql，所以首先从日志入手。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2020-10-15 10:43:27 - insert overwrite table dm_qqq_broker_detail_v2_daily partition(cal_dt &#x3D; &#39;2020-10-14&#39;)</span><br><span class="line">2020-10-15 10:43:27 - select </span><br><span class="line">2020-10-15 10:43:27 - a.*</span><br><span class="line">2020-10-15 10:43:27 - from wangzy_broker_detail_daily_tmp01 a</span><br><span class="line">2020-10-15 10:43:27 - join dw_qwt_account_city_config_daily cfg on a.city_id&#x3D;cfg.city_id_ajk and cfg.cal_dt&#x3D;&#39;2020-10-14&#39;;</span><br><span class="line">2020-10-15 10:43:27 - INFO  HiveRunner - This is not a VIEW JOB...</span><br><span class="line">2020-10-15 10:43:27 - INFO  CommandLogger - ls: cannot access &#x2F;usr&#x2F;lib&#x2F;software&#x2F;spark&#x2F;lib&#x2F;spark-assembly-*.jar: No such file or directory</span><br><span class="line">2020-10-15 10:43:29 - INFO  CommandLogger - 20&#x2F;10&#x2F;15 10:43:29 WARN conf.HiveConf main: HiveConf of name hive.metastore.local does not exist</span><br><span class="line">2020-10-15 10:43:29 - INFO  CommandLogger - 20&#x2F;10&#x2F;15 10:43:29 WARN conf.HiveConf main: HiveConf of name hive.files.umask.value does not exist</span><br><span class="line">2020-10-15 10:43:29 - INFO  CommandLogger - </span><br><span class="line">2020-10-15 10:43:29 - INFO  CommandLogger - Logging initialized using configuration in file:&#x2F;usr&#x2F;lib&#x2F;software&#x2F;apache-hive-1.1.0U16.1-cdh5.4.4-bin&#x2F;conf&#x2F;hive-log4j.properties</span><br><span class="line">2020-10-15 10:43:29 - INFO  CommandLogger - SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">2020-10-15 10:43:29 - INFO  CommandLogger - SLF4J: See http:&#x2F;&#x2F;www.slf4j.org&#x2F;codes.html#multiple_bindings for an explanation.</span><br><span class="line">2020-10-15 10:43:29 - INFO  CommandLogger - SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]</span><br><span class="line">2020-10-15 10:43:37 - INFO  CommandLogger - OK</span><br><span class="line">2020-10-15 10:43:37 - INFO  CommandLogger - Time taken: 0.683 seconds</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger - Query ID &#x3D; hadoop_20201015104343_10a0c26d-63ab-4925-abc7-2830ca650e5f</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger - Total jobs &#x3D; 3</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger - Launching Job 1 out of 3</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger - Number of reduce tasks not specified. Estimated from input data size: 2</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger - In order to change the average load for a reducer (in bytes):</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger -   set hive.exec.reducers.bytes.per.reducer&#x3D;&lt;number&gt;</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger - In order to limit the maximum number of reducers:</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger -   set hive.exec.reducers.max&#x3D;&lt;number&gt;</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger - In order to set a constant number of reducers:</span><br><span class="line">2020-10-15 10:43:39 - INFO  CommandLogger -   set mapreduce.job.reduces&#x3D;&lt;number&gt;</span><br><span class="line">2020-10-15 10:43:45 - INFO  CommandLogger - Starting Job &#x3D; job_1591776234013_38784236, Tracking URL &#x3D; http:&#x2F;&#x2F;wq-yarn1-rm2.58dns.org:9088&#x2F;proxy&#x2F;application_1591776234013_38784236&#x2F;</span><br><span class="line">2020-10-15 10:43:45 - INFO  CommandLogger - Kill Command &#x3D; &#x2F;usr&#x2F;lib&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop job  -kill job_1591776234013_38784236</span><br><span class="line">2020-10-15 10:45:02 - INFO  CommandLogger - Hadoop job information for Stage-1: number of mappers: 7; number of reducers: 2</span><br><span class="line">2020-10-15 10:45:02 - INFO  CommandLogger - 2020-10-15 10:45:02,110 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class="line">2020-10-15 10:45:13 - INFO  CommandLogger - 2020-10-15 10:45:13,676 Stage-1 map &#x3D; 14%,  reduce &#x3D; 0%, Cumulative CPU 3.34 sec</span><br><span class="line">2020-10-15 10:45:35 - INFO  CommandLogger - 2020-10-15 10:45:35,530 Stage-1 map &#x3D; 15%,  reduce &#x3D; 0%, Cumulative CPU 175.21 sec</span><br><span class="line">2020-10-15 10:46:35 - INFO  CommandLogger - 2020-10-15 10:46:35,729 Stage-1 map &#x3D; 15%,  reduce &#x3D; 0%, Cumulative CPU 590.7 sec</span><br><span class="line">2020-10-15 10:47:13 - INFO  CommandLogger - 2020-10-15 10:47:13,956 Stage-1 map &#x3D; 19%,  reduce &#x3D; 0%, Cumulative CPU 848.16 sec</span><br><span class="line">2020-10-15 10:47:33 - INFO  CommandLogger - 2020-10-15 10:47:33,604 Stage-1 map &#x3D; 24%,  reduce &#x3D; 0%, Cumulative CPU 978.12 sec</span><br><span class="line">2020-10-15 10:47:45 - INFO  CommandLogger - 2020-10-15 10:47:45,980 Stage-1 map &#x3D; 26%,  reduce &#x3D; 0%, Cumulative CPU 1057.46 sec</span><br><span class="line">2020-10-15 10:47:59 - INFO  CommandLogger - 2020-10-15 10:47:59,482 Stage-1 map &#x3D; 31%,  reduce &#x3D; 0%, Cumulative CPU 1152.96 sec</span><br><span class="line">2020-10-15 10:48:13 - INFO  CommandLogger - 2020-10-15 10:48:13,918 Stage-1 map &#x3D; 33%,  reduce &#x3D; 0%, Cumulative CPU 1246.33 sec</span><br><span class="line">2020-10-15 10:48:21 - INFO  CommandLogger - 2020-10-15 10:48:21,137 Stage-1 map &#x3D; 35%,  reduce &#x3D; 0%, Cumulative CPU 1298.7 sec</span><br><span class="line">2020-10-15 10:49:21 - INFO  CommandLogger - 2020-10-15 10:49:21,958 Stage-1 map &#x3D; 35%,  reduce &#x3D; 0%, Cumulative CPU 1708.65 sec</span><br><span class="line">2020-10-15 10:49:40 - INFO  CommandLogger - 2020-10-15 10:49:40,496 Stage-1 map &#x3D; 37%,  reduce &#x3D; 0%, Cumulative CPU 1835.72 sec</span><br><span class="line">2020-10-15 10:49:55 - INFO  CommandLogger - 2020-10-15 10:49:55,958 Stage-1 map &#x3D; 39%,  reduce &#x3D; 0%, Cumulative CPU 1941.92 sec</span><br><span class="line">2020-10-15 10:50:08 - INFO  CommandLogger - 2020-10-15 10:50:08,316 Stage-1 map &#x3D; 42%,  reduce &#x3D; 0%, Cumulative CPU 2023.94 sec</span><br><span class="line">2020-10-15 10:50:43 - INFO  CommandLogger - 2020-10-15 10:50:43,368 Stage-1 map &#x3D; 50%,  reduce &#x3D; 0%, Cumulative CPU 2263.36 sec</span><br><span class="line">2020-10-15 10:50:47 - INFO  CommandLogger - 2020-10-15 10:50:47,504 Stage-1 map &#x3D; 51%,  reduce &#x3D; 0%, Cumulative CPU 2236.66 sec</span><br><span class="line">2020-10-15 10:50:49 - INFO  CommandLogger - 2020-10-15 10:50:49,564 Stage-1 map &#x3D; 54%,  reduce &#x3D; 0%, Cumulative CPU 2304.41 sec</span><br><span class="line">2020-10-15 10:50:53 - INFO  CommandLogger - 2020-10-15 10:50:53,695 Stage-1 map &#x3D; 58%,  reduce &#x3D; 0%, Cumulative CPU 2339.7 sec</span><br><span class="line">2020-10-15 10:51:54 - INFO  CommandLogger - 2020-10-15 10:51:54,605 Stage-1 map &#x3D; 62%,  reduce &#x3D; 0%, Cumulative CPU 2671.11 sec</span><br><span class="line">2020-10-15 10:51:57 - INFO  CommandLogger - 2020-10-15 10:51:57,702 Stage-1 map &#x3D; 65%,  reduce &#x3D; 0%, Cumulative CPU 2687.17 sec</span><br><span class="line">2020-10-15 10:52:06 - INFO  CommandLogger - 2020-10-15 10:52:06,960 Stage-1 map &#x3D; 67%,  reduce &#x3D; 0%, Cumulative CPU 2735.25 sec</span><br><span class="line">2020-10-15 10:52:16 - INFO  CommandLogger - 2020-10-15 10:52:16,223 Stage-1 map &#x3D; 72%,  reduce &#x3D; 0%, Cumulative CPU 2733.19 sec</span><br><span class="line">2020-10-15 10:52:18 - INFO  CommandLogger - 2020-10-15 10:52:18,298 Stage-1 map &#x3D; 73%,  reduce &#x3D; 0%, Cumulative CPU 2736.4 sec</span><br><span class="line">2020-10-15 10:52:21 - INFO  CommandLogger - 2020-10-15 10:52:21,384 Stage-1 map &#x3D; 75%,  reduce &#x3D; 0%, Cumulative CPU 2750.12 sec</span><br><span class="line">2020-10-15 10:52:31 - INFO  CommandLogger - 2020-10-15 10:52:31,662 Stage-1 map &#x3D; 80%,  reduce &#x3D; 0%, Cumulative CPU 2796.53 sec</span><br><span class="line">2020-10-15 10:52:37 - INFO  CommandLogger - 2020-10-15 10:52:37,830 Stage-1 map &#x3D; 82%,  reduce &#x3D; 0%, Cumulative CPU 2817.78 sec</span><br><span class="line">2020-10-15 10:52:42 - INFO  CommandLogger - 2020-10-15 10:52:42,980 Stage-1 map &#x3D; 84%,  reduce &#x3D; 0%, Cumulative CPU 2831.15 sec</span><br><span class="line">2020-10-15 10:53:43 - INFO  CommandLogger - 2020-10-15 10:53:43,714 Stage-1 map &#x3D; 84%,  reduce &#x3D; 0%, Cumulative CPU 2958.42 sec</span><br><span class="line">2020-10-15 10:54:04 - INFO  CommandLogger - 2020-10-15 10:54:04,284 Stage-1 map &#x3D; 88%,  reduce &#x3D; 0%, Cumulative CPU 2999.64 sec</span><br><span class="line">2020-10-15 10:54:07 - INFO  CommandLogger - 2020-10-15 10:54:07,367 Stage-1 map &#x3D; 90%,  reduce &#x3D; 0%, Cumulative CPU 3005.6 sec</span><br><span class="line">2020-10-15 10:54:16 - INFO  CommandLogger - 2020-10-15 10:54:16,631 Stage-1 map &#x3D; 93%,  reduce &#x3D; 0%, Cumulative CPU 3023.81 sec</span><br><span class="line">2020-10-15 10:55:17 - INFO  CommandLogger - 2020-10-15 10:55:17,450 Stage-1 map &#x3D; 93%,  reduce &#x3D; 0%, Cumulative CPU 3101.93 sec</span><br><span class="line">2020-10-15 10:55:21 - INFO  CommandLogger - 2020-10-15 10:55:21,570 Stage-1 map &#x3D; 98%,  reduce &#x3D; 0%, Cumulative CPU 3109.75 sec</span><br><span class="line">2020-10-15 10:55:30 - INFO  CommandLogger - 2020-10-15 10:55:30,823 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 3119.14 sec</span><br><span class="line">2020-10-15 10:55:50 - INFO  CommandLogger - 2020-10-15 10:55:50,564 Stage-1 map &#x3D; 100%,  reduce &#x3D; 67%, Cumulative CPU 3148.68 sec</span><br><span class="line">2020-10-15 10:56:37 - INFO  CommandLogger - 2020-10-15 10:56:37,250 Stage-1 map &#x3D; 100%,  reduce &#x3D; 68%, Cumulative CPU 3280.13 sec</span><br><span class="line">2020-10-15 10:57:37 - INFO  CommandLogger - 2020-10-15 10:57:37,992 Stage-1 map &#x3D; 100%,  reduce &#x3D; 68%, Cumulative CPU 3474.28 sec</span><br><span class="line">2020-10-15 10:57:39 - INFO  CommandLogger - 2020-10-15 10:57:39,020 Stage-1 map &#x3D; 100%,  reduce &#x3D; 69%, Cumulative CPU 3487.24 sec</span><br><span class="line">2020-10-15 10:58:39 - INFO  CommandLogger - 2020-10-15 10:58:39,876 Stage-1 map &#x3D; 100%,  reduce &#x3D; 69%, Cumulative CPU 3696.85 sec</span><br><span class="line">2020-10-15 10:59:07 - INFO  CommandLogger - 2020-10-15 10:59:07,920 Stage-1 map &#x3D; 100%,  reduce &#x3D; 70%, Cumulative CPU 3790.21 sec</span><br><span class="line">2020-10-15 11:00:07 - INFO  CommandLogger - 2020-10-15 11:00:07,945 Stage-1 map &#x3D; 100%,  reduce &#x3D; 70%, Cumulative CPU 4016.91 sec</span><br><span class="line">2020-10-15 11:00:33 - INFO  CommandLogger - 2020-10-15 11:00:33,773 Stage-1 map &#x3D; 100%,  reduce &#x3D; 71%, Cumulative CPU 4109.18 sec</span><br><span class="line">2020-10-15 11:01:33 - INFO  CommandLogger - 2020-10-15 11:01:33,541 Stage-1 map &#x3D; 100%,  reduce &#x3D; 72%, Cumulative CPU 4316.44 sec</span><br><span class="line">2020-10-15 11:02:34 - INFO  CommandLogger - 2020-10-15 11:02:34,232 Stage-1 map &#x3D; 100%,  reduce &#x3D; 72%, Cumulative CPU 4510.48 sec</span><br><span class="line">2020-10-15 11:02:58 - INFO  CommandLogger - 2020-10-15 11:02:58,887 Stage-1 map &#x3D; 100%,  reduce &#x3D; 73%, Cumulative CPU 4600.11 sec</span><br><span class="line">2020-10-15 11:03:59 - INFO  CommandLogger - 2020-10-15 11:03:59,564 Stage-1 map &#x3D; 100%,  reduce &#x3D; 73%, Cumulative CPU 4806.97 sec</span><br><span class="line">2020-10-15 11:05:00 - INFO  CommandLogger - 2020-10-15 11:05:00,238 Stage-1 map &#x3D; 100%,  reduce &#x3D; 73%, Cumulative CPU 5013.44 sec</span><br><span class="line">2020-10-15 11:05:02 - INFO  CommandLogger - 2020-10-15 11:05:02,302 Stage-1 map &#x3D; 100%,  reduce &#x3D; 74%, Cumulative CPU 5022.53 sec</span><br><span class="line">2020-10-15 11:06:02 - INFO  CommandLogger - 2020-10-15 11:06:02,474 Stage-1 map &#x3D; 100%,  reduce &#x3D; 74%, Cumulative CPU 5240.58 sec</span><br><span class="line">2020-10-15 11:07:02 - INFO  CommandLogger - 2020-10-15 11:07:02,569 Stage-1 map &#x3D; 100%,  reduce &#x3D; 74%, Cumulative CPU 5470.51 sec</span><br><span class="line">2020-10-15 11:08:03 - INFO  CommandLogger - 2020-10-15 11:08:03,427 Stage-1 map &#x3D; 100%,  reduce &#x3D; 74%, Cumulative CPU 5723.89 sec</span><br><span class="line">2020-10-15 11:08:36 - INFO  CommandLogger - 2020-10-15 11:08:36,313 Stage-1 map &#x3D; 100%,  reduce &#x3D; 75%, Cumulative CPU 5856.92 sec</span><br><span class="line">2020-10-15 11:09:36 - INFO  CommandLogger - 2020-10-15 11:09:36,891 Stage-1 map &#x3D; 100%,  reduce &#x3D; 75%, Cumulative CPU 6086.42 sec</span><br><span class="line">2020-10-15 11:10:15 - INFO  CommandLogger - 2020-10-15 11:10:15,957 Stage-1 map &#x3D; 100%,  reduce &#x3D; 76%, Cumulative CPU 6232.2 sec</span><br><span class="line">2020-10-15 11:11:16 - INFO  CommandLogger - 2020-10-15 11:11:16,611 Stage-1 map &#x3D; 100%,  reduce &#x3D; 76%, Cumulative CPU 6478.2 sec</span><br><span class="line">2020-10-15 11:12:17 - INFO  CommandLogger - 2020-10-15 11:12:17,303 Stage-1 map &#x3D; 100%,  reduce &#x3D; 76%, Cumulative CPU 6700.76 sec</span><br><span class="line">2020-10-15 11:12:58 - INFO  CommandLogger - 2020-10-15 11:12:58,476 Stage-1 map &#x3D; 100%,  reduce &#x3D; 77%, Cumulative CPU 6859.71 sec</span><br><span class="line">2020-10-15 11:13:59 - INFO  CommandLogger - 2020-10-15 11:13:59,170 Stage-1 map &#x3D; 100%,  reduce &#x3D; 77%, Cumulative CPU 7070.44 sec</span><br><span class="line">2020-10-15 11:14:01 - INFO  CommandLogger - 2020-10-15 11:14:01,220 Stage-1 map &#x3D; 100%,  reduce &#x3D; 78%, Cumulative CPU 7078.4 sec</span><br><span class="line">2020-10-15 11:15:01 - INFO  CommandLogger - 2020-10-15 11:15:01,725 Stage-1 map &#x3D; 100%,  reduce &#x3D; 78%, Cumulative CPU 7288.16 sec</span><br><span class="line">2020-10-15 11:15:41 - INFO  CommandLogger - 2020-10-15 11:15:41,770 Stage-1 map &#x3D; 100%,  reduce &#x3D; 79%, Cumulative CPU 7440.69 sec</span><br><span class="line">2020-10-15 11:16:42 - INFO  CommandLogger - 2020-10-15 11:16:42,298 Stage-1 map &#x3D; 100%,  reduce &#x3D; 79%, Cumulative CPU 7664.06 sec</span><br><span class="line">2020-10-15 11:17:43 - INFO  CommandLogger - 2020-10-15 11:17:43,260 Stage-1 map &#x3D; 100%,  reduce &#x3D; 79%, Cumulative CPU 7868.57 sec</span><br><span class="line">2020-10-15 11:17:52 - INFO  CommandLogger - 2020-10-15 11:17:52,542 Stage-1 map &#x3D; 100%,  reduce &#x3D; 80%, Cumulative CPU 7892.0 sec</span><br><span class="line">2020-10-15 11:18:53 - INFO  CommandLogger - 2020-10-15 11:18:53,505 Stage-1 map &#x3D; 100%,  reduce &#x3D; 80%, Cumulative CPU 8099.71 sec</span><br><span class="line">2020-10-15 11:19:54 - INFO  CommandLogger - 2020-10-15 11:19:54,142 Stage-1 map &#x3D; 100%,  reduce &#x3D; 80%, Cumulative CPU 8307.22 sec</span><br><span class="line">2020-10-15 11:20:06 - INFO  CommandLogger - 2020-10-15 11:20:06,605 Stage-1 map &#x3D; 100%,  reduce &#x3D; 81%, Cumulative CPU 8358.3 sec</span><br><span class="line">2020-10-15 11:21:07 - INFO  CommandLogger - 2020-10-15 11:21:07,159 Stage-1 map &#x3D; 100%,  reduce &#x3D; 81%, Cumulative CPU 8575.22 sec</span><br><span class="line">2020-10-15 11:21:25 - INFO  CommandLogger - 2020-10-15 11:21:25,652 Stage-1 map &#x3D; 100%,  reduce &#x3D; 82%, Cumulative CPU 8637.31 sec</span><br><span class="line">2020-10-15 11:22:26 - INFO  CommandLogger - 2020-10-15 11:22:26,363 Stage-1 map &#x3D; 100%,  reduce &#x3D; 82%, Cumulative CPU 8848.1 sec</span><br><span class="line">2020-10-15 11:23:26 - INFO  CommandLogger - 2020-10-15 11:23:26,997 Stage-1 map &#x3D; 100%,  reduce &#x3D; 82%, Cumulative CPU 9038.11 sec</span><br><span class="line">2020-10-15 11:23:54 - INFO  CommandLogger - 2020-10-15 11:23:54,660 Stage-1 map &#x3D; 100%,  reduce &#x3D; 83%, Cumulative CPU 9127.46 sec</span><br><span class="line">2020-10-15 11:24:55 - INFO  CommandLogger - 2020-10-15 11:24:55,096 Stage-1 map &#x3D; 100%,  reduce &#x3D; 83%, Cumulative CPU 9321.88 sec</span><br><span class="line">2020-10-15 11:25:20 - INFO  CommandLogger - 2020-10-15 11:25:20,755 Stage-1 map &#x3D; 100%,  reduce &#x3D; 84%, Cumulative CPU 9389.88 sec</span><br><span class="line">2020-10-15 11:26:21 - INFO  CommandLogger - 2020-10-15 11:26:21,205 Stage-1 map &#x3D; 100%,  reduce &#x3D; 84%, Cumulative CPU 9585.18 sec</span><br><span class="line">2020-10-15 11:27:21 - INFO  CommandLogger - 2020-10-15 11:27:21,669 Stage-1 map &#x3D; 100%,  reduce &#x3D; 84%, Cumulative CPU 9793.14 sec</span><br><span class="line">2020-10-15 11:27:40 - INFO  CommandLogger - 2020-10-15 11:27:40,412 Stage-1 map &#x3D; 100%,  reduce &#x3D; 85%, Cumulative CPU 9867.98 sec</span><br><span class="line">2020-10-15 11:28:40 - INFO  CommandLogger - 2020-10-15 11:28:40,944 Stage-1 map &#x3D; 100%,  reduce &#x3D; 85%, Cumulative CPU 10081.2 sec</span><br><span class="line">2020-10-15 11:29:20 - INFO  CommandLogger - 2020-10-15 11:29:20,895 Stage-1 map &#x3D; 100%,  reduce &#x3D; 86%, Cumulative CPU 10215.68 sec</span><br><span class="line">2020-10-15 11:30:21 - INFO  CommandLogger - 2020-10-15 11:30:21,648 Stage-1 map &#x3D; 100%,  reduce &#x3D; 86%, Cumulative CPU 10429.4 sec</span><br><span class="line">2020-10-15 11:31:15 - INFO  CommandLogger - 2020-10-15 11:31:15,194 Stage-1 map &#x3D; 100%,  reduce &#x3D; 87%, Cumulative CPU 10623.16 sec</span><br><span class="line">2020-10-15 11:32:15 - INFO  CommandLogger - 2020-10-15 11:32:15,694 Stage-1 map &#x3D; 100%,  reduce &#x3D; 87%, Cumulative CPU 10842.0 sec</span><br><span class="line">2020-10-15 11:32:59 - INFO  CommandLogger - 2020-10-15 11:32:59,905 Stage-1 map &#x3D; 100%,  reduce &#x3D; 88%, Cumulative CPU 11015.27 sec</span><br><span class="line">2020-10-15 11:34:00 - INFO  CommandLogger - 2020-10-15 11:34:00,676 Stage-1 map &#x3D; 100%,  reduce &#x3D; 88%, Cumulative CPU 11235.16 sec</span><br><span class="line">2020-10-15 11:34:36 - INFO  CommandLogger - 2020-10-15 11:34:36,661 Stage-1 map &#x3D; 100%,  reduce &#x3D; 89%, Cumulative CPU 11347.48 sec</span><br><span class="line">2020-10-15 11:35:37 - INFO  CommandLogger - 2020-10-15 11:35:37,327 Stage-1 map &#x3D; 100%,  reduce &#x3D; 89%, Cumulative CPU 11546.86 sec</span><br><span class="line">2020-10-15 11:36:04 - INFO  CommandLogger - 2020-10-15 11:36:04,108 Stage-1 map &#x3D; 100%,  reduce &#x3D; 90%, Cumulative CPU 11639.91 sec</span><br><span class="line">2020-10-15 11:37:04 - INFO  CommandLogger - 2020-10-15 11:37:04,747 Stage-1 map &#x3D; 100%,  reduce &#x3D; 90%, Cumulative CPU 11852.84 sec</span><br><span class="line">2020-10-15 11:37:51 - INFO  CommandLogger - 2020-10-15 11:37:51,860 Stage-1 map &#x3D; 100%,  reduce &#x3D; 91%, Cumulative CPU 11990.68 sec</span><br><span class="line">2020-10-15 11:38:52 - INFO  CommandLogger - 2020-10-15 11:38:52,275 Stage-1 map &#x3D; 100%,  reduce &#x3D; 91%, Cumulative CPU 12189.24 sec</span><br><span class="line">2020-10-15 11:39:22 - INFO  CommandLogger - 2020-10-15 11:39:22,995 Stage-1 map &#x3D; 100%,  reduce &#x3D; 92%, Cumulative CPU 12302.73 sec</span><br><span class="line">2020-10-15 11:40:23 - INFO  CommandLogger - 2020-10-15 11:40:23,508 Stage-1 map &#x3D; 100%,  reduce &#x3D; 92%, Cumulative CPU 12497.38 sec</span><br><span class="line">2020-10-15 11:40:36 - INFO  CommandLogger - 2020-10-15 11:40:36,931 Stage-1 map &#x3D; 100%,  reduce &#x3D; 93%, Cumulative CPU 12554.85 sec</span><br><span class="line">2020-10-15 11:41:37 - INFO  CommandLogger - 2020-10-15 11:41:37,492 Stage-1 map &#x3D; 100%,  reduce &#x3D; 93%, Cumulative CPU 12750.91 sec</span><br><span class="line">2020-10-15 11:41:51 - INFO  CommandLogger - 2020-10-15 11:41:51,831 Stage-1 map &#x3D; 100%,  reduce &#x3D; 94%, Cumulative CPU 12796.6 sec</span><br><span class="line">2020-10-15 11:42:40 - INFO  CommandLogger - 2020-10-15 11:42:40,927 Stage-1 map &#x3D; 100%,  reduce &#x3D; 95%, Cumulative CPU 12969.88 sec</span><br><span class="line">2020-10-15 11:43:39 - INFO  CommandLogger - 2020-10-15 11:43:39,777 Stage-1 map &#x3D; 100%,  reduce &#x3D; 96%, Cumulative CPU 13162.3 sec</span><br><span class="line">2020-10-15 11:44:17 - INFO  CommandLogger - 2020-10-15 11:44:17,876 Stage-1 map &#x3D; 100%,  reduce &#x3D; 97%, Cumulative CPU 13292.29 sec</span><br><span class="line">2020-10-15 11:44:55 - INFO  CommandLogger - 2020-10-15 11:44:55,866 Stage-1 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 13421.68 sec</span><br><span class="line">2020-10-15 11:45:56 - INFO  CommandLogger - 2020-10-15 11:45:56,621 Stage-1 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 13534.96 sec</span><br><span class="line">2020-10-15 11:46:13 - INFO  CommandLogger - 2020-10-15 11:46:13,087 Stage-1 map &#x3D; 100%,  reduce &#x3D; 99%, Cumulative CPU 13563.99 sec</span><br><span class="line">2020-10-15 11:47:13 - INFO  CommandLogger - 2020-10-15 11:47:13,719 Stage-1 map &#x3D; 100%,  reduce &#x3D; 99%, Cumulative CPU 13664.41 sec</span><br><span class="line">2020-10-15 11:47:59 - INFO  CommandLogger - 2020-10-15 11:47:59,880 Stage-1 map &#x3D; 100%,  reduce &#x3D; 100%, Cumulative CPU 13737.04 sec</span><br><span class="line">2020-10-15 11:48:01 - INFO  CommandLogger - MapReduce Total cumulative CPU time: 0 days 3 hours 48 minutes 57 seconds 40 msec</span><br><span class="line">2020-10-15 11:48:01 - INFO  CommandLogger - Ended Job &#x3D; job_1591776234013_38784236</span><br><span class="line">2020-10-15 11:48:03 - INFO  CommandLogger - Stage-4 is filtered out by condition resolver.</span><br><span class="line">2020-10-15 11:48:03 - INFO  CommandLogger - Stage-3 is selected by condition resolver.</span><br><span class="line">2020-10-15 11:48:03 - INFO  CommandLogger - Stage-5 is filtered out by condition resolver.</span><br><span class="line">2020-10-15 11:48:05 - INFO  CommandLogger - Launching Job 3 out of 3</span><br><span class="line">2020-10-15 11:48:05 - INFO  CommandLogger - Number of reduce tasks is set to 0 since there&#39;s no reduce operator</span><br><span class="line">2020-10-15 11:48:08 - INFO  CommandLogger - Starting Job &#x3D; job_1591776234013_38803470, Tracking URL &#x3D; http:&#x2F;&#x2F;wq-yarn1-rm2.58dns.org:9088&#x2F;proxy&#x2F;application_1591776234013_38803470&#x2F;</span><br><span class="line">2020-10-15 11:48:08 - INFO  CommandLogger - Kill Command &#x3D; &#x2F;usr&#x2F;lib&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop job  -kill job_1591776234013_38803470</span><br><span class="line">2020-10-15 11:48:31 - INFO  CommandLogger - Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 0</span><br><span class="line">2020-10-15 11:48:31 - INFO  CommandLogger - 2020-10-15 11:48:31,985 Stage-3 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class="line">2020-10-15 11:49:32 - INFO  CommandLogger - 2020-10-15 11:49:32,219 Stage-3 map &#x3D; 0%,  reduce &#x3D; 0%, Cumulative CPU 258.89 sec</span><br><span class="line">2020-10-15 11:49:45 - INFO  CommandLogger - 2020-10-15 11:49:45,678 Stage-3 map &#x3D; 2%,  reduce &#x3D; 0%, Cumulative CPU 320.74 sec</span><br><span class="line">2020-10-15 11:50:46 - INFO  CommandLogger - 2020-10-15 11:50:46,676 Stage-3 map &#x3D; 2%,  reduce &#x3D; 0%, Cumulative CPU 627.4 sec</span><br><span class="line">2020-10-15 11:51:26 - INFO  CommandLogger - 2020-10-15 11:51:26,018 Stage-3 map &#x3D; 9%,  reduce &#x3D; 0%, Cumulative CPU 811.35 sec</span><br><span class="line">2020-10-15 11:51:30 - INFO  CommandLogger - 2020-10-15 11:51:30,134 Stage-3 map &#x3D; 29%,  reduce &#x3D; 0%, Cumulative CPU 831.16 sec</span><br><span class="line">2020-10-15 11:51:31 - INFO  CommandLogger - 2020-10-15 11:51:31,159 Stage-3 map &#x3D; 36%,  reduce &#x3D; 0%, Cumulative CPU 834.16 sec</span><br><span class="line">2020-10-15 11:51:33 - INFO  CommandLogger - 2020-10-15 11:51:33,240 Stage-3 map &#x3D; 43%,  reduce &#x3D; 0%, Cumulative CPU 842.08 sec</span><br><span class="line">2020-10-15 11:52:33 - INFO  CommandLogger - 2020-10-15 11:52:33,339 Stage-3 map &#x3D; 43%,  reduce &#x3D; 0%, Cumulative CPU 1055.24 sec</span><br><span class="line">2020-10-15 11:52:38 - INFO  CommandLogger - 2020-10-15 11:52:38,547 Stage-3 map &#x3D; 48%,  reduce &#x3D; 0%, Cumulative CPU 1072.15 sec</span><br><span class="line">2020-10-15 11:53:38 - INFO  CommandLogger - 2020-10-15 11:53:38,621 Stage-3 map &#x3D; 48%,  reduce &#x3D; 0%, Cumulative CPU 1301.1 sec</span><br><span class="line">2020-10-15 11:54:02 - INFO  CommandLogger - 2020-10-15 11:54:02,531 Stage-3 map &#x3D; 54%,  reduce &#x3D; 0%, Cumulative CPU 1385.39 sec</span><br><span class="line">2020-10-15 11:54:19 - INFO  CommandLogger - 2020-10-15 11:54:19,058 Stage-3 map &#x3D; 61%,  reduce &#x3D; 0%, Cumulative CPU 1423.86 sec</span><br><span class="line">2020-10-15 11:54:40 - INFO  CommandLogger - 2020-10-15 11:54:40,723 Stage-3 map &#x3D; 67%,  reduce &#x3D; 0%, Cumulative CPU 1527.8 sec</span><br><span class="line">2020-10-15 11:55:34 - INFO  CommandLogger - 2020-10-15 11:55:34,333 Stage-3 map &#x3D; 73%,  reduce &#x3D; 0%, Cumulative CPU 1726.6 sec</span><br><span class="line">2020-10-15 11:56:34 - INFO  CommandLogger - 2020-10-15 11:56:34,717 Stage-3 map &#x3D; 73%,  reduce &#x3D; 0%, Cumulative CPU 1944.37 sec</span><br><span class="line">2020-10-15 11:56:35 - INFO  CommandLogger - 2020-10-15 11:56:35,744 Stage-3 map &#x3D; 80%,  reduce &#x3D; 0%, Cumulative CPU 1946.54 sec</span><br><span class="line">2020-10-15 11:57:14 - INFO  CommandLogger - 2020-10-15 11:57:14,163 Stage-3 map &#x3D; 87%,  reduce &#x3D; 0%, Cumulative CPU 2051.26 sec</span><br><span class="line">2020-10-15 11:57:31 - INFO  CommandLogger - 2020-10-15 11:57:31,644 Stage-3 map &#x3D; 94%,  reduce &#x3D; 0%, Cumulative CPU 2085.4 sec</span><br><span class="line">2020-10-15 11:58:32 - INFO  CommandLogger - 2020-10-15 11:58:32,303 Stage-3 map &#x3D; 94%,  reduce &#x3D; 0%, Cumulative CPU 2140.42 sec</span><br><span class="line">2020-10-15 11:58:47 - INFO  CommandLogger - 2020-10-15 11:58:47,830 Stage-3 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 2153.37 sec</span><br><span class="line">2020-10-15 11:58:49 - INFO  CommandLogger - MapReduce Total cumulative CPU time: 35 minutes 53 seconds 370 msec</span><br><span class="line">2020-10-15 11:58:49 - INFO  CommandLogger - Ended Job &#x3D; job_1591776234013_38803470</span><br><span class="line">2020-10-15 11:58:51 - INFO  CommandLogger - Loading data to table dm_qqq_broker_detail_v2_daily partition (cal_dt&#x3D;2020-10-14)</span><br><span class="line">2020-10-15 11:58:53 - INFO  CommandLogger - Partition dm_qqq_broker_detail_v2_daily&#123;cal_dt&#x3D;2020-10-14&#125; stats: [numFiles&#x3D;14, numRows&#x3D;7468075, totalSize&#x3D;1407456718, rawDataSize&#x3D;14422227854]</span><br><span class="line">2020-10-15 11:58:54 - INFO  CommandLogger - MapReduce Jobs Launched: </span><br><span class="line">2020-10-15 11:58:54 - INFO  CommandLogger - Stage-Stage-1: Map: 7  Reduce: 2   Cumulative CPU: 13737.04 sec   HDFS Read: 2050308004 HDFS Write: 1407466153 SUCCESS</span><br><span class="line">2020-10-15 11:58:54 - INFO  CommandLogger - Stage-Stage-3: Map: 5   Cumulative CPU: 2153.37 sec   HDFS Read: 1407795023 HDFS Write: 1407456718 SUCCESS</span><br><span class="line">2020-10-15 11:58:54 - INFO  CommandLogger - Total MapReduce CPU Time Spent: 0 days 4 hours 24 minutes 50 seconds 410 msec</span><br><span class="line">2020-10-15 11:58:54 - INFO  CommandLogger - OK</span><br><span class="line">2020-10-15 11:58:54 - INFO  CommandLogger - Time taken: 4516.979 seconds</span><br><span class="line">2020-10-15 11:58:54 - INFO  HiveRunner - run hql(s) over</span><br><span class="line">2020-10-15 11:58:54 - INFO  SpringContextUtil - executor global spring context will be closed.</span><br><span class="line">2020-10-15 11:58:54 - job execute finished, exitCode: 0, exitState: 运行成功</span><br><span class="line">2020-10-15 11:58:54 - signal generated: jobState(400), return: (true)</span><br><span class="line">2020-10-15 11:58:54 - End</span><br></pre></td></tr></table></figure>
<p>这是最后一步的日志。从上面的sql可以看到，最后一步的sql只有短短的一个join。join的也是一个小表。但是sql足足运行了一个多小时。这绝对不能忍！！！<br>所以这里就需要吧这个小表写进内存里面，在不改变sql语句的情况下。只需要加上一个参数</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>这个参数是开启mapjoin。hive会自动将小表加入内存里面读取。<br>优化完后运行时间直接从2小时46分降到了2小时。</p>
<h2 id="第二次优化-寻找数据倾斜"><a href="#第二次优化-寻找数据倾斜" class="headerlink" title="第二次优化(寻找数据倾斜)"></a>第二次优化(寻找数据倾斜)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO  CommandLogger - 2020-10-21 06:42:59,147 Stage-3 map &#x3D; 100%,  reduce &#x3D; 97%, Cumulative CPU 2712.84 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:43:41,517 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 2778.26 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:44:42,347 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 2878.27 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:45:42,353 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 2969.98 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:46:43,126 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3070.45 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:47:43,817 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3153.5 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:48:44,622 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3227.89 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:49:44,652 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3295.1 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:50:44,820 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3358.19 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:51:44,916 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3419.95 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:52:45,752 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3482.48 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:53:46,528 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3548.78 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:54:47,274 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3616.27 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:55:48,197 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3686.04 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:56:49,222 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3751.53 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:57:49,974 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3832.56 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:58:50,215 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3900.31 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 06:59:51,068 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 3984.47 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:00:51,944 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4069.52 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:01:52,924 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4154.65 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:02:53,948 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4228.69 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:03:54,043 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4298.79 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:04:54,779 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4377.97 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:05:55,540 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4456.26 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:06:56,310 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4533.57 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:07:56,950 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4612.82 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:08:57,617 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4695.32 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:09:58,284 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4777.87 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:10:58,978 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4861.28 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:11:59,590 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 4938.43 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:13:00,297 Stage-3 map &#x3D; 100%,  reduce &#x3D; 98%, Cumulative CPU 5008.75 sec</span><br><span class="line">INFO  CommandLogger - 2020-10-21 07:13:12,645 Stage-3 map &#x3D; 100%,  reduce &#x3D; 99%, Cumulative CPU 5020.57 sec</span><br></pre></td></tr></table></figure>
<p>继续去看日志，很容易就能看到这么一段，这一看就是数据倾斜了。<br>所以继续去yarn的历史日志去确认是否数据倾斜。<br><img src="https://i.loli.net/2020/10/26/7mdcADaHn9bu4QW.png" alt="yarn.png"><br>从上图就可以很明显看到。有一个reduce倾斜了。<br>所以现在就要找到是哪个语句的reduce倾斜了。<br>在这个日志的最上方开始处有这么一条信息</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">CommandLogger <span class="operator">-</span> Hadoop job information <span class="keyword">for</span> Stage<span class="number">-3</span>: number <span class="keyword">of</span> mappers: <span class="number">30</span>; number <span class="keyword">of</span> reducers: <span class="number">4</span></span><br></pre></td></tr></table></figure>
<p>很容易可以看到。是stage3的时候，数据倾斜。<br>所以，只要找到stage是哪个语句就能找到数据倾斜的地方。<br>通过explain。将sql输出语法树。<br>这个语法树会非常的长。但是只要找到stage-3是哪个语句就好了。<br>语法树的日志就不详细贴出。<br>通过文本搜索’Stage-3’。<br>在语法树的下面能看到 alias:e 的标签。<br>所以，上面的sql语句是e表的join出现了数据倾斜。<br>回顾join e表的语句。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">left outer join dm_bbb_broker_detail_normal_daily e on a.user_id=e.user_id and e.cal_dt = $&#123;dealDate&#125; and coalesce(e.user_id,0)&gt;0</span><br></pre></td></tr></table></figure>
<p>这个语句的数据倾斜一共就两个地方：1.主表的user_id倾斜了。2.从表的user_id倾斜了。<br>通过查找。可以发现是主表的user_id倾斜了。里面user_id为0的条数有几十万条。<br>单找到了数据倾斜的地方。优化就变得简单了。<br>现在直接将主表的user_id打散就好了。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">left outer join dm_bbb_broker_detail_normal_daily e on (case when a.user_id = 0 then cast(ceiling(rand() * -65535) as bigint) else a.user_id end)=e.user_id and e.cal_dt = $&#123;dealDate&#125; and coalesce(e.user_id,0)&gt;0</span><br></pre></td></tr></table></figure>
<p>这里用了一个随机数，将user_id为0的字段全部分散。</p>
<p>这里的优化效果很明显。<br>原来2小时的运行时间。现在一小时就能结束了。</p>
<h2 id="第三次优化-调整map数量与reduce数量"><a href="#第三次优化-调整map数量与reduce数量" class="headerlink" title="第三次优化(调整map数量与reduce数量)"></a>第三次优化(调整map数量与reduce数量)</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">134217728</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="number">1073741824</span>;</span><br></pre></td></tr></table></figure>
<p>从上面的参数可以看到。这里sql设置的map切分是128兆。随便挑一个任务去yarn看一下详细情况。<br><img src="https://i.loli.net/2020/10/26/RL28tnwjkT47yNH.png" alt="map num.png"><br>能看到起了100多个map。<br>但是每一个map的时间都很短<br><img src="https://i.loli.net/2020/10/26/FwDhPnxXRyrUm9j.png" alt="map时间.png"></p>
<p>这里就会造成了yarn会启动大量的map，造成了资源浪费。所以减少map数量迫在眉睫。<br>经过调试</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">150</span>;</span><br></pre></td></tr></table></figure>
<p>将map切分数量过大了近10倍，能明显减少map数量<br>这里需要经过多次的调试，确定一个最优的值。</p>
<p>最后将直接缩短到了50分钟</p>
<h2 id="最后的sql"><a href="#最后的sql" class="headerlink" title="最后的sql"></a>最后的sql</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">1024000000</span>;</span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">150</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.text.output.compress.split<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> wangzy_broker_detail_daily_tmp01;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> wangzy_broker_detail_daily_tmp01 <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">	a.broker_id,</span><br><span class="line">	b.broker_id,</span><br><span class="line">	c.broker_id,</span><br><span class="line">	d.broker_id</span><br><span class="line">	e.user_id,</span><br><span class="line">	f.broker_id,</span><br><span class="line">	g.broker_id,</span><br><span class="line">	h.broker_id,</span><br><span class="line">	i.broker_id</span><br><span class="line"><span class="keyword">from</span> dm_qqq_broker_detail_basis_daily a </span><br><span class="line">left outer join dm_qqq_broker_detail_combo_daily b on a.broker_id=b.broker_id and b.cal_dt = $&#123;dealDate&#125; and b.broker_id&gt;0</span><br><span class="line">left outer join dm_qqq_broker_detail_account_daily c on a.broker_id=c.broker_id and c.cal_dt = $&#123;dealDate&#125; and c.broker_id&gt;0</span><br><span class="line">left outer join dm_aaa_broker_detail_normal_daily d on a.broker_id=d.broker_id and d.cal_dt = $&#123;dealDate&#125; and d.broker_id&gt;0</span><br><span class="line">left outer join dm_bbb_broker_detail_normal_daily e on (case when a.user_id = 0 then cast(ceiling(rand() * -65535) as bigint) else a.user_id end)=e.user_id and e.cal_dt = $&#123;dealDate&#125; and coalesce(e.user_id,0)&gt;0</span><br><span class="line">left outer join dm_qqq_broker_detail_link_daily f on a.broker_id=f.broker_id and f.cal_dt = $&#123;dealDate&#125; and f.broker_id&gt;0</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">	<span class="keyword">select</span> </span><br><span class="line">		user_id <span class="keyword">as</span> broker_id</span><br><span class="line">	<span class="keyword">from</span>(</span><br><span class="line">			<span class="keyword">select</span> </span><br><span class="line">				user_id</span><br><span class="line">			<span class="keyword">from</span> dw_new_prop_base_esf </span><br><span class="line">			where cal_dt=$&#123;dealDate&#125; and state in(1,2) and substr(add_date,1,10)=$&#123;dealDate&#125; and COALESCE(user_id,0)&gt;0</span><br><span class="line">			<span class="keyword">union</span> <span class="keyword">ALL</span></span><br><span class="line">			<span class="keyword">select</span> </span><br><span class="line">				user_id</span><br><span class="line">			<span class="keyword">from</span> dw_new_prop_base_zf </span><br><span class="line">			where cal_dt=$&#123;dealDate&#125; and state in(1,2) and substr(add_date,1,10)=$&#123;dealDate&#125; and COALESCE(user_id,0)&gt;0</span><br><span class="line">			<span class="keyword">union</span> <span class="keyword">ALL</span></span><br><span class="line">			<span class="keyword">select</span> </span><br><span class="line">				user_id</span><br><span class="line">			<span class="keyword">from</span> dw_new_prop_base_sydc </span><br><span class="line">			where cal_dt=$&#123;dealDate&#125; and state in(1,2) and substr(add_date,1,10)=$&#123;dealDate&#125; and COALESCE(user_id,0)&gt;0</span><br><span class="line">	)aa <span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line">) g <span class="keyword">on</span> a.broker_id<span class="operator">=</span>g.broker_id</span><br><span class="line"></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">	<span class="keyword">select</span> </span><br><span class="line">		broker_id</span><br><span class="line">	<span class="keyword">from</span> dm_aaa_user_coupon_daily </span><br><span class="line">	where cal_dt=$&#123;dealDate&#125;</span><br><span class="line">	<span class="keyword">group</span> <span class="keyword">by</span> broker_id</span><br><span class="line">) h <span class="keyword">on</span> a.broker_id<span class="operator">=</span>h.broker_id <span class="keyword">and</span> h.broker_id<span class="operator">&gt;</span><span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> (</span><br><span class="line">	<span class="keyword">select</span> </span><br><span class="line">		 broker_id</span><br><span class="line">	<span class="keyword">from</span> dm_house_daily</span><br><span class="line">	where cal_dt=$&#123;dealDate&#125;</span><br><span class="line">	<span class="keyword">group</span> <span class="keyword">by</span> broker_id</span><br><span class="line">) i <span class="keyword">on</span> a.broker_id<span class="operator">=</span>i.broker_id <span class="keyword">and</span> i.broker_id<span class="operator">&gt;</span><span class="number">0</span></span><br><span class="line"></span><br><span class="line">left outer join da_qqq_broker_quality_daily j on a.broker_id = j.broker_id and j.cal_dt = $&#123;dealDate&#125;</span><br><span class="line"></span><br><span class="line">where a.cal_dt = $&#123;dealDate&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">use hdp_anjuke_dw_db;</span><br><span class="line">alter table dm_qqq_broker_detail_v2_daily drop partition(cal_dt = $&#123;dealDate&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">insert overwrite table dm_broker_detail_daily partition(cal_dt = $&#123;dealDate&#125;)</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">a.<span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> hdp_anjuke_dw_stage.wangzy_broker_detail_daily_tmp01 a</span><br><span class="line">join dw_qqq_account_city_config_daily cfg on a.city_id=cfg.city_id_ajk and cfg.cal_dt=$&#123;dealDate&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark性能优化</title>
    <url>/2020/07/04/spark/spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<h1 id="Spark性能优化"><a href="#Spark性能优化" class="headerlink" title="Spark性能优化"></a>Spark性能优化</h1><p>在大数据处理过程中，涉及到最多的就是性能优化。这个也是大数据场景的重点与难点。<br>本文将从常见的几个方面与实现spark的优化</p>
<h2 id="常规性能调优"><a href="#常规性能调优" class="headerlink" title="常规性能调优"></a>常规性能调优</h2><h3 id="1-1-常规性能调优一：最优资源配置"><a href="#1-1-常规性能调优一：最优资源配置" class="headerlink" title="1.1 常规性能调优一：最优资源配置"></a>1.1 常规性能调优一：最优资源配置</h3><p>&emsp;&emsp;Spark性能调优的第一步，就是为任务分配更多的资源，在一定范围内，增加资源的分配与性能的提升是成正比的，实现了最优的资源配置后，在此基础上再考虑进行后面论述的性能调优策略。<br>&emsp;&emsp; 资源的分配在使用脚本提交Spark任务时进行指定，标准的Spark任务提交脚本如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class com.atguigu.spark.Analysis \</span><br><span class="line">--master yarn</span><br><span class="line">--deploy-mode cluster</span><br><span class="line">--num-executors 80 \</span><br><span class="line">--driver-memory 6g \</span><br><span class="line">--executor-memory 6g \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">/usr/opt/modules/spark/jar/spark.jar \</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp; 可以进行分配的资源如表所示：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td>–num-executors</td>
<td align="left">配置Executor的数量</td>
</tr>
<tr>
<td>–driver-memory</td>
<td align="left">配置Driver内存（影响不大）</td>
</tr>
<tr>
<td>–executor-memory</td>
<td align="left">配置每个Executor的CPU core数量</td>
</tr>
</tbody></table>
<p>调节原则：尽量将任务分配的资源调节到可以使用的资源的最大限度。<br>对于具体资源的分配，我们分别讨论Spark的两种Cluster运行模式：</p>
<ol>
<li>第一种是Spark Standalone模式，你在提交任务前，一定知道或者可以从运维部门获取到你可以使用的资源情况，在编写submit脚本的时候，就根据可用的资源情况进行资源的分配，比如说集群有15台机器，每台机器为8G内存，2个CPU core，那么就指定15个Executor，每个Executor分配8G内存，2个CPU core。</li>
<li>第二种是Spark Yarn模式，由于Yarn使用资源队列进行资源的分配和调度，在编写submit脚本的时候，就根据Spark作业要提交到的资源队列，进行资源的分配，比如资源队列有400G内存，100个CPU core，那么指定50个Executor，每个Executor分配8G内存，2个CPU core。<br>对各项资源进行了调节后，得到的性能提升会有如下表现：<table>
<thead>
<tr>
<th>名称</th>
<th align="left">解析</th>
</tr>
</thead>
<tbody><tr>
<td>增加Executor·个数</td>
<td align="left">在资源允许的情况下，增加Executor的个数可以提高执行task的并行度。比如有4个Executor，每个Executor有2个CPU core，那么可以并行执行8个task，如果将Executor的个数增加到8个（资源允许的情况下），那么可以并行执行16个task，此时的并行能力提升了一倍。</td>
</tr>
<tr>
<td>增加每个Executor的CPU core个数</td>
<td align="left">在资源允许的情况下，增加每个Executor的Cpu core个数，可以提高执行task的并行度。比如有4个Executor，每个Executor有2个CPU core，那么可以并行执行8个task，如果将每个Executor的CPU core个数增加到4个（资源允许的情况下），那么可以并行执行16个task，此时的并行能力提升了一倍。</td>
</tr>
<tr>
<td>增加每个Executor的内存量</td>
<td align="left">在资源允许的情况下，增加每个Executor的内存量以后，对性能的提升有三点：<br>1.可以缓存更多的数据（即对RDD进行cache），写入磁盘的数据相应减少，甚至可以不写入磁盘，减少了可能的磁盘IO；<br>2. 可以为shuffle操作提供更多内存，即有更多空间来存放reduce端拉取的数据，写入磁盘的数据相应减少，甚至可以不写入磁盘，减少了可能的磁盘IO；<br>3. 可以为task的执行提供更多内存，在task的执行过程中可能创建很多对象，内存较小时会引发频繁的GC，增加内存后，可以避免频繁的GC，提升整体性能。</td>
</tr>
</tbody></table>
</li>
</ol>
<p>**生产环境Spark submit脚本配置 ** </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class com.atguigu.spark.WordCount \</span><br><span class="line">--master yarn\</span><br><span class="line">--deploy-mode cluster\</span><br><span class="line">--num-executors 80 \</span><br><span class="line">--driver-memory 6g \</span><br><span class="line">--executor-memory 6g \</span><br><span class="line">--executor-cores 3 \</span><br><span class="line">--queue root.default \</span><br><span class="line">--conf spark.yarn.executor.memoryOverhead=2048 \</span><br><span class="line">--conf spark.core.connection.ack.wait.timeout=300 \</span><br><span class="line">/usr/<span class="built_in">local</span>/spark/spark.jar</span><br></pre></td></tr></table></figure>
<p>参数配置参考值：</p>
<ul>
<li>–num-executors：50~100</li>
<li>–driver-memory：1G~5G</li>
<li>–executor-memory:6G~10G</li>
<li>–executor-core: 3</li>
<li>master:实际生产环境一定使用yarn</li>
</ul>
<h3 id="1-2-常规性能调优二：RDD优化"><a href="#1-2-常规性能调优二：RDD优化" class="headerlink" title="1.2 常规性能调优二：RDD优化"></a>1.2 常规性能调优二：RDD优化</h3><ol>
<li>RDD复用<br>在对RDD进行算子时，要避免相同的算子和计算逻辑之下对RDD进行重复的计算<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[HDFS] --&gt;B[RDD1]</span><br><span class="line">    B --&gt;|重复| C[RDD2]</span><br><span class="line">    B --&gt;|重复| D[RDD2]</span><br><span class="line">    C --&gt;|不同| E[RDD3]</span><br><span class="line">    D --&gt;|不同| F[RDD4]</span><br></pre></td></tr></table></figure>
对上图中的RDD计算架构进行修改，得到如下图所示的优化结果：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[HDFS] --&gt;B[RDD1]</span><br><span class="line">    B --&gt; C[RDD2]</span><br><span class="line">    C --&gt;|不同| E[RDD3]</span><br><span class="line">    C --&gt;|不同| F[RDD4]</span><br></pre></td></tr></table></figure>
在生产环境下，有着大量的RDD转化，就需要先预架构RDD，看那些RDD可以得到复用</li>
<li>RDD持久化<br>在Spark中，当多次对同一个RDD执行算子操作时，每一次都会对这个RDD以之前的父RDD重新计算一次，这种情况是必须要避免的，对同一个RDD的重复计算是对资源的极大浪费，因此，必须对多次使用的RDD进行持久化，通过持久化将公共RDD的数据缓存到内存/磁盘中，之后对于公共RDD的计算都会从内存/磁盘中直接获取RDD数据。<br>对于RDD的持久化，有两点需要说明：<ol>
<li>RDD的持久化是可以进行序列化的，当内存无法将RDD的数据完整的进行存放的时候，可以考虑使用序列化的方式减小数据体积，将数据完整存储在内存中。</li>
<li>如果对于数据的可靠性要求很高，并且内存充足，可以使用副本机制，对RDD数据进行持久化。当持久化启用了复本机制时，对于持久化的每个数据单元都存储一个副本，放在其他节点上面，由此实现数据的容错，一旦一个副本数据丢失，不需要重新计算，还可以使用另外一个副本。</li>
</ol>
</li>
<li>RDD尽可能早的进行filter操作<br>获取到初始RDD后，应该考虑尽早地过滤掉不需要的数据，进而减少对内存的占用，从而提升Spark作业的运行效率。<h3 id="1-3-常规性能调优三：并行度调节"><a href="#1-3-常规性能调优三：并行度调节" class="headerlink" title="1.3 常规性能调优三：并行度调节"></a>1.3 常规性能调优三：并行度调节</h3>&emsp;&emsp;Spark作业中的并行度指各个stage的task的数量。<br>&emsp;&emsp;如果并行度设置不合理而导致并行度过低，会导致资源的极大浪费，例如，20个Executor，每个Executor分配3个CPU core，而Spark作业有40个task，这样每个Executor分配到的task个数是2个，这就使得每个Executor有一个CPU core空闲，导致资源的浪费。<br>&emsp;&emsp;理想的并行度设置，应该是让并行度与资源相匹配，简单来说就是在资源允许的前提下，并行度要设置的尽可能大，达到可以充分利用集群资源。合理的设置并行度，可以提升整个Spark作业的性能和运行速度。<br>&emsp;&emsp;Spark<strong>官方推荐，task数量应该设置为Spark作业总CPU core数量的2~3倍。</strong>之所以没有推荐task数量与CPU core总数相等，是因为task的执行时间不同，有的task执行速度快而有的task执行速度慢，如果task数量与CPU core总数相等，那么执行快的task执行完成后，会出现CPU core空闲的情况。如果task数量设置为CPU core总数的2~3倍，那么一个task执行完毕后，CPU core会立刻执行下一个task，降低了资源的浪费，同时提升了Spark作业运行的效率。<br>Spark作业并行度的设置如下所示:</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">	.set(<span class="string">&quot;spark.default.parallelism&quot;</span>, <span class="string">&quot;500&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-4-常规性能调优四：广播大变量"><a href="#1-4-常规性能调优四：广播大变量" class="headerlink" title="1.4 常规性能调优四：广播大变量"></a>1.4 常规性能调优四：广播大变量</h3><p>&emsp;&emsp;默认情况下，task中的算子中如果使用了外部的变量，每个task都会获取一份变量的复本，这就造成了内存的极大消耗。一方面，如果后续对RDD进行持久化，可能就无法将RDD数据存入内存，只能写入磁盘，磁盘IO将会严重消耗性能；另一方面，task在创建对象的时候，也许会发现堆内存无法存放新创建的对象，这就会导致频繁的GC，GC会导致工作线程停止，进而导致Spark暂停工作一段时间，严重影响Spark性能。<br>&emsp;&emsp;假设当前任务配置了20个Executor，指定500个task，有一个20M的变量被所有task共用，此时会在500个task中产生500个副本，耗费集群10G的内存，如果使用了广播变量， 那么每个Executor保存一个副本，一共消耗400M内存，内存消耗减少了5倍。<br>广播变量在每个Executor保存一个副本，此Executor的所有task共用此广播变量，这让变量产生的副本数量大大减少。<br>&emsp;&emsp;在初始阶段，广播变量只在Driver中有一份副本。task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中尝试获取变量，如果本地没有，BlockManager就会从Driver或者其他节点的BlockManager上远程拉取变量的复本，并由本地的BlockManager进行管理；之后此Executor的所有task都会直接从本地的BlockManager中获取变量。</p>
<h3 id="1-5-常规性能调优五：Kryo序列化"><a href="#1-5-常规性能调优五：Kryo序列化" class="headerlink" title="1.5 常规性能调优五：Kryo序列化"></a>1.5 常规性能调优五：Kryo序列化</h3><p>　　默认情况下，Spark使用Java的序列化机制。Java的序列化机制使用方便，不需要额外的配置，在算子中使用的变量实现Serializable接口即可，但是，Java序列化机制的效率不高，序列化速度慢并且序列化后的数据所占用的空间依然较大。<br>　　Kryo序列化机制比Java序列化机制性能提高10倍左右，Spark之所以没有默认使用Kryo作为序列化类库，是因为它不支持所有对象的序列化，同时Kryo需要用户在使用前注册需要序列化的类型，不够方便，但从Spark 2.0.0版本开始，<font color=red>简单类型、简单类型数组、字符串类型的Shuffling RDDs 已经默认使用Kryo序列化方式了。</font></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyKryoRegistrator</span> <span class="keyword">implements</span> <span class="title">KryoRegistrator</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registerClasses</span><span class="params">(Kryo kryo)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    kryo.register(StartupReportLogs.class);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>配置Kryo序列化方式的实例代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建SparkConf对象</span></span><br><span class="line">val conf = <span class="keyword">new</span> SparkConf().setMaster(…).setAppName(…)</span><br><span class="line"><span class="comment">//使用Kryo序列化库，如果要使用Java序列化库，需要把该行屏蔽掉</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>);  </span><br><span class="line"><span class="comment">//在Kryo序列化库中注册自定义的类集合，如果要使用Java序列化库，需要把该行屏蔽掉</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.kryo.registrator&quot;</span>, <span class="string">&quot;atguigu.com.MyKryoRegistrator&quot;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="1-6-常规性能调优六：调节本地化等待时长"><a href="#1-6-常规性能调优六：调节本地化等待时长" class="headerlink" title="1.6 常规性能调优六：调节本地化等待时长"></a>1.6 常规性能调优六：调节本地化等待时长</h3><p>　　Spark作业运行过程中，Driver会对每一个stage的task进行分配。根据Spark的task分配算法，Spark希望task能够运行在它要计算的数据算在的节点（数据本地化思想），这样就可以避免数据的网络传输。通常来说，task可能不会被分配到它处理的数据所在的节点，因为这些节点可用的资源可能已经用尽，此时，Spark会等待一段时间，默认3s，如果等待指定时间后仍然无法在指定节点运行，那么会自动降级，尝试将task分配到比较差的本地化级别所对应的节点上，比如将task分配到离它要计算的数据比较近的一个节点，然后进行计算，如果当前级别仍然不行，那么继续降级。<br>　　当task要处理的数据不在task所在节点上时，会发生数据的传输。task会通过所在节点的BlockManager获取数据，BlockManager发现数据不在本地时，户通过网络传输组件从数据所在节点的BlockManager处获取数据。<br>　　网络传输数据的情况是我们不愿意看到的，大量的网络传输会严重影响性能，因此，我们希望通过调节本地化等待时长，如果在等待时长这段时间内，目标节点处理完成了一部分task，那么当前的task将有机会得到执行，这样就能够改善Spark作业的整体性能。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th align="left">解析</th>
</tr>
</thead>
<tbody><tr>
<td>PROCESS_LOCAL</td>
<td align="left">进程本地化，task和数据在同一个Executor中，性能最好。</td>
</tr>
<tr>
<td>NODE_LOCAL</td>
<td align="left">节点本地化，task和数据在同一个节点中，但是task和数据不在同一个Executor中，数据需要在进程间进行传输。</td>
</tr>
<tr>
<td>RACK_LOCAL</td>
<td align="left">机架本地化，task和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。</td>
</tr>
<tr>
<td>NO_PREF</td>
<td align="left">对于task来说，从哪里获取都一样，没有好坏之分。</td>
</tr>
<tr>
<td>ANY</td>
<td align="left">task和数据可以在集群的任何地方，而且不在一个机架中，性能最差。</td>
</tr>
<tr>
<td>在Spark项目开发阶段，可以使用client模式对程序进行测试，此时，可以在本地看到比较全的日志信息，日志信息中有明确的task数据本地化的级别，如果大部分都是PROCESS_LOCAL，那么就无需进行调节，但是如果发现很多的级别都是NODE_LOCAL、ANY，那么需要对本地化的等待时长进行调节，通过延长本地化等待时长，看看task的本地化级别有没有提升，并观察Spark作业的运行时间有没有缩短。</td>
<td align="left"></td>
</tr>
<tr>
<td>注意，过犹不及，不要将本地化等待时长延长地过长，导致因为大量的等待时长，使得Spark作业的运行时间反而增加了。</td>
<td align="left"></td>
</tr>
<tr>
<td>Spark本地化等待时长的设置如代码所示：</td>
<td align="left"></td>
</tr>
</tbody></table>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  .set(<span class="string">&quot;spark.locality.wait&quot;</span>, <span class="string">&quot;6&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="算子调优"><a href="#算子调优" class="headerlink" title="算子调优"></a>算子调优</h2><h3 id="2-1-算子调优一：mapPartitions"><a href="#2-1-算子调优一：mapPartitions" class="headerlink" title="2.1 算子调优一：mapPartitions"></a>2.1 算子调优一：mapPartitions</h3><p>　　普通的map算子对RDD中的每一个元素进行操作，而mapPartitions算子对RDD中每一个分区进行操作。如果是普通的map算子，假设一个partition有1万条数据，那么map算子中的function要执行1万次，也就是对每个元素进行操作。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[单条数据]</span><br><span class="line">B[单条数据]</span><br><span class="line">C[单条数据]</span><br><span class="line">E[单条数据]</span><br><span class="line">F[单条数据]</span><br><span class="line">G[单条数据]</span><br><span class="line">    A --&gt; D[task function]</span><br><span class="line">    B --&gt; D[task function]</span><br><span class="line">    C --&gt; D[task function]</span><br><span class="line">    E --&gt; D[task function]</span><br><span class="line">    F --&gt; D[task function]</span><br><span class="line">    G --&gt; D[task function]</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>　　如果是mapPartition算子，由于一个task处理一个RDD的partition，那么一个task只会执行一次function，function一次接收所有的partition数据，效率比较高。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[分区数据] --&gt; B(task function)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>　　比如，当要把RDD中的所有数据通过JDBC写入数据，如果使用map算子，那么需要对RDD中的每一个元素都创建一个数据库连接，这样对资源的消耗很大，如果使用mapPartitions算子，那么针对一个分区的数据，只需要建立一个数据库连接。<br>　　mapPartitions算子也存在一些缺点：对于普通的map操作，一次处理一条数据，如果在处理了2000条数据后内存不足，那么可以将已经处理完的2000条数据从内存中垃圾回收掉；但是如果使用mapPartitions算子，但数据量非常大时，function一次处理一个分区的数据，如果一旦内存不足，此时无法回收内存，就可能会OOM，即内存溢出。<br>　　因此，mapPartitions算子适用于数据量不是特别大的时候，此时使用mapPartitions算子对性能的提升效果还是不错的。（当数据量很大的时候，一旦使用mapPartitions算子，就会直接OOM）<br>　　在项目中，应该首先估算一下RDD的数据量、每个partition的数据量，以及分配给每个Executor的内存资源，如果资源允许，可以考虑使用mapPartitions算子代替map。</p>
<h3 id="2-2-算子调优二：foreachPartition优化数据库操作"><a href="#2-2-算子调优二：foreachPartition优化数据库操作" class="headerlink" title="2.2 算子调优二：foreachPartition优化数据库操作"></a>2.2 算子调优二：foreachPartition优化数据库操作</h3><p>　　在生产环境中，通常使用foreachPartition算子来完成数据库的写入，通过foreachPartition算子的特性，可以优化写数据库的性能。<br>　　如果使用foreach算子完成数据库的操作，由于foreach算子是遍历RDD的每条数据，因此，每条数据都会建立一个数据库连接，这是对资源的极大浪费，因此，对于写数据库操作，我们应当使用foreachPartition算子。<br>　　与mapPartitions算子非常相似，foreachPartition是将RDD的每个分区作为遍历对象，一次处理一个分区的数据，也就是说，如果涉及数据库的相关操作，一个分区的数据只需要创建一次数据库连接，如图所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[分区数据] --&gt;B[task function]</span><br><span class="line">    B --&gt; c[数据连接]</span><br><span class="line">    c --&gt; d[数据库]</span><br></pre></td></tr></table></figure>
<p>使用了foreachPartition算子后，可以获得以下的性能提升：</p>
<ul>
<li>对于我们写的function函数，一次处理一整个分区的数据；</li>
<li>对于一个分区内的数据，创建唯一的数据库连接；</li>
<li>只需要向数据库发送一次SQL预计和多组参数<br>在生产环境中，全部都会使用foreachPartition算子完成数据库操作。foreachPartition算子存在一个问题，与mapPatitions算子类似，如果一个分区的数据量特别大，可能会造成OOM。即内存溢出。<h3 id="2-3-算子调优三：filter与coalesce的配合使用"><a href="#2-3-算子调优三：filter与coalesce的配合使用" class="headerlink" title="2.3 算子调优三：filter与coalesce的配合使用"></a>2.3 算子调优三：filter与coalesce的配合使用</h3>　　在Spark任务中我们经常会使用filter算子完成RDD中数据的过滤，在任务初始阶段，从各个分区中加载到的数据量是相近的，但是一旦进过filter过滤后，每个分区的数据量有可能会存在较大差异，如图所示：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[分区数据 1000条]</span><br><span class="line">B[分区数据 1000条]</span><br><span class="line">C[分区数据 1000条]</span><br><span class="line">    A --&gt; D[filter]</span><br><span class="line">    B --&gt; D[filter]</span><br><span class="line">    C --&gt; D[filter]</span><br><span class="line">    D --&gt; E[分区数据 300条]</span><br><span class="line">    D --&gt; F[分区数据 100条]</span><br><span class="line">    D --&gt; G[分区数据 800条]</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
根据图中信息我们可以发现两个问题：</li>
</ul>
<ol>
<li>每个partition的数据量变小了，如果还按照之前与partition相等的task个数去处理当前数据，有点浪费task的计算资源；</li>
<li>每个partition的数据量不一样，会导致后面的每个task处理每个partition数据的时候，每个task要处理的数据量不同，这很有可能导致数据倾斜问题。<br>　　每个task要处理的数据量不同，这很有可能导致数据倾斜问题。<br>如上图所示，第二个分区的数据过滤后只剩100条，而第三个分区的数据过滤后剩下800条，在相同的处理逻辑下，第二个分区对应的task处理的数据量与第三个分区对应的task处理的数据量差距达到了8倍，这也会导致运行速度可能存在数倍的差距，这也就是数据倾斜问题。<br>针对上述的两个问题，我们分别进行分析：</li>
</ol>
<ul>
<li>针对第一个问题，既然分区的数据量变小了，我们希望可以对分区数据进行重新分配，比如将原来4个分区的数据转化到2个分区中，这样只需要用后面的两个task进行处理即可，避免了资源的浪费。</li>
<li>针对第二个问题，解决方法和第一个问题的解决方法非常相似，对分区数据重新分配，让每个partition中的数据量差不多，这就避免了数据倾斜问题<br>那么具体应该如何实现上面的解决思路？我们需要coalesce算子。<br>repartition与coalesce都可以用来进行重分区，其中repartition只是coalesce接口中shuffle为true的简易实现，coalesce默认情况下不进行shuffle，但是可以通过参数进行设置。<br>假设我们希望将原本的分区个数A通过重新分区变为B，那么有以下几种情况：</li>
<li> <strong>A &gt; B（多数分区合并为少数分区）</strong></li>
</ul>
<ol>
<li>   A与B相差值不大<br>此时使用coalesce即可，无需shuffle过程。</li>
<li>   A与B相差值很大<br>此时可以使用coalesce并且不启用shuffle过程，但是会导致合并过程性能低下，所以推荐设置coalesce的第二个参数为true，即启动shuffle过程。</li>
</ol>
<ul>
<li><strong>A &lt; B（少数分区分解为多数分区）</strong><br>此时使用repartition即可，如果使用coalesce需要将shuffle设置为true，否则coalesce无效。<br>我们可以在filter操作之后，使用coalesce算子针对每个partition的数据量各不相同的情况，压缩partition的数量，而且让每个partition的数据量尽量均匀紧凑，以便于后面的task进行计算操作，在某种程度上能够在一定程度上提升性能。<br>注意：local模式是进程内模拟集群运行，已经对并行度和分区数量有了一定的内部优化，因此不用去设置并行度和分区数量。<h3 id="2-4-算子调优四：repartition解决SparkSQL低并行度问题"><a href="#2-4-算子调优四：repartition解决SparkSQL低并行度问题" class="headerlink" title="2.4 算子调优四：repartition解决SparkSQL低并行度问题"></a>2.4 算子调优四：repartition解决SparkSQL低并行度问题</h3>　　在第一节的常规性能调优中我们讲解了并行度的调节策略，但是，并行度的设置对于Spark SQL是不生效的，用户设置的并行度只对于Spark SQL以外的所有Spark的stage生效。<br>　　Spark SQL的并行度不允许用户自己指定，Spark SQL自己会默认根据hive表对应的HDFS文件的split个数自动设置Spark SQL所在的那个stage的并行度，用户自己通spark.default.parallelism参数指定的并行度，只会在没Spark SQL的stage中生效。<br>　　由于Spark SQL所在stage的并行度无法手动设置，如果数据量较大，并且此stage中后续的transformation操作有着复杂的业务逻辑，而Spark SQL自动设置的task数量很少，这就意味着每个task要处理为数不少的数据量，然后还要执行非常复杂的处理逻辑，这就可能表现为第一个有Spark SQL的stage速度很慢，而后续的没有Spark SQL的stage运行速度非常快。<br>　　为了解决Spark SQL无法设置并行度和task数量的问题，我们可以使用repartition算子。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[HIVE] --&gt;B[分区]</span><br><span class="line">A[HIVE] --&gt;D[分区]</span><br><span class="line">B --&gt; C[tesk]</span><br><span class="line">D --&gt; E[tesk]</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[HIVE] --&gt;B[分区]</span><br><span class="line">A[HIVE] --&gt;C[分区]</span><br><span class="line">B --&gt; D[repartition]</span><br><span class="line">C --&gt; D[repartition]</span><br><span class="line">D --&gt; E[分区]</span><br><span class="line">D --&gt; F[分区]</span><br><span class="line">D --&gt; G[分区]</span><br><span class="line">D --&gt; H[分区]</span><br><span class="line">D --&gt; J[分区]</span><br><span class="line">D --&gt; K[分区]</span><br><span class="line">D --&gt; L[分区]</span><br><span class="line">E --&gt; M[tesk]</span><br><span class="line">F --&gt; N[tesk]</span><br><span class="line">G --&gt; O[tesk]</span><br><span class="line">H --&gt; P[tesk]</span><br><span class="line">J --&gt; Q[tesk]</span><br><span class="line">K --&gt; R[tesk]</span><br><span class="line">L --&gt; S[tesk]</span><br></pre></td></tr></table></figure>
<p>　　Spark SQL这一步的并行度和task数量肯定是没有办法去改变了，但是，对于Spark SQL查询出来的RDD，立即使用repartition算子，去重新进行分区，这样可以重新分区为多个partition，从repartition之后的RDD操作，由于不再设计Spark SQL，因此stage的并行度就会等于你手动设置的值，这样就避免了Spark SQL所在的stage只能用少量的task去处理大量数据并执行复杂的算法逻辑。</p>
<h3 id="2-5-算子调优五：reduceByKey预聚合"><a href="#2-5-算子调优五：reduceByKey预聚合" class="headerlink" title="2.5 算子调优五：reduceByKey预聚合"></a>2.5 算子调优五：reduceByKey预聚合</h3><p>　　reduceByKey相较于普通的shuffle操作一个显著的特点就是会进行map端的本地聚合，map端会先对本地的数据进行combine操作，然后将数据写入给下个stage的每个task创建的文件中，也就是在map端，对每一个key对应的value，执行reduceByKey算子函数。reduceByKey算子的执行过程如图所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A[task1]</span><br><span class="line">B[task2]</span><br><span class="line">    A --&gt; |spark,1 spark,1|D[filter]</span><br><span class="line">    B --&gt; |spark,1 spark,1|F[filter]</span><br><span class="line">    A --&gt; |hadoop,1 hadoop,1|E[filter]</span><br><span class="line">    B --&gt; |hadoop,1 hadoop,1|G[filter]</span><br><span class="line">    D --&gt; |spark,2|H[task]</span><br><span class="line">    F --&gt; |spark,2|H[task]</span><br><span class="line">    E --&gt; |hadoop,2|I[task]</span><br><span class="line">    G --&gt; |hadoop,2|I[task]</span><br></pre></td></tr></table></figure>
<p>使用reduceByKey对性能的提升如下：</p>
<ul>
<li>本地聚合后，在map端的数据量变少，减少了磁盘IO，也减少了对磁盘空间的占用；</li>
<li>本地聚合后，下一个stage拉取的数据量变少，减少了网络传输的数据量；</li>
<li>本地聚合后，在reduce端进行数据缓存的内存占用减少；</li>
<li>本地聚合后，在reduce端进行聚合的数据量减少。<br>　　基于reduceByKey的本地聚合特征，我们应该考虑使用reduceByKey代替其他的shuffle算子，例如groupByKey。<br>　　groupByKey不会进行map端的聚合，而是将所有map端的数据shuffle到reduce端，然后在reduce端进行数据的聚合操作。由于reduceByKey有map端聚合的特性，使得网络传输的数据量减小，因此效率要明显高于groupByKey。<h2 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a>Shuffle调优</h2><h3 id="3-1-Shuffle调优一：调节map端缓冲区大小"><a href="#3-1-Shuffle调优一：调节map端缓冲区大小" class="headerlink" title="3.1 Shuffle调优一：调节map端缓冲区大小"></a>3.1 Shuffle调优一：调节map端缓冲区大小</h3>　　在Spark任务运行过程中，如果shuffle的map端处理的数据量比较大，但是map端缓冲的大小是固定的，可能会出现map端缓冲数据频繁spill溢写到磁盘文件中的情况，使得性能非常低下，通过调节map端缓冲的大小，可以避免频繁的磁盘IO操作，进而提升Spark任务的整体性能。<br>　　map端缓冲的默认配置是32KB，如果每个task处理640KB的数据，那么会发生640/32 = 20次溢写，如果每个task处理64000KB的数据，机会发生64000/32=2000此溢写，这对于性能的影响是非常严重的。<br>　　map端缓冲的配置方法如代码清单所示：<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  .set(<span class="string">&quot;spark.shuffle.file.buffer&quot;</span>, <span class="string">&quot;64&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-2-Shuffle调优二：调节reduce端拉取数据缓冲区大小"><a href="#3-2-Shuffle调优二：调节reduce端拉取数据缓冲区大小" class="headerlink" title="3.2 Shuffle调优二：调节reduce端拉取数据缓冲区大小"></a>3.2 Shuffle调优二：调节reduce端拉取数据缓冲区大小</h3>　　Spark Shuffle过程中，shuffle reduce task的buffer缓冲区大小决定了reduce task每次能够缓冲的数据量，也就是每次能够拉取的数据量，如果内存资源较为充足，适当增加拉取数据缓冲区的大小，可以减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。<br>reduce端数据拉取缓冲区的大小可以通过spark.reducer.maxSizeInFlight参数进行设置，默认为48MB，该参数的设置方法如代码清单所示：<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  .set(<span class="string">&quot;spark.reducer.maxSizeInFlight&quot;</span>, <span class="string">&quot;96&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-3-Shuffle调优三：调节reduce端拉取数据重试次数"><a href="#3-3-Shuffle调优三：调节reduce端拉取数据重试次数" class="headerlink" title="3.3 Shuffle调优三：调节reduce端拉取数据重试次数"></a>3.3 Shuffle调优三：调节reduce端拉取数据重试次数</h3>　　Spark Shuffle过程中，reduce task拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试。对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。<br>　　reduce端拉取数据重试次数可以通过spark.shuffle.io.maxRetries参数进行设置，该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败，默认为3，该参数的设置方法如代码清单所示：<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  .set(<span class="string">&quot;spark.shuffle.io.maxRetries&quot;</span>, <span class="string">&quot;6&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-4-Shuffle调优四：调节reduce端拉取数据等待间隔"><a href="#3-4-Shuffle调优四：调节reduce端拉取数据等待间隔" class="headerlink" title="3.4 Shuffle调优四：调节reduce端拉取数据等待间隔"></a>3.4 Shuffle调优四：调节reduce端拉取数据等待间隔</h3>　　Spark Shuffle过程中，reduce task拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试，在一次失败后，会等待一定的时间间隔再进行重试，可以通过加大间隔时长（比如60s），以增加shuffle操作的稳定性。<br>　　reduce端拉取数据等待间隔可以通过spark.shuffle.io.retryWait参数进行设置，默认值为5s，该参数的设置方法如代码清单所示：<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  .set(<span class="string">&quot;spark.shuffle.io.retryWait&quot;</span>, <span class="string">&quot;60s&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-5-Shuffle调优五：调节SortShuffle排序操作阈值"><a href="#3-5-Shuffle调优五：调节SortShuffle排序操作阈值" class="headerlink" title="3.5 Shuffle调优五：调节SortShuffle排序操作阈值"></a>3.5 Shuffle调优五：调节SortShuffle排序操作阈值</h3>　　对于SortShuffleManager，如果shuffle reduce task的数量小于某一阈值则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。<br>　　当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量，那么此时map-side就不会进行排序了，减少了排序的性能开销，但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。<br>SortShuffleManager排序操作阈值的设置可以通过spark.shuffle.sort. bypassMergeThreshold这一参数进行设置，默认值为200，该参数的设置方法如代码清单所示：<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  .set(<span class="string">&quot;spark.shuffle.sort.bypassMergeThreshold&quot;</span>, <span class="string">&quot;400&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="JVM调优"><a href="#JVM调优" class="headerlink" title="JVM调优"></a>JVM调优</h2>对于JVM调优，首先应该明确，full gc/minor gc，都会导致JVM的工作线程停止工作，即stop the world。<h3 id="4-1-JVM调优一：降低cache操作的内存占比"><a href="#4-1-JVM调优一：降低cache操作的内存占比" class="headerlink" title="4.1 JVM调优一：降低cache操作的内存占比"></a>4.1 JVM调优一：降低cache操作的内存占比</h3></li>
</ul>
<ol>
<li><pre><code>静态内存管理机制
</code></pre>
　　根据Spark静态内存管理机制，堆内存被划分为了两块，Storage和Execution。Storage主要用于缓存RDD数据和broadcast数据，Execution主要用于缓存在shuffle过程中产生的中间数据，Storage占系统内存的60%，Execution占系统内存的20%，并且两者完全独立。<br>　　在一般情况下，Storage的内存都提供给了cache操作，但是如果在某些情况下cache操作内存不是很紧张，而task的算子中创建的对象很多，Execution内存又相对较小，这回导致频繁的minor gc，甚至于频繁的full gc，进而导致Spark频繁的停止工作，性能影响会很大。<br>　　在Spark UI中可以查看每个stage的运行情况，包括每个task的运行时间、gc时间等等，如果发现gc太频繁，时间太长，就可以考虑调节Storage的内存占比，让task执行算子函数式，有更多的内存可以使用。<br>　　Storage内存区域可以通过spark.storage.memoryFraction参数进行指定，默认为0.6，即60%，可以逐级向下递减，如代码清单所示：<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  .set(<span class="string">&quot;spark.storage.memoryFraction&quot;</span>, <span class="string">&quot;0.4&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><pre><code>2.     统一内存管理机制
</code></pre>
　　根据Spark统一内存管理机制，堆内存被划分为了两块，Storage和Execution。Storage主要用于缓存数据，Execution主要用于缓存在shuffle过程中产生的中间数据，两者所组成的内存部分称为统一内存，Storage和Execution各占统一内存的50%，由于动态占用机制的实现，shuffle过程需要的内存过大时，会自动占用Storage的内存区域，因此无需手动进行调节。<h3 id="4-2-JVM调优二：调节Executor堆外内存"><a href="#4-2-JVM调优二：调节Executor堆外内存" class="headerlink" title="4.2 JVM调优二：调节Executor堆外内存"></a>4.2 JVM调优二：调节Executor堆外内存</h3>　　Executor的堆外内存主要用于程序的共享库、Perm Space、 线程Stack和一些Memory mapping等, 或者类C方式allocate object。<br>　　有时，如果你的Spark作业处理的数据量非常大，达到几亿的数据量，此时运行Spark作业会时不时地报错，例如shuffle output file cannot find，executor lost，task lost，out of memory等，这可能是Executor的堆外内存不太够用，导致Executor在运行的过程中内存溢出。<br>　　stage的task在运行的时候，可能要从一些Executor中去拉取shuffle map output文件，但是Executor可能已经由于内存溢出挂掉了，其关联的BlockManager也没有了，这就可能会报出shuffle output file cannot find，executor lost，task lost，out of memory等错误，此时，就可以考虑调节一下Executor的堆外内存，也就可以避免报错，与此同时，堆外内存调节的比较大的时候，对于性能来讲，也会带来一定的提升。<br>　　默认情况下，Executor堆外内存上限大概为300多MB，在实际的生产环境下，对海量数据进行处理的时候，这里都会出现问题，导致Spark作业反复崩溃，无法运行，此时就会去调节这个参数，到至少1G，甚至于2G、4G。<br>　　Executor堆外内存的配置需要在spark-submit脚本里配置，如代码清单所示：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--conf spark.yarn.executor.memoryOverhead=2048</span><br></pre></td></tr></table></figure>
　　以上参数配置完成后，会避免掉某些JVM OOM的异常问题，同时，可以提升整体Spark作业的性能。<h3 id="4-3-JVM调优三：调节连接等待时长"><a href="#4-3-JVM调优三：调节连接等待时长" class="headerlink" title="4.3 JVM调优三：调节连接等待时长"></a>4.3 JVM调优三：调节连接等待时长</h3>　　在Spark作业运行过程中，Executor优先从自己本地关联的BlockManager中获取某份数据，如果本地BlockManager没有的话，会通过TransferService远程连接其他节点上Executor的BlockManager来获取数据。<br>　　如果task在运行过程中创建大量对象或者创建的对象较大，会占用大量的内存，这回导致频繁的垃圾回收，但是垃圾回收会导致工作现场全部停止，也就是说，垃圾回收一旦执行，Spark的Executor进程就会停止工作，无法提供相应，此时，由于没有响应，无法建立网络连接，会导致网络连接超时。<br>　　在生产环境下，有时会遇到file not found、file lost这类错误，在这种情况下，很有可能是Executor的BlockManager在拉取数据的时候，无法建立连接，然后超过默认的连接等待时长60s后，宣告数据拉取失败，如果反复尝试都拉取不到数据，可能会导致Spark作业的崩溃。这种情况也可能会导致DAGScheduler反复提交几次stage，TaskScheduler返回提交几次task，大大延长了我们的Spark作业的运行时间。<br>　　此时，可以考虑调节连接的超时时长，连接等待时长需要在spark-submit脚本中进行设置，设置方式如代码清单所示：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--conf spark.core.connection.ack.wait.timeout=300</span><br></pre></td></tr></table></figure>
调节连接等待时长后，通常可以避免部分的XX文件拉取失败、XX文件lost等报错。</li>
</ol>
]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>spark</tag>
      </tags>
  </entry>
</search>
